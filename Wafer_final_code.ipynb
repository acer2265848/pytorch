{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#f47641\">cread detile csv</font>  \n",
    "###  <font color=\"#DAC9A6\"># deal with normalization </font>  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "import numpy as np\n",
    "sr = 22050 # sample rate\n",
    "T = 0.2    # seconds\n",
    "t = numpy.linspace(0, T, int(T*sr), endpoint=False) # time variable\n",
    "x = 0.5*numpy.sin(2*numpy.pi*490*t)                # pure sine wave at 440 Hz\n",
    "ipd.Audio(x, rate=sr,autoplay=True) # load a NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "# import re\n",
    "# import pandas as pd\n",
    "# from sklearn import preprocessing \n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# count = 0\n",
    "# is_normal=[\"normal\",\"abnormal\"]\n",
    "# for a in is_normal:\n",
    "#     df = pd.DataFrame(columns=['batch','sensor','value','status'])\n",
    "#     df[\"value\"]=df[\"value\"].astype(\"float\")\n",
    "#     for filename in (glob.glob('wafer/'+a+'/*')):\n",
    "#         file = pd.read_csv(filename,sep=\"\\t\",header=None)\n",
    "#         batch=re.search(\"1.+?(?=\\.)\",filename)\n",
    "#         sensor=re.search(\"(?<=\\.)(.*)\",filename)\n",
    "#         status=a\n",
    "#         if (sensor.group(0)!=\"ann\" and sensor.group(0)!=\"box\"):\n",
    "#             value = file[1].values\n",
    "#             #normalization\n",
    "#             value = preprocessing.minmax_scale(value,feature_range=(0,100))\n",
    "#             df.loc[count]=[batch.group(0),sensor.group(0),value,status]\n",
    "#         count+=1\n",
    "\n",
    "#     df.to_json(\"wafer_\"+a+\".json\")\n",
    "\n",
    "# #write csv to let the value type in array\n",
    "\n",
    "# print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#f47641\">create train and test dataset csv</font>  \n",
    "###  <font color=\"#DAC9A6\"> # if want change train and test set change seed </font>  \n",
    "###  <font color=\"#DAC9A6\"> # separate data to train test validation        </font>  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import random \n",
    "# import numpy as np\n",
    "\n",
    "# def dataset(is_whole):\n",
    "#     if is_whole==True:\n",
    "#         np.random.seed(1)\n",
    "#         df_normal = pd.read_json(\"wafer_normal.json\")\n",
    "#         df_abnormal = pd.read_json(\"wafer_abnormal.json\")\n",
    "#         np.random.seed(1)\n",
    "#         normal_train =np.random.choice(df_normal.batch.unique(),747,replace=False)\n",
    "#         abnormal_train =np.random.choice(df_abnormal.batch.unique(),89,replace=False)\n",
    "\n",
    "#         df_normal_train=df_normal[df_normal.batch.isin(normal_train)]\n",
    "#         df_normal_left=df_normal[~df_normal.batch.isin(normal_train)]\n",
    "#         normal_validation =np.random.choice(df_normal_left.batch.unique(),160,replace=False)\n",
    "#         df_normal_validation=df_normal_left[df_normal_left.batch.isin(normal_validation)]\n",
    "#         df_normal_test=df_normal_left[~df_normal_left.batch.isin(normal_validation)]\n",
    "\n",
    "#         df_abnormal_train=df_abnormal[df_abnormal.batch.isin(abnormal_train)]\n",
    "#         df_abnormal_left=df_abnormal[~df_abnormal.batch.isin(abnormal_train)]\n",
    "#         abnormal_validation =np.random.choice(df_abnormal_left.batch.unique(),19,replace=False)\n",
    "#         df_abnormal_validation=df_abnormal_left[df_abnormal_left.batch.isin(abnormal_validation)]\n",
    "#         df_abnormal_test=df_abnormal_left[~df_abnormal_left.batch.isin(abnormal_validation)]\n",
    "\n",
    "\n",
    "#         df_normal_train.to_json(\"normal_train_all.json\")\n",
    "#         df_abnormal_train.to_json(\"abnormal_train_all.json\")\n",
    "#         df_normal_validation.to_json(\"normal_validation_all.json\")\n",
    "#         df_abnormal_validation.to_json(\"abnormal_validation_all.json\")\n",
    "#         df_normal_test.to_json(\"normal_test_all.json\")\n",
    "#         df_abnormal_test.to_json(\"abnormal_test_all.json\")\n",
    "        \n",
    "#     else: \n",
    "#         np.random.seed(1)\n",
    "#         df_normal = pd.read_json(\"wafer_normal.json\")\n",
    "#         normal_sample =np.random.choice(df_normal.batch.unique(),200,replace=False)\n",
    "#         df_normal_sample=df_normal[df_normal.batch.isin(normal_sample)]\n",
    "\n",
    "#         df_abnormal = pd.read_json(\"wafer_abnormal.json\")\n",
    "\n",
    "#         np.random.seed(1)\n",
    "#         normal_train =np.random.choice(df_normal_sample.batch.unique(),140,replace=False)\n",
    "#         abnormal_train =np.random.choice(df_abnormal.batch.unique(),89,replace=False)\n",
    "\n",
    "#         df_normal_train=df_normal_sample[df_normal_sample.batch.isin(normal_train)]\n",
    "#         df_normal_left=df_normal_sample[~df_normal_sample.batch.isin(normal_train)]\n",
    "#         normal_validation =np.random.choice(df_normal_left.batch.unique(),30,replace=False)\n",
    "#         df_normal_validation=df_normal_left[df_normal_left.batch.isin(normal_validation)]\n",
    "#         df_normal_test=df_normal_left[~df_normal_left.batch.isin(normal_validation)]\n",
    "\n",
    "#         df_abnormal_train=df_abnormal[df_abnormal.batch.isin(abnormal_train)]\n",
    "#         df_abnormal_left=df_abnormal[~df_abnormal.batch.isin(abnormal_train)]\n",
    "#         abnormal_validation =np.random.choice(df_abnormal_left.batch.unique(),19,replace=False)\n",
    "#         df_abnormal_validation=df_abnormal_left[df_abnormal_left.batch.isin(abnormal_validation)]\n",
    "#         df_abnormal_test=df_abnormal_left[~df_abnormal_left.batch.isin(abnormal_validation)]\n",
    "\n",
    "\n",
    "#         df_normal_train.to_json(\"normal_train.json\")\n",
    "#         df_abnormal_train.to_json(\"abnormal_train.json\")\n",
    "#         df_normal_validation.to_json(\"normal_validation.json\")\n",
    "#         df_abnormal_validation.to_json(\"abnormal_validation.json\")\n",
    "#         df_normal_test.to_json(\"normal_test.json\")\n",
    "#         df_abnormal_test.to_json(\"abnormal_test.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#f47641\">create crossvalidation dataset csv</font>  \n",
    "###  <font color=\"#DAC9A6\"> # if want change train and test set change seed </font>  \n",
    "###  <font color=\"#DAC9A6\"> # separate data to 5-fold crossvalidation        </font>  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def dataset_cross(is_whole):\n",
    "#     import pandas as pd\n",
    "#     import random \n",
    "#     import numpy as np\n",
    "#     if is_whole==True:\n",
    "#         np.random.seed(1)\n",
    "#         df_normal = pd.read_json(\"wafer_normal.json\")\n",
    "#         df_abnormal = pd.read_json(\"wafer_abnormal.json\")\n",
    "#         np.random.seed(1)\n",
    "\n",
    "#         normal_coross_1 =np.random.choice(df_normal.batch.unique(),213,replace=False)\n",
    "#         df_normal_cross_1=df_normal[df_normal.batch.isin(normal_coross_1)]\n",
    "#         df_normal_cross_1[\"cross\"]=1\n",
    "#         df_normal_left=df_normal[~df_normal.batch.isin(normal_coross_1)]\n",
    "\n",
    "#         normal_coross_2 =np.random.choice(df_normal_left.batch.unique(),213,replace=False)\n",
    "#         df_normal_cross_2=df_normal_left[df_normal_left.batch.isin(normal_coross_2)]\n",
    "#         df_normal_cross_2[\"cross\"]=2\n",
    "#         df_normal_left=df_normal_left[~df_normal_left.batch.isin(normal_coross_2)]\n",
    "\n",
    "#         normal_coross_3 =np.random.choice(df_normal_left.batch.unique(),214,replace=False)\n",
    "#         df_normal_cross_3=df_normal_left[df_normal_left.batch.isin(normal_coross_3)]\n",
    "#         df_normal_cross_3[\"cross\"]=3\n",
    "#         df_normal_left=df_normal_left[~df_normal_left.batch.isin(normal_coross_3)]\n",
    "\n",
    "#         normal_coross_4 =np.random.choice(df_normal_left.batch.unique(),213,replace=False)\n",
    "#         df_normal_cross_4=df_normal_left[df_normal_left.batch.isin(normal_coross_4)]\n",
    "#         df_normal_cross_4[\"cross\"]=4\n",
    "#         df_normal_cross_5=df_normal_left[~df_normal_left.batch.isin(normal_coross_4)]\n",
    "#         df_normal_cross_5[\"cross\"]=5\n",
    "\n",
    "#         abnormal_coross_1 =np.random.choice(df_abnormal.batch.unique(),25,replace=False)\n",
    "#         df_abnormal_cross_1=df_abnormal[df_abnormal.batch.isin(abnormal_coross_1)]\n",
    "#         df_abnormal_cross_1[\"cross\"]=1\n",
    "#         df_abnormal_left=df_abnormal[~df_abnormal.batch.isin(abnormal_coross_1)]\n",
    "\n",
    "#         abnormal_coross_2 =np.random.choice(df_abnormal_left.batch.unique(),26,replace=False)\n",
    "#         df_abnormal_cross_2=df_abnormal_left[df_abnormal_left.batch.isin(abnormal_coross_2)]\n",
    "#         df_abnormal_cross_2[\"cross\"]=2\n",
    "#         df_abnormal_left=df_abnormal_left[~df_abnormal_left.batch.isin(abnormal_coross_2)]\n",
    "\n",
    "#         abnormal_coross_3 =np.random.choice(df_abnormal_left.batch.unique(),25,replace=False)\n",
    "#         df_abnormal_cross_3=df_abnormal_left[df_abnormal_left.batch.isin(abnormal_coross_3)]\n",
    "#         df_abnormal_cross_3[\"cross\"]=3\n",
    "#         df_abnormal_left=df_abnormal_left[~df_abnormal_left.batch.isin(abnormal_coross_3)]\n",
    "\n",
    "#         abnormal_coross_4 =np.random.choice(df_abnormal_left.batch.unique(),25,replace=False)\n",
    "#         df_abnormal_cross_4=df_abnormal_left[df_abnormal_left.batch.isin(abnormal_coross_4)]\n",
    "#         df_abnormal_cross_4[\"cross\"]=4\n",
    "#         df_abnormal_cross_5=df_abnormal_left[~df_abnormal_left.batch.isin(abnormal_coross_4)]\n",
    "#         df_abnormal_cross_5[\"cross\"]=5\n",
    "\n",
    "\n",
    "#         df_normal_cross_1.to_json(\"normal_all_cross_1.json\")\n",
    "#         df_normal_cross_2.to_json(\"normal_all_cross_2.json\")\n",
    "#         df_normal_cross_3.to_json(\"normal_all_cross_3.json\")\n",
    "#         df_normal_cross_4.to_json(\"normal_all_cross_4.json\")\n",
    "#         df_normal_cross_5.to_json(\"normal_all_cross_5.json\")\n",
    "\n",
    "#         df_abnormal_cross_1.to_json(\"abnormal_all_cross_1.json\")\n",
    "#         df_abnormal_cross_2.to_json(\"abnormal_all_cross_2.json\")\n",
    "#         df_abnormal_cross_3.to_json(\"abnormal_all_cross_3.json\")\n",
    "#         df_abnormal_cross_4.to_json(\"abnormal_all_cross_4.json\")\n",
    "#         df_abnormal_cross_5.to_json(\"abnormal_all_cross_5.json\")\n",
    "\n",
    "#         print(len(df_normal_cross_1),len(df_normal_cross_2),len(df_normal_cross_3),len(df_normal_cross_4),len(df_normal_cross_5))\n",
    "#         print(len(df_abnormal_cross_1),len(df_abnormal_cross_2),len(df_abnormal_cross_3),len(df_abnormal_cross_4),len(df_abnormal_cross_5))\n",
    "#     else:\n",
    "#         np.random.seed(1)\n",
    "#         df_normal = pd.read_json(\"wafer_normal.json\")\n",
    "#         normal_sample =np.random.choice(df_normal.batch.unique(),200,replace=False)\n",
    "#         df_normal_sample=df_normal[df_normal.batch.isin(normal_sample)]\n",
    "#         df_abnormal = pd.read_json(\"wafer_abnormal.json\")\n",
    "#         np.random.seed(1)\n",
    "\n",
    "#         normal_coross_1 =np.random.choice(df_normal_sample.batch.unique(),40,replace=False)\n",
    "#         df_normal_cross_1=df_normal_sample[df_normal_sample.batch.isin(normal_coross_1)]\n",
    "#         df_normal_cross_1[\"cross\"]=1\n",
    "#         df_normal_left=df_normal_sample[~df_normal_sample.batch.isin(normal_coross_1)]\n",
    "\n",
    "#         normal_coross_2 =np.random.choice(df_normal_left.batch.unique(),40,replace=False)\n",
    "#         df_normal_cross_2=df_normal_left[df_normal_left.batch.isin(normal_coross_2)]\n",
    "#         df_normal_cross_2[\"cross\"]=2\n",
    "#         df_normal_left=df_normal_left[~df_normal_left.batch.isin(normal_coross_2)]\n",
    "\n",
    "#         normal_coross_3 =np.random.choice(df_normal_left.batch.unique(),40,replace=False)\n",
    "#         df_normal_cross_3=df_normal_left[df_normal_left.batch.isin(normal_coross_3)]\n",
    "#         df_normal_cross_3[\"cross\"]=3\n",
    "#         df_normal_left=df_normal_left[~df_normal_left.batch.isin(normal_coross_3)]\n",
    "\n",
    "#         normal_coross_4 =np.random.choice(df_normal_left.batch.unique(),40,replace=False)\n",
    "#         df_normal_cross_4=df_normal_left[df_normal_left.batch.isin(normal_coross_4)]\n",
    "#         df_normal_cross_4[\"cross\"]=4\n",
    "#         df_normal_cross_5=df_normal_left[~df_normal_left.batch.isin(normal_coross_4)]\n",
    "#         df_normal_cross_5[\"cross\"]=5\n",
    "\n",
    "#         abnormal_coross_1 =np.random.choice(df_abnormal.batch.unique(),25,replace=False)\n",
    "#         df_abnormal_cross_1=df_abnormal[df_abnormal.batch.isin(abnormal_coross_1)]\n",
    "#         df_abnormal_cross_1[\"cross\"]=1\n",
    "#         df_abnormal_left=df_abnormal[~df_abnormal.batch.isin(abnormal_coross_1)]\n",
    "\n",
    "#         abnormal_coross_2 =np.random.choice(df_abnormal_left.batch.unique(),26,replace=False)\n",
    "#         df_abnormal_cross_2=df_abnormal_left[df_abnormal_left.batch.isin(abnormal_coross_2)]\n",
    "#         df_abnormal_cross_2[\"cross\"]=2\n",
    "#         df_abnormal_left=df_abnormal_left[~df_abnormal_left.batch.isin(abnormal_coross_2)]\n",
    "\n",
    "#         abnormal_coross_3 =np.random.choice(df_abnormal_left.batch.unique(),25,replace=False)\n",
    "#         df_abnormal_cross_3=df_abnormal_left[df_abnormal_left.batch.isin(abnormal_coross_3)]\n",
    "#         df_abnormal_cross_3[\"cross\"]=3\n",
    "#         df_abnormal_left=df_abnormal_left[~df_abnormal_left.batch.isin(abnormal_coross_3)]\n",
    "\n",
    "#         abnormal_coross_4 =np.random.choice(df_abnormal_left.batch.unique(),25,replace=False)\n",
    "#         df_abnormal_cross_4=df_abnormal_left[df_abnormal_left.batch.isin(abnormal_coross_4)]\n",
    "#         df_abnormal_cross_4[\"cross\"]=4\n",
    "#         df_abnormal_cross_5=df_abnormal_left[~df_abnormal_left.batch.isin(abnormal_coross_4)]\n",
    "#         df_abnormal_cross_5[\"cross\"]=5\n",
    "\n",
    "\n",
    "#         df_normal_cross_1.to_json(\"normal_cross_1.json\")\n",
    "#         df_normal_cross_2.to_json(\"normal_cross_2.json\")\n",
    "#         df_normal_cross_3.to_json(\"normal_cross_3.json\")\n",
    "#         df_normal_cross_4.to_json(\"normal_cross_4.json\")\n",
    "#         df_normal_cross_5.to_json(\"normal_cross_5.json\")\n",
    "\n",
    "#         df_abnormal_cross_1.to_json(\"abnormal_cross_1.json\")\n",
    "#         df_abnormal_cross_2.to_json(\"abnormal_cross_2.json\")\n",
    "#         df_abnormal_cross_3.to_json(\"abnormal_cross_3.json\")\n",
    "#         df_abnormal_cross_4.to_json(\"abnormal_cross_4.json\")\n",
    "#         df_abnormal_cross_5.to_json(\"abnormal_cross_5.json\")\n",
    "\n",
    "#         print(len(df_normal_cross_1),len(df_normal_cross_2),len(df_normal_cross_3),len(df_normal_cross_4),len(df_normal_cross_5))\n",
    "#         print(len(df_abnormal_cross_1),len(df_abnormal_cross_2),len(df_abnormal_cross_3),len(df_abnormal_cross_4),len(df_abnormal_cross_5))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_cross(True)\n",
    "# dataset_cross(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#f47641\">Represent to image</font>  \n",
    "###  <font color=\"#DAC9A6\"> # Write in function just need call method </font>  \n",
    "###  <font color=\"#DAC9A6\"> # is_whole : decide call all datasate or sample datasat </font>  \n",
    "###  <font color=\"#DAC9A6\"> # is_cross : decide call crossvalidation dataset or not </font>  \n",
    "\n",
    "###  <font color=\"#DAC9A6\"> # method : \"mtf\",\"gasf\",\"gadf\",\"recurrence_plots\" </font>  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167713_12\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQUAAAD8CAYAAAB+fLH0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAGKdJREFUeJztnTuvHTUXht+Q5OT+5R5xkBAgcY3SIUSTio6GloJfQoEQ8GeoKZBoaKACpUsiQEiAKBKSQ27k5A581Tp7vOI1tmfP7DMhz9NM5uaxfXaWX9vLyzv+/fdfAQAYz2x3BgBgXmAUACABowAACRgFAEjAKABAAkYBABIwCgCQgFEAgASMAgAk7NruDEjSJ598Mrpb5b1795LzvXv3Zu9/8MEHW9f27NkjSVpfX0/e2bUrX03eG9TOv/zyS0nSuXPnBuW9y99//y1J2rlzZ5LvPj799FNJ0m+//SZJunHjhiTp7t27kqRHjx4ladtRerxMxo4dOyRJzzzzTHJuz9u5HQ17vi/N6Dy63s1j9F3jn3/+yV63NKK/Yfffm5ubkqSLFy9Kkg4ePChJOnz4cHK034+vo24d+PxG+fb5sud++OEHSdLly5ez7/Xx8ccf5z/mQCkAQAJGAQASMAoAkIBRAIAEjAIAJGAUACABowAACRgFAEjAKABAAkYBABIwCgCQgFEAgASMAgAkYBQAIAGjAAAJGAUASMAoAEACRgEAEjAKAJCAUQCAhFkEbrWAl9vxjQcPHjx2zQKZWtBPH5zUiIJu7tu3r/ebLVge+gKgRu9Y2ex4//59SYvArXbsBjf1gU6jQKNR2X0+u8+V6m+ugVut3uxogXzt72tHe96C7Brd82UDt1ow4Sn/z6AUACABowAACRgFAEiYxZiC9dXGpLRpit1fW1vbumb9NOsD+k09PFF/1DZdGaNcQzaDsXxb2ezo07K+MZvB5NO2f9vYi/0+rF/vj6vYDMb+/lP8nzFQCgCQgFEAgASMAgAkzGJMAT+FGPwUtn9MAT8FAHiq2RGNNq+SM2fOjJ4Jb0n9aK3df++997au3blzR5J04cIFSdLGxoYk6fr168n9aCt3a5UePnwoSXrxxReXLocf5a9pIaw1OXv2rCTpxIkTkhZbqPvZh25L5lt636La0crq8+d/T92WOvqt9bXW3fOoVe1Lu6Q+apSN1dsrr7wiafE7uH37tiTp5s2bkhYzA15h9s3uRErG58ue++yzzyRJR48ezb7Xx/nz59mKHgDawSgAQMIsBhqndMQofcMGBbt4pxRz/rFug5eedjSZOKaDiaUdycwchw8flrQomz8u032IBl//K92HLnbP/w68U5MN5FqaU3YfrGuK8xIArIxZKIVLly6Nnqa1goZZdX//9OnTW9e89f3jjz8kSVevXpUk3bp1S9KiZbA0zXrb+blz5ySNUy7fEvty5XjnnXckSWfOnJEkHT9+XJK0f/9+SQtl4N25/b+7342OntqWMPdsbZrd1r80UF5SCn3TovZvU1jPPfecpMXf2wYcNzc3k+teKeTUUqkeo2nYa9euJd+eApQCACTMQilM0T8yq234Fsvunzx5cuuajQWcOnUqm6ZNB3onIDu3vqNZ+VWUK4fl36YibfrKjyn4MRGp7Ezjj17JRM/3pRmd++tTjCnUKAX7u1v97d69W9LjYzPeGSynFHx+W5WC/cYYUwCAlTELpeD7+2PQ5xrbvd91AjHr21UPXWwU2hSFH322cxulnsKa97WWhimFY8eOSVrMRvilvdbS9bkiG61jCbnrtY5yQxzqxlIKuWfs7+kVgj/a379v3KBWHZV+v1P8nzFQCgCQMAulMAWl1sbu24i8tOgb/u9//5O0UAQ+sIm1uNa/X6VSqGlFLf8HDhyQtOgLW75yYwlGNG9f6ydR2xLWvNvCUKVQc90Ugt3zC57s21HdjakU7Ntd34exQSkAQMJ/VinU0p33N2tsLar3bPQtvx/FN+XgW5JVY/m1Fs7K6H0casYUfCu4zLhAi1fmqqgJFxf9PUvKIXe+bB1479kpQCkAQMIslELNiHorfV5w3ftdK29W3LesdrSW14/8euufa4GHUvIHyGH59bMMUUDRmhF4n58on9F7pe/UfGNMWsYUamcwvJrK+VfU1mf0XM2sybKgFAAgYRZKYTvJKQVTBD5Euh3tOa8YfMs8BkNaBMu/Hf1sQ98KwZKfQm34sKHP1Lw3xtqH2vt9z9QqhS7L1kFLWL6hoBQAIGEWSmHK/lHpG13L633ZfbwBP7Zg+JHgMccUhuDHFKIWbcz+aU1fubY/XXovd6/VT6El7Wjjm1Iauby1jA3l0qoZB1oWlAIAJMxCKbz77rujp+m9EG3u3t/vzt3bM2+++aakhTX3odJ9wFZ/NL7//vslSzFs2ziLp2D5tW3sbA1+NCbSvWfUqomor1uzrqJ0v6Z1bR3raFnFaXX+yy+/SFp4iB46dEjSwnPUez7mWvValRGNo7z//vuSpCtXroT5XhaUAgAkzEIpvPXWW6OnWasUfv31161rtqZhfX09ecdv+mFEnmumNMZgiFIwBfD7779Lkm7cuCFpoRiiEPVSuW/+NG4wa5GVLl68KGkR8t1Wn0arUHOzPMtuBvP2229Lki5fvpx9bwxQCgCQgFEAgASMAgAkYBQAIAGjAAAJGAUASMAoAEACRgEAEjAKAJCAUQCABIwCACRgFAAgAaMAAAkYBQBIwCgAQAJGAQASZhFkZYqw1aVgH7kQX0ODafr3fODUMRgSsNMCjNQepcdDykVBVXw48yhoSC4gSu15dH3VQVZ8PfngNLl6jPK0bJAVv7HPFKAUACBhFkphio1HvaX136jZ0qv1G4aFOxujXL7lblEzfhMYf8y1lqXW+kkLxxbRohR8vVlr7Vvtmk13llUKXp1MAUoBABJmoRR8QNRVfsPCoHfx1rh1ExALAT5GuSwPLX1Ie8eHpr9//76khZLJKZpSiPdSCxeN3eTeeVLGFKze7GhjRj6gb27TYn++rFLwwYSnAKUAAAkYBQBIwCgAQAJGAQASMAoAkIBRAIAEjAIAJGAUACBhR+sioClYW1sbPRPeicY7sdj9r7/+euua7eh84cIFSdKVK1eS461bt5LnzPnn4cOHybmlaU4uY1LjxPThhx9Kks6ePStJOnr0qKSFU1XkpiuVnWlqnH6i6zXP9D3XR2m37Oh6nwOVdxg6efKkpMXf25yZbDdvu+6d3vrqwJ6NFpz559544w1J0u7du7Pl6uPBgwdVvuAoBQBImIWb8wsvvDB6mmtra8m5d2e2+99+++3WNbP458+flyRtbGxIkq5duyZJ2tzclLRoEcwd2rsLW9rPP//80uWwFsOn3cc333yTnB8/flySdPDgQUkLBWPHnBuup6/16zvvW7gzRG34PEb5MSJlVbOAy+4dOHBAkvTyyy9LWvxObt++LUm6efOmpIVyiJZa58pUq3DsuRMnTkiSjhw5kn1vDFAKAJAwC6UwhdWrVQqmBqRFC/Dnn39Kkq5fvy5p0RLYfVMGUZAN67uPUa4hSsHyffXq1eS6jYUsoxRKC4z8eV9/+klRCvZ3t1b6zp07khZjTPb7sPq130VOLZXGZnJl7KZhag+lAAArYxZKwUZ3p0zTtxh23/qF0qJFsJbAzq2vaGrDjyH4/rb1Qccol7Xi1vrUpOnHQEy5+DRtBLtbN76eopatRhl0891HrYLoC6Qy1uxD30yM/S6sXu1ovyH7nfQpBa+0IuUVzT7Y33+K/zNb354sZQB4IpmFUhgy51rC+wj4b9j9bkvmA3JGYwZRq+kDooxRLh/iq8b3wfLhg6nY0dNNMwpbF5XVUwqD15d2lMaUSqHPn8We8fXnZ538ua/nvjGFVj8F+1tN8X/GQCkAQMIslIIPXzUGvkX11tvum89B9xnfAkSKIVIQ3mNwGXwQ0BqlELVcVta+lnlZj8Y+pRC15qUgpDVKobZv7vF/o5xHY6S4Io9WP6bQVaO1SipSCmP+tiJQCgCQ8J9VCt7S+m/4+X/pcQtfezR8izaFUqhZ+1AaG7HzXFqRJ92USmHKMYXWsYYuVj9R/ZWutygFP6YQPYdSAICVg1EAgIRZdB9ad/epoW//ge79nGNJbTfC8OdD9n2M8Ls5tez/UJoy9dI1l36URu2A2ZhuzrluxFA3ZyNaUt+9F5XdX/f3/e8o925Nt6t7PnRHrBZQCgCQgFEAgASMAgAkzGJMYQpKIb1y/dPaaT8/fdT67RYit+MhtO4L2b1XmhbMjdFEaXl82i3TiMv2rVveH7q4qsbVu1TPpboZE5QCACTMQilMYf2GOMSUWtLWXZHHKFfJqaUPPwtSU65llIC0cKopLZzqUmola4KsRIwRZCWqx9L1XNn7vteXLzuPZn/GBKUAAAmzUAo1rUkrpXnfXKvU2rJG/byWVrLEkBYico2OWp8aPwV/v9Y9Nzfv77/fypjjAH3+LNHfu/Z34n1MpLxvSE2+S1sWjAlKAQASZqEU5jamMFUehlAKrZ6jtgWr8bwsjTGU3uu2aNE7Q1vP6Duld/ruDxlj6lMGPs3aFj96bszxqgiUAgAkzEIpTOnH3fKNsa3vGOWa0td9SHn71iG0pt3aL+5b+zAHWvJSKntpDIy1DwCwMmahFLZzTKFLrR+Cvx6NwI9RriFptvaFa/q8rb4aNdT2r4eMedR+u2VMIfp2yZ8l937J74MxBQCYDf9ZpVBaM+BjC3QpecHVMoVSWGZ+umXNg9EXeyGXzz5vxNK7tV6TOaboY0e+AjVKoOV+N+3aekYpAMDKmIVSmCslxdA3Hz0nlvEAjMpYUgY5NVKqryi6VE10pIjWtQ99vhCld/vSiJ7x+YvUSEvErWVBKQBAwiyUwpR+3KVvdC2wrfAr+baXmHLtQwvRGgh/7NuK3p/3jcV0n8+1+qUZoEiB9YUz96syo/xE1/vK27rGwW8G7NPp5jNSVKV4Cj4e5BSgFAAgAaMAAAmz6D5MwTLh2DylgabS8uFlqA2D3qVW9hp9deDrKZo2jORvzZRkiZJLdR/LdB+iZ1vrtcb5qpSPVQ5ioxQAIAGjAAAJGAUASJjFmMJcFkR5WvtxJcefISyzEGbIMtvSs63h2YY4L7W4DZfqY5kxhSELnaS2cPe14xBT/LYiUAoAkDALpbCd1Mw+PGnUto4t4diiWYjWPHQZEtJ9yHfGfC/37hC351rHrdJ7U4BSAICEWSiFuY0pLLt0ei6bwYwxr1+78GnIsueSa6//Vi6PQ8cUovs1fgqld32deLfnmjQZUwCA2TALpbCdjOHRODdqxxRaPO5q+/m1wUK6adYqhtK3W56tSaNWMUbnfeVZxktzalAKAJCAUQCABIwCACTMYkxhbtvG1c7F+/fmEuLd56tl1eQyffAuudmI0uxC6/jFkHy1vN+6OrJlJsOoHVdZZhaqFZQCACTMQilsJ12LG4WBb2WOI8pSrDr6Yh5ELVKtN2LtvT5qRupbw7G1fLd0vaVctWluJygFAEiYhVKYomVdJpJNq9WeopXyaQzxjRgSQLbUkkWt5BCV1dri1qy8bKVm/KJU9trz2nt9aa3CRwalAAAJGAUASJhF92FuC6JapVk0lTa3KcmWd/z3W6+3BBopTUlGeat5p0TN37p1QVRN2q1lZUoSALaNWSiFKXa7KQ181YR4j5x+Irw1n2KHqGXCsUU7RtU4L3mG7G04paNR63L30oKu7jPLOjH17X3ZugiPHaIAYOVgFAAgAaMAAAkYBQBIwCgAQMIsZh927tw5epp+NNp/w+7nFkT52YPS3LC/byPFY5TLl6Nl1N/yb4FD7bwvZFo0ql2qixo356G+I1H99uXHaJ0l6ebb15Ovg9rzbpq1fie+/uzcflNT/J8xUAoAkIBRAIAEjAIAJGAUACABowAACbOYfdjOICu52YdonUTtyPEUgTBa0qwNiNKy4q70TjRr0RfurjZgS1+afdf68lUzKxHNuJSO0WxF37P+9+jz1xLMdllQCgCQ8NQrhW5LUlII/nr03JhKoWUbNp+f0rx5Ls2ohbJjbrPU7v3ovb5novPo+hh+Ct5no4+S/0pUrznlWavWIj8FwrEBwMrBKABAAkYBABJmMaYwBa1Rkrr/HhrSewpa4v4ZUR+3NJrefcbjxxKGzCRMOfvQGokoiiGZG2NonYXw+Rxz9mFIxKtWUAoAkIBRAICEWXQfViKJgm/kpHM09ZR7J3d9SGj1iDHSKsna3HLhUlq1XYE+Wd/qCDWEUlq+vLm6KDkplZyWaroPpannmm7OWKAUACBhFkrhwIEDo6e5Z8+e5Pz+/fvZ+7dv3966du/ePUnSnTt3JEkPHjxIjg8fPpS0GHSLWpC9e/dKGqdcvgXx5cph5djc3EzyY+zatSs5dgN2RAOZrUFWWpRCdB5dH8N5qSYcu79n9emP9huyeveKIefwVesqb1ha+/fvlzTN/xkDpQAACbNQCqdPnx49Td86mhX397/44outa6YmLl26JEm6e/eupIVCKE2VmXV/6aWXJI1TLj9V5cuVw8r0888/S5I2NjYkLVoZrxS6rWPJzbmkFJZRBnNQCjmldPDgweTclMHNmzeTo/1+vJIcEo4t2o7vtddekyStr69n3xsDlAIAJMxCKXT79WPx6NGj5NwrBbu/b9++rWtmnXfv3i3p8TGEaDGQYdbcFMYY5bL+vn3blyuHKQArm++HWutp5WwJAlravm6IY1SUhj/vc9yqbXH9da8kclvoWb35frz9Lexo9ejrqPu7iRbdlfJtz/lxjClAKQBAwiyUgvXJxsTPNnilYPcPHTq0dc1aTmtZI4VQ6hfeunVL0jjl8krBlyuHjTtYX9jKaOfMPtQtQbZ6MYVw+PBhSYt689jMkB9TGHP24fLly5JShTs2KAUASHjqlYJZf0laW1uTtFAK1lc0S2/Wu+SnYH4OY5TL+ynUKAVrRaxsR44ckbRQCtYCmjIac/ahzzuxVgls55hCF7tn9VZSCvYb61MKtd6bkaflTz/9JKluFmooKAUASJiFUqhp/VrxLYT/ht23VlRaWHFTDL7v7fuGuW3BpMWsxRjlGhKOzfJrfVxrVey8b0yhVilES36XUQrbMaZQoxR8/fnxHa9Cp1QK5l07xf+ZrW9PljIAPJHMQimMuSrOiFp1fz/Xn/ZHP1IdHf23pljtV/KVkBYtvy+H35w0t1lp7exDNI/uz3Otuk+zlFYNU/gpRPUX1WffikufT7860iuuaE3GFL8tD0oBABJmoRSGtAytadbMiZdaleh61K8eo1xD0vStYK3i6T5rROv9o2+2lDn61hSb6ETfrvFojJ4pHX08htz3fX1G3yz5iYwJSgEAEmahFGr6ya1EYwj+fnfOOeprRy1BNCre58k2FEurpS8ZKYWoj+z/3X2n1DKNMfsQtaw1fgpRXUfrOlrWPpTGEPx9n+8ukTLwRPmqXYezDCgFAEjAKABAwiy6D1NQK3dzcjEKvFE6TjEIVLtoqEs0sBhNnfWFIIvSrpXBufo1WuupZvFS7Ts1QVZaBxRLv5vus/73V5qSnHLvSA9KAQASZqEU5jIl6Wm1zmM44XiWmYoaskNx6Vn//cgNu8Z5Kaqv2t29cmn2PZu7PkQplNIuTd/WpFn6FlOSALAynnql0OfmbNS6N/u0xyjXkPGKqI9bctvNvWPnkVuuT9vIOSKVphrnEGQl1/8vjSFEv5scPr+l/PlznJcAYOXMQik8aUxppcegduYld75s37UvMOlYQVZa8jR0zEEavuio75u1ZVvlbIMHpQAACSiFAQxpsVZJy+h96fnWsrYEhRnaSo45+9DH0E1c++psTv4IESgFAEiYhVKYwjqWLLAfVe/+O+oX1y5jHuIfUKIlzWjDlmjj076l01EYttKS6lwdbUc4tohoQVSXyEektAV9dMzlszYcW+QBOQUoBQBIwCgAQAJGAQASZjGm8KR7NEbeZnPxaIwC0tYEWYm+W6qDJ8WjsSbEe2199gVsifJb69Hox27waASAlTELpfD666+PnqbfoMNvs2X3P/roo61rto3asWPHJD2+Rt+HRItGnz///HNJ0nfffbdkKRbftLz4cuX46quvJEkbGxuSpL/++it514f06tuspFYdtaw6bKUvdHq0FsMohS2rWT1r2wD++OOPkhYbzj777LOSpFdffVXSYhOhSFn05bOEr4NLly4NSqfqW5OlDABPJLNQCuvr66OnWasUrl27tnXNtgWzDVrtHb+9mh2jVubUqVOSxinXEKVg29bduHEjOd69e1fS4xvn9ikFI1ohWFIKfX32Wm++vvUYpXn7yA8gGovI+VVsbm5KWrTOtuGs1af9Tez3U6MUalfa+vdMnUwJSgEAEjAKAJCAUQCABIwCACRgFAAgAaMAAAkYBQBIwCgAQAJGAQASMAoAkIBRAIAEjAIAJOyYa5hyANgeUAoAkIBRAIAEjAIAJGAUACABowAACRgFAEjAKABAAkYBABIwCgCQgFEAgASMAgAkYBQAIAGjAAAJGAUASMAoAEACRgEAEjAKAJCAUQCABIwCACRgFAAgAaMAAAkYBQBIwCgAQML/ARygj4m2XEMHAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=128x128 at 0x24361F83400>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyts.visualization import plot_gasf,plot_gadf,plot_mtf,plot_recurrence_plots\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "method=\"gadf\"\n",
    "path=\"_all\"\n",
    "a = \"train\"\n",
    "b=\"normal\"\n",
    "x= pd.read_json(b+\"_\"+a+path+\".json\")\n",
    "i=0\n",
    "x_time=np.array(x.iloc[:,3].values[i])\n",
    "x_class= str(x.iloc[:,0].values[i])+'_'+str(x.iloc[:,1].values[i])\n",
    "print(x_class)\n",
    "locals()[\"plot_\"+method.lower()](x_time,\n",
    "                             cmap=\"gray\",\n",
    "                             image_size=104,\n",
    "                             output_file=\"wafer_img\"+path+\"/\"+method+\"/\"+a+\"/\"+b+\"/\"+x_class+\".png\")\n",
    "\n",
    "x_time=np.array(x.iloc[:,3].values[i])\n",
    "x_class= str(x.iloc[:,0].values[i])+'_'+str(x.iloc[:,1].values[i])\n",
    "img = Image.open(\"wafer_img\"+path+\"/\"+method+\"/\"+a+\"/\"+b+\"/\"+x_class+\".png\")\n",
    "# crop(起始點橫坐標，起始點縱座標，寬，高)\n",
    "img=img.crop((20, 3, 237, 220))\n",
    "img.thumbnail((128,128))  #resize\n",
    "# img.save(\"wafer_img\"+path+\"/\"+method+\"/\"+a+\"/\"+b+\"/\"+x_class+\".png\")\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformation(is_whole,is_cross,method):\n",
    "    from pyts.visualization import plot_gasf,plot_gadf,plot_mtf,plot_recurrence_plots\n",
    "    import matplotlib.pyplot as plt\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from PIL import Image\n",
    "\n",
    "    \n",
    "    if is_cross==False:\n",
    "        is_train=[\"train\",\"test\",\"validation\"]\n",
    "        is_normal=[\"normal\",\"abnormal\"]\n",
    "        if is_whole==True:\n",
    "            path=\"_all\"\n",
    "        else:\n",
    "            path=\"\"\n",
    "        for a in is_train:\n",
    "            for b in is_normal:\n",
    "                x= pd.read_json(b+\"_\"+a+path+\".json\")\n",
    "                for i in range(len(x)):\n",
    "                    x_time=np.array(x.iloc[:,3].values[i])\n",
    "                    x_class= str(x.iloc[:,0].values[i])+'_'+str(x.iloc[:,1].values[i])\n",
    "                    if method.lower() != \"recurrence_plots\":\n",
    "\n",
    "                        locals()[\"plot_\"+method.lower()](x_time,\n",
    "                                                 cmap=\"gray\",\n",
    "                                                 image_size=20,\n",
    "                                                 output_file=\"2_wafer_img\"+path+\"/\"+method+\"/\"+a+\"/\"+b+\"/\"+x_class+\".png\")\n",
    "                    else:\n",
    "                        locals()[\"plot_\"+method.lower()](x_time,\n",
    "                                                 output_file=\"2_wafer_img\"+path+\"/\"+method+\"/\"+a+\"/\"+b+\"/\"+x_class+\".png\")\n",
    "\n",
    "                for i in range(len(x)):\n",
    "                    x_time=np.array(x.iloc[:,3].values[i])\n",
    "                    x_class= str(x.iloc[:,0].values[i])+'_'+str(x.iloc[:,1].values[i])\n",
    "                    img = Image.open(\"2_wafer_img\"+path+\"/\"+method+\"/\"+a+\"/\"+b+\"/\"+x_class+\".png\")\n",
    "                    img=img.crop((20, 3, 237, 220))\n",
    "                    img.thumbnail((127,127))  #resize\n",
    "                    img.save(\"2_wafer_img\"+path+\"/\"+method+\"/\"+a+\"/\"+b+\"/\"+x_class+\".png\")\n",
    "    else:\n",
    "        if is_whole==True:\n",
    "            path=\"_all\"\n",
    "        else:\n",
    "            path=\"\"\n",
    "        cross=[\"1\",\"2\",\"3\",\"4\",\"5\"]\n",
    "        is_normal=[\"normal\",\"abnormal\"]\n",
    "        for a in cross:\n",
    "            for b in is_normal:\n",
    "                x= pd.read_json(b+path+\"_cross_\"+a+\".json\")\n",
    "                for i in range(len(x)):\n",
    "                    x_time=np.array(x.iloc[:,3].values[i])\n",
    "                    x_class= str(x.iloc[:,0].values[i])+'_'+str(x.iloc[:,1].values[i])\n",
    "                    if method.lower() != \"recurrence_plots\":\n",
    "\n",
    "                        locals()[\"plot_\"+method.lower()](x_time,\n",
    "                                                 cmap=\"gray\",\n",
    "                                                 image_size=20,\n",
    "                                                 output_file=\"2_wafer_img_cross\"+path+\"/\"+method+\"/\"+a+\"/\"+b+\"/\"+x_class+\".png\")\n",
    "                    else:\n",
    "                        locals()[\"plot_\"+method.lower()](x_time,\n",
    "                                                 output_file=\"2_wafer_img_cross\"+path+\"/\"+method+\"/\"+a+\"/\"+b+\"/\"+x_class+\".png\")\n",
    "\n",
    "                for i in range(len(x)):\n",
    "                    x_time=np.array(x.iloc[:,3].values[i])\n",
    "                    x_class= str(x.iloc[:,0].values[i])+'_'+str(x.iloc[:,1].values[i])\n",
    "                    img = Image.open(\"2_wafer_img_cross\"+path+\"/\"+method+\"/\"+a+\"/\"+b+\"/\"+x_class+\".png\")\n",
    "                    img=img.crop((20, 3, 237, 220))\n",
    "                    img.thumbnail((127,127))  #resize\n",
    "                    img.save(\"2_wafer_img_cross\"+path+\"/\"+method+\"/\"+a+\"/\"+b+\"/\"+x_class+\".png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformation(True,True,\"gasf\")\n",
    "transformation(True,True,\"gadf\")\n",
    "transformation(True,True,\"mtf\")\n",
    "transformation(True,True,\"recurrence_plots\")\n",
    "print(\"done whole=true cross=true\")\n",
    "\n",
    "transformation(False,True,\"gasf\")\n",
    "transformation(False,True,\"gadf\")\n",
    "transformation(False,True,\"mtf\")\n",
    "transformation(False,True,\"recurrence_plots\")\n",
    "print(\"done whole=False cross=true\")\n",
    "\n",
    "transformation(True,False,\"gasf\")\n",
    "transformation(True,False,\"gadf\")\n",
    "transformation(True,False,\"mtf\")\n",
    "transformation(True,False,\"recurrence_plots\")\n",
    "print(\"done whole=true cross=False\")\n",
    "\n",
    "transformation(False,False,\"gasf\")\n",
    "transformation(False,False,\"gadf\")\n",
    "transformation(False,False,\"mtf\")\n",
    "transformation(False,False,\"recurrence_plots\")\n",
    "print(\"done whole=False cross=False\")\n",
    "\n",
    "import IPython.display as ipd\n",
    "import numpy\n",
    "sr = 22050 # sample rate\n",
    "T = 0.3    # seconds\n",
    "t = numpy.linspace(0, T, int(T*sr), endpoint=False) # time variable\n",
    "x = 0.5*numpy.sin(2*numpy.pi*490*t)                # pure sine wave at 440 284Hz\n",
    "ipd.Audio(x, rate=sr,autoplay=True) # load a NumPy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "transformation(True,True,\"mtf\")\n",
    "transformation(True,True,\"gasf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#f47641\">create dataset & dataloader</font>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## six channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function setting done\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self,train_x,train_y):\n",
    "        self.train_x=train_x\n",
    "        self.train_y=train_y\n",
    "        self.data_len = len(train_x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        train_x=self.train_x[index]\n",
    "        train_y=self.train_y[index]\n",
    "        return (train_x, train_y)\n",
    "    def __len__(self):\n",
    "        return self.data_len\n",
    "    \n",
    "class dataset_dataloader:\n",
    "    def __init__ (self,is_whole,is_cross,method,cross_number_for_test=1,batchsize=24,numworkers=2):\n",
    "        if is_cross==False:\n",
    "            import pandas as pd\n",
    "            import numpy as np\n",
    "            from PIL import Image\n",
    "            from pyts.visualization import plot_gasf\n",
    "            import matplotlib.pyplot as plt\n",
    "            import torch\n",
    "            from torch.autograd import Variable\n",
    "\n",
    "            if is_whole==True:\n",
    "                whole=\"_all\"\n",
    "            else:\n",
    "                whole=\"\"\n",
    "\n",
    "            normal   = pd.read_json(\"normal_train\"+whole+\".json\")\n",
    "            abnormal = pd.read_json(\"abnormal_train\"+whole+\".json\")\n",
    "            x=normal.append(abnormal)\n",
    "            x=x.sample(frac=1)\n",
    "\n",
    "            def pil_loader(path):\n",
    "                with open(path, 'rb') as f:\n",
    "                    img = Image.open(f)\n",
    "#                   轉成灰階\n",
    "                    return img.convert('L')\n",
    "            lst = list()\n",
    "            count=0\n",
    "            lst2=list()\n",
    "            y=list()\n",
    "\n",
    "            for i in x.batch.unique():\n",
    "                df=x[x.batch==i]\n",
    "                lst = list()\n",
    "                if np.all(df.status==\"abnormal\"):\n",
    "                    path=\"2_wafer_img\"+whole+\"/\"+method+\"/train/abnormal/\"\n",
    "                else:\n",
    "                    path=\"2_wafer_img\"+whole+\"/\"+method+\"/train/normal/\"\n",
    "\n",
    "                for j in x.sensor.unique():\n",
    "                    img=pil_loader(path+str(i)+\"_\"+str(j)+\".png\")\n",
    "                    lst.append(np.array(img)/255)\n",
    "                    \n",
    "                arr = np.array(lst)\n",
    "                if df.iloc[0].status==\"normal\":\n",
    "                    y.append(0)\n",
    "                else:\n",
    "                    y.append(1)\n",
    "\n",
    "                lst2.append(arr)    \n",
    "            #     count+=1\n",
    "            #     if count==1:\n",
    "            #         break\n",
    "            train_x = torch.from_numpy(np.array(lst2)).float()\n",
    "            train_y = torch.LongTensor(np.array(y))\n",
    "\n",
    "            train_data = Dataset(train_x,train_y)\n",
    "            train_loader = DataLoader(dataset=train_data,\n",
    "                                      batch_size=24,\n",
    "                                      shuffle=True,\n",
    "                                      num_workers=2)\n",
    "            self.train_x=train_x\n",
    "            self.train_y=train_y\n",
    "            self.train_loader=train_loader\n",
    "\n",
    "            #test data set\n",
    "            normal   = pd.read_json(\"normal_validation\"+whole+\".json\")\n",
    "            abnormal = pd.read_json(\"abnormal_validation\"+whole+\".json\")\n",
    "            x=normal.append(abnormal)\n",
    "            x=x.sample(frac=1)\n",
    "\n",
    "            # print(x)\n",
    "            lst = list()\n",
    "            count=0\n",
    "            lst2=list()\n",
    "            y=list()\n",
    "            path=\"\"\n",
    "            for i in x.batch.unique():\n",
    "                df=x[x.batch==i]\n",
    "                lst = list()\n",
    "                if np.all(df.status==\"abnormal\"):\n",
    "                    path=\"2_wafer_img\"+whole+\"/\"+method+\"/validation/abnormal/\"\n",
    "                else:\n",
    "                    path=\"2_wafer_img\"+whole+\"/\"+method+\"/validation/normal/\"\n",
    "\n",
    "                for j in x.sensor.unique():\n",
    "                    img=pil_loader(path+str(i)+\"_\"+str(j)+\".png\")\n",
    "                    lst.append(np.array(img)/255)\n",
    "                    \n",
    "                arr = np.array(lst)\n",
    "                if df.iloc[0].status==\"normal\":\n",
    "                    y.append(0)\n",
    "                else:\n",
    "                    y.append(1)\n",
    "\n",
    "                lst2.append(arr)    \n",
    "            #     if count==1:\n",
    "            #         break\n",
    "            test_x = Variable(torch.from_numpy(np.array(lst2)).float()).cuda()\n",
    "            test_y = torch.LongTensor(np.array(y)).cuda()\n",
    "            self.test_x=test_x\n",
    "            self.test_y=test_y        \n",
    "\n",
    "            print(\"done\")\n",
    "            \n",
    "        else:\n",
    "            import pandas as pd\n",
    "            import numpy as np\n",
    "            from PIL import Image\n",
    "            from pyts.visualization import plot_gasf\n",
    "            import matplotlib.pyplot as plt\n",
    "            import torch\n",
    "            from torch.autograd import Variable\n",
    "\n",
    "            if is_whole==True:\n",
    "                whole=\"_all\"\n",
    "            else:\n",
    "                whole=\"\"\n",
    "            cross=[\"1\",\"2\",\"3\",\"4\",\"5\"]\n",
    "#             remove \"1\"\n",
    "            cross.remove(str(cross_number_for_test))\n",
    "            x = [  \n",
    "                pd.read_json(\"normal\"+whole+\"_cross_\"+cross[0]+\".json\"),\n",
    "                pd.read_json(\"normal\"+whole+\"_cross_\"+cross[1]+\".json\"),\n",
    "                pd.read_json(\"normal\"+whole+\"_cross_\"+cross[2]+\".json\"),\n",
    "                pd.read_json(\"normal\"+whole+\"_cross_\"+cross[3]+\".json\"),\n",
    "                pd.read_json(\"abnormal\"+whole+\"_cross_\"+cross[0]+\".json\"),\n",
    "                pd.read_json(\"abnormal\"+whole+\"_cross_\"+cross[1]+\".json\"),\n",
    "                pd.read_json(\"abnormal\"+whole+\"_cross_\"+cross[2]+\".json\"),\n",
    "                pd.read_json(\"abnormal\"+whole+\"_cross_\"+cross[3]+\".json\")\n",
    "            ]\n",
    "            x = pd.concat(x)\n",
    "            x =  x.sample(frac=1)\n",
    "\n",
    "\n",
    "            def pil_loader(path):\n",
    "                with open(path, 'rb') as f:\n",
    "                    img = Image.open(f)\n",
    "                    return img.convert('L')\n",
    "            lst = list()\n",
    "            count=0\n",
    "            lst2=list()\n",
    "            y=list()\n",
    "\n",
    "            for i in x.batch.unique():\n",
    "                df=x[x.batch==i]\n",
    "                lst = list()\n",
    "                if np.all(df.status==\"abnormal\"):\n",
    "                    path=\"2_wafer_img_cross\"+whole+\"/\"+method+\"/\"+str(df.iloc[0].cross)+\"/abnormal/\"\n",
    "                else:\n",
    "                    path=\"2_wafer_img_cross\"+whole+\"/\"+method+\"/\"+str(df.iloc[0].cross)+\"/normal/\"\n",
    "                for j in x.sensor.unique():\n",
    "                    img=pil_loader(path+str(i)+\"_\"+str(j)+\".png\")\n",
    "                    lst.append(np.array(img)/255)\n",
    "\n",
    "                arr = np.array(lst)\n",
    "                if df.iloc[0].status==\"normal\":\n",
    "                    y.append(0)\n",
    "                else:\n",
    "                    y.append(1)\n",
    "\n",
    "                lst2.append(arr)    \n",
    "            #     count+=1\n",
    "            #     if count==1:\n",
    "            #         break\n",
    "            train_x = torch.from_numpy(np.array(lst2)).float()\n",
    "            train_y = torch.LongTensor(np.array(y))\n",
    "\n",
    "            train_data = Dataset(train_x,train_y)\n",
    "            train_loader = DataLoader(dataset=train_data,\n",
    "                                      batch_size=24,\n",
    "                                      shuffle=True,\n",
    "                                      num_workers=2)\n",
    "            self.train_x=train_x\n",
    "            self.train_y=train_y\n",
    "            self.train_loader=train_loader\n",
    "\n",
    "            #test data set\n",
    "            normal   = pd.read_json(\"normal\"+whole+\"_cross_\"+str(cross_number_for_test)+\".json\")\n",
    "            abnormal = pd.read_json(\"abnormal\"+whole+\"_cross_\"+str(cross_number_for_test)+\".json\")\n",
    "            x=normal.append(abnormal)\n",
    "            x=x.sample(frac=1)\n",
    "\n",
    "            # print(x)\n",
    "            lst = list()\n",
    "            count=0\n",
    "            lst2=list()\n",
    "            y=list()\n",
    "            path=\"\"\n",
    "            for i in x.batch.unique():\n",
    "                df=x[x.batch==i]\n",
    "                lst = list()\n",
    "                if np.all(df.status==\"abnormal\"):\n",
    "                    path=\"2_wafer_img_cross\"+whole+\"/\"+method+\"/\"+str(df.iloc[0].cross)+\"/abnormal/\"\n",
    "                else:\n",
    "                    path=\"2_wafer_img_cross\"+whole+\"/\"+method+\"/\"+str(df.iloc[0].cross)+\"/normal/\"\n",
    "\n",
    "                for j in x.sensor.unique():\n",
    "                    img=pil_loader(path+str(i)+\"_\"+str(j)+\".png\")\n",
    "                    lst.append(np.array(img)/255)\n",
    "                    \n",
    "                arr = np.array(lst)\n",
    "                if df.iloc[0].status==\"normal\":\n",
    "                    y.append(0)\n",
    "                else:\n",
    "                    y.append(1)\n",
    "\n",
    "                lst2.append(arr)    \n",
    "            #     if count==1:\n",
    "            #         break\n",
    "            test_x = Variable(torch.from_numpy(np.array(lst2)).float()).cuda()\n",
    "            test_y = torch.LongTensor(np.array(y)).cuda()\n",
    "            self.test_x=test_x\n",
    "            self.test_y=test_y        \n",
    "\n",
    "            print(\"done\")\n",
    "            \n",
    "        \n",
    "print(\"Function setting done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one chaneel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function setting done\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self,train_x,train_y):\n",
    "        self.train_x=train_x\n",
    "        self.train_y=train_y\n",
    "        self.data_len = len(train_x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        train_x=self.train_x[index]\n",
    "        train_y=self.train_y[index]\n",
    "        return (train_x, train_y)\n",
    "    def __len__(self):\n",
    "        return self.data_len\n",
    "    \n",
    "class dataset_dataloader:\n",
    "    def __init__ (self,is_whole,is_cross,method,cross_number_for_test=1,batchsize=24,numworkers=2):\n",
    "        if is_cross==False:\n",
    "            import pandas as pd\n",
    "            import numpy as np\n",
    "            from PIL import Image\n",
    "            from pyts.visualization import plot_gasf\n",
    "            import matplotlib.pyplot as plt\n",
    "            import torch\n",
    "            from torch.autograd import Variable\n",
    "\n",
    "            if is_whole==True:\n",
    "                whole=\"_all\"\n",
    "            else:\n",
    "                whole=\"\"\n",
    "\n",
    "            normal   = pd.read_json(\"normal_train\"+whole+\".json\")\n",
    "            abnormal = pd.read_json(\"abnormal_train\"+whole+\".json\")\n",
    "            x=normal.append(abnormal)\n",
    "            x=x.sample(frac=1)\n",
    "\n",
    "            def pil_loader(path):\n",
    "                with open(path, 'rb') as f:\n",
    "                    img = Image.open(f)\n",
    "                    return img.convert('L')\n",
    "            lst = list()\n",
    "            count=0\n",
    "            lst2=list()\n",
    "            y=list()\n",
    "\n",
    "            for i in x.batch.unique():\n",
    "                df=x[x.batch==i]\n",
    "                lst = list()\n",
    "                if np.all(df.status==\"abnormal\"):\n",
    "                    path=\"wafer_img\"+whole+\"/\"+method+\"/train/abnormal/\"\n",
    "                else:\n",
    "                    path=\"wafer_img\"+whole+\"/\"+method+\"/train/normal/\"\n",
    "\n",
    "                list_im = [\n",
    "                    path+str(i)+'_6.png', \n",
    "                    path+str(i)+'_7.png', \n",
    "                    path+str(i)+'_8.png',                     \n",
    "                    path+str(i)+'_11.png', \n",
    "                    path+str(i)+'_12.png', \n",
    "                    path+str(i)+'_15.png', \n",
    "                ]\n",
    "                imgs = [ pil_loader(j) for j in list_im ]\n",
    "                min_shape = sorted( [(np.sum(j.size), j.size ) for j in imgs])[0][1]\n",
    "                imgs_comb = np.vstack( (np.asarray( j.resize(min_shape) ) for j in imgs ) )\n",
    "                imgs_comb = Image.fromarray( imgs_comb)\n",
    "                lst.append(np.array(imgs_comb)/255)\n",
    "\n",
    "                arr = np.array(lst)\n",
    "                if df.iloc[0].status==\"normal\":\n",
    "                    y.append(0)\n",
    "                else:\n",
    "                    y.append(1)\n",
    "\n",
    "                lst2.append(arr)    \n",
    "            #     count+=1\n",
    "            #     if count==1:\n",
    "            #         break\n",
    "            train_x = torch.from_numpy(np.array(lst2)).float()\n",
    "            train_y = torch.LongTensor(np.array(y))\n",
    "\n",
    "            train_data = Dataset(train_x,train_y)\n",
    "            train_loader = DataLoader(dataset=train_data,\n",
    "                                      batch_size=24,\n",
    "                                      shuffle=True,\n",
    "                                      num_workers=2)\n",
    "            self.train_x=train_x\n",
    "            self.train_y=train_y\n",
    "            self.train_loader=train_loader\n",
    "\n",
    "            #test data set\n",
    "            normal   = pd.read_json(\"normal_validation\"+whole+\".json\")\n",
    "            abnormal = pd.read_json(\"abnormal_validation\"+whole+\".json\")\n",
    "            x=normal.append(abnormal)\n",
    "            x=x.sample(frac=1)\n",
    "\n",
    "            # print(x)\n",
    "            lst = list()\n",
    "            count=0\n",
    "            lst2=list()\n",
    "            y=list()\n",
    "            path=\"\"\n",
    "            for i in x.batch.unique():\n",
    "                df=x[x.batch==i]\n",
    "                lst = list()\n",
    "                if np.all(df.status==\"abnormal\"):\n",
    "                    path=\"wafer_img\"+whole+\"/\"+method+\"/validation/abnormal/\"\n",
    "                else:\n",
    "                    path=\"wafer_img\"+whole+\"/\"+method+\"/validation/normal/\"\n",
    "\n",
    "                list_im = [\n",
    "                    path+str(i)+'_6.png', \n",
    "                    path+str(i)+'_7.png', \n",
    "                    path+str(i)+'_8.png',                     \n",
    "                    path+str(i)+'_11.png', \n",
    "                    path+str(i)+'_12.png', \n",
    "                    path+str(i)+'_15.png', \n",
    "                ]\n",
    "                imgs = [ pil_loader(i) for i in list_im ]\n",
    "                min_shape = sorted( [(np.sum(i.size), i.size ) for i in imgs])[0][1]\n",
    "                imgs_comb = np.vstack( (np.asarray( i.resize(min_shape) ) for i in imgs ) )\n",
    "                imgs_comb = Image.fromarray( imgs_comb)\n",
    "                lst.append(np.array(imgs_comb)/255)\n",
    "                arr = np.array(lst)\n",
    "                if df.iloc[0].status==\"normal\":\n",
    "                    y.append(0)\n",
    "                else:\n",
    "                    y.append(1)\n",
    "\n",
    "                lst2.append(arr)    \n",
    "            #     if count==1:\n",
    "            #         break\n",
    "            test_x = Variable(torch.from_numpy(np.array(lst2)).float()).cuda()\n",
    "            test_y = torch.LongTensor(np.array(y)).cuda()\n",
    "            self.test_x=test_x\n",
    "            self.test_y=test_y        \n",
    "\n",
    "            print(\"done\")\n",
    "            \n",
    "        else:\n",
    "            import pandas as pd\n",
    "            import numpy as np\n",
    "            from PIL import Image\n",
    "            from pyts.visualization import plot_gasf\n",
    "            import matplotlib.pyplot as plt\n",
    "            import torch\n",
    "            from torch.autograd import Variable\n",
    "\n",
    "            if is_whole==True:\n",
    "                whole=\"_all\"\n",
    "            else:\n",
    "                whole=\"\"\n",
    "            cross=[\"1\",\"2\",\"3\",\"4\",\"5\"]\n",
    "            cross.remove(str(cross_number_for_test))\n",
    "            x = [  \n",
    "                pd.read_json(\"normal\"+whole+\"_cross_\"+cross[0]+\".json\"),\n",
    "                pd.read_json(\"normal\"+whole+\"_cross_\"+cross[1]+\".json\"),\n",
    "                pd.read_json(\"normal\"+whole+\"_cross_\"+cross[2]+\".json\"),\n",
    "                pd.read_json(\"normal\"+whole+\"_cross_\"+cross[3]+\".json\"),\n",
    "                pd.read_json(\"abnormal\"+whole+\"_cross_\"+cross[0]+\".json\"),\n",
    "                pd.read_json(\"abnormal\"+whole+\"_cross_\"+cross[1]+\".json\"),\n",
    "                pd.read_json(\"abnormal\"+whole+\"_cross_\"+cross[2]+\".json\"),\n",
    "                pd.read_json(\"abnormal\"+whole+\"_cross_\"+cross[3]+\".json\")\n",
    "            ]\n",
    "            x = pd.concat(x)\n",
    "            x =  x.sample(frac=1)\n",
    "\n",
    "\n",
    "            def pil_loader(path):\n",
    "                with open(path, 'rb') as f:\n",
    "                    img = Image.open(f)\n",
    "                    return img.convert('L')\n",
    "            lst = list()\n",
    "            count=0\n",
    "            lst2=list()\n",
    "            y=list()\n",
    "\n",
    "            for i in x.batch.unique():\n",
    "                df=x[x.batch==i]\n",
    "                lst = list()\n",
    "                if np.all(df.status==\"abnormal\"):\n",
    "                    path=\"wafer_img_cross\"+whole+\"/\"+method+\"/\"+str(df.iloc[0].cross)+\"/abnormal/\"\n",
    "                else:\n",
    "                    path=\"wafer_img_cross\"+whole+\"/\"+method+\"/\"+str(df.iloc[0].cross)+\"/normal/\"\n",
    "\n",
    "                list_im = [\n",
    "                    path+str(i)+'_6.png', \n",
    "                    path+str(i)+'_7.png', \n",
    "                    path+str(i)+'_8.png',                     \n",
    "                    path+str(i)+'_11.png', \n",
    "                    path+str(i)+'_12.png', \n",
    "                    path+str(i)+'_15.png', \n",
    "                ]\n",
    "                imgs = [ pil_loader(i) for i in list_im ]\n",
    "                min_shape = sorted( [(np.sum(i.size), i.size ) for i in imgs])[0][1]\n",
    "                imgs_comb = np.vstack( (np.asarray( i.resize(min_shape) ) for i in imgs ) )\n",
    "                imgs_comb = Image.fromarray( imgs_comb)\n",
    "                lst.append(np.array(imgs_comb)/255)\n",
    "\n",
    "                arr = np.array(lst)\n",
    "                if df.iloc[0].status==\"normal\":\n",
    "                    y.append(0)\n",
    "                else:\n",
    "                    y.append(1)\n",
    "\n",
    "                lst2.append(arr)    \n",
    "            #     count+=1\n",
    "            #     if count==1:\n",
    "            #         break\n",
    "            train_x = torch.from_numpy(np.array(lst2)).float()\n",
    "            train_y = torch.LongTensor(np.array(y))\n",
    "\n",
    "            train_data = Dataset(train_x,train_y)\n",
    "            train_loader = DataLoader(dataset=train_data,\n",
    "                                      batch_size=24,\n",
    "                                      shuffle=True,\n",
    "                                      num_workers=2)\n",
    "            self.train_x=train_x\n",
    "            self.train_y=train_y\n",
    "            self.train_loader=train_loader\n",
    "\n",
    "            #test data set\n",
    "            normal   = pd.read_json(\"normal\"+whole+\"_cross_\"+str(cross_number_for_test)+\".json\")\n",
    "            abnormal = pd.read_json(\"abnormal\"+whole+\"_cross_\"+str(cross_number_for_test)+\".json\")\n",
    "            x=normal.append(abnormal)\n",
    "            x=x.sample(frac=1)\n",
    "\n",
    "            # print(x)\n",
    "            lst = list()\n",
    "            count=0\n",
    "            lst2=list()\n",
    "            y=list()\n",
    "            path=\"\"\n",
    "            for i in x.batch.unique():\n",
    "                df=x[x.batch==i]\n",
    "                lst = list()\n",
    "                if np.all(df.status==\"abnormal\"):\n",
    "                    path=\"wafer_img_cross\"+whole+\"/\"+method+\"/\"+str(df.iloc[0].cross)+\"/abnormal/\"\n",
    "                else:\n",
    "                    path=\"wafer_img_cross\"+whole+\"/\"+method+\"/\"+str(df.iloc[0].cross)+\"/normal/\"\n",
    "\n",
    "                list_im = [\n",
    "                    path+str(i)+'_6.png', \n",
    "                    path+str(i)+'_7.png', \n",
    "                    path+str(i)+'_8.png',                     \n",
    "                    path+str(i)+'_11.png', \n",
    "                    path+str(i)+'_12.png', \n",
    "                    path+str(i)+'_15.png', \n",
    "                ]\n",
    "                imgs = [ pil_loader(i) for i in list_im ]\n",
    "                min_shape = sorted( [(np.sum(i.size), i.size ) for i in imgs])[0][1]\n",
    "                imgs_comb = np.vstack( (np.asarray( i.resize(min_shape) ) for i in imgs ) )\n",
    "                imgs_comb = Image.fromarray( imgs_comb)\n",
    "                lst.append(np.array(imgs_comb)/255)\n",
    "                arr = np.array(lst)\n",
    "                if df.iloc[0].status==\"normal\":\n",
    "                    y.append(0)\n",
    "                else:\n",
    "                    y.append(1)\n",
    "\n",
    "                lst2.append(arr)    \n",
    "            #     if count==1:\n",
    "            #         break\n",
    "            test_x = Variable(torch.from_numpy(np.array(lst2)).float()).cuda()\n",
    "            test_y = torch.LongTensor(np.array(y)).cuda()\n",
    "            self.test_x=test_x\n",
    "            self.test_y=test_y        \n",
    "\n",
    "            print(\"done\")\n",
    "            \n",
    "        \n",
    "print(\"Function setting done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=dataset_dataloader(True,True,\"mtf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.train_x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __init__ (self,is_whole,is_cross,method,cross_number_for_test=1,batchsize=24,numworkers=2):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#f47641\">Cnn start</font>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [24 x 19220], m2: [20480 x 2] at c:\\a\\w\\1\\s\\tmp_conda_3.7_183942\\conda\\conda-bld\\pytorch_1549564939537\\work\\aten\\src\\thc\\generic/THCTensorMathBlas.cu:266",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-7c34b7511c25>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     73\u001b[0m             \u001b[0mb_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# batch y\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;31m#             print(\"b_x.size:\",b_x.size())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m             \u001b[1;31m# cnn output\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_y\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# cross entropy loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m           \u001b[1;31m# clear gradients for this training step\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-7c34b7511c25>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;31m#             x=self.conv3(x)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#將三維轉二維 (batch , 32*7*7)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m             \u001b[0moutput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1350\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1351\u001b[0m         \u001b[1;31m# fused op is marginally faster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1352\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unwrap_optional\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1353\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1354\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: size mismatch, m1: [24 x 19220], m2: [20480 x 2] at c:\\a\\w\\1\\s\\tmp_conda_3.7_183942\\conda\\conda-bld\\pytorch_1549564939537\\work\\aten\\src\\thc\\generic/THCTensorMathBlas.cu:266"
     ]
    }
   ],
   "source": [
    "### import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "accuracy_matrix =[]\n",
    "import dataset\n",
    "import dataset_six\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    min_error=100\n",
    "    min_lose=100\n",
    "    count=0\n",
    "    number=1\n",
    "#     cross=dataset.dataset_dataloader(False,True,\"mtf\",i+1)\n",
    "    cross=dataset_six.dataset_dataloader(True,True,\"mtf\",i+1)\n",
    "    import torch.nn as nn\n",
    "\n",
    "    EPOCH = 50              # train the training data n times, to save time, we just train 1 epoch\n",
    "    LR = 0.0023\n",
    "    \n",
    "    class CNN(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(CNN, self).__init__()\n",
    "            self.conv1 =nn.Sequential( #卷基層   \n",
    "                nn.Conv2d(\n",
    "                    in_channels  = 6  , # 圖片是有幾層的 若 RGB三層 灰階 1層\n",
    "                    out_channels = 10, # 同時有幾個filter 進行掃描 會提取擠個特徵 代表下一層高度為擠\n",
    "                    kernel_size  = 5  , # 一次畫出來的框 畫幾格 ex 5*5\n",
    "                    stride       = 1  , # 每一個框框跳幾格\n",
    "                    padding      = 2, # 在 5x5逐步掃描後會有少的格子 因此2代表在外面為兩圈的0\n",
    "                                        # if stride =1 padding = (kernel_size-1)/2\n",
    "                ),  # 過濾器 卷基層 蒐集圖片訊息 三維的空間  \n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2), #找出更重要的特徵  像是在2x2中 找到 最大的值\n",
    "            )                                # -> (16,64,64)\n",
    "            self.conv2 = nn.Sequential(\n",
    "                nn.Conv2d(10,20,5,1,2), # 輸入的是上一層的16 把輸出層在變大因此是32 其他不變\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2)             # -> (16,32,32)\n",
    "            )\n",
    "#             self.conv3 = nn.Sequential(\n",
    "#                 nn.Conv2d(5,10,5,1,2), # 輸入的是上一層的16 把輸出層在變大因此是32 其他不變\n",
    "#                 nn.ReLU(),\n",
    "#                 nn.MaxPool2d(2)\n",
    "#             )\n",
    "            self.FC = nn.Sequential(\n",
    "                nn.Linear(20*32*32,20),\n",
    "#                 nn.Dropout(0.2),\n",
    "#                 nn.Linear(20,2)\n",
    "            )\n",
    "        def forward(self,x):\n",
    "\n",
    "            x=self.conv1(x)\n",
    "            x=self.conv2(x) #(batch.32.7.7)\n",
    "#             x=self.conv3(x)\n",
    "            x=x.view(x.size(0),-1) #將三維轉二維 (batch_size , 32*7*7)\n",
    "            \n",
    "            output=self.FC(x)\n",
    "            return output,x\n",
    "\n",
    "    cnn=CNN()\n",
    "    cnn.cuda()\n",
    "#     print(cnn)\n",
    "\n",
    "    optimizer = torch.optim.Adam(cnn.parameters(), lr=LR)   # optimize all cnn parameters\n",
    "    loss_func = nn.CrossEntropyLoss()                       # the target label is not one-hotted\n",
    "\n",
    "    # following function (plot_with_labels) is for visualization, can be ignored if not interested\n",
    "\n",
    "    # training and testing\n",
    "\n",
    "\n",
    "    for epoch in range(EPOCH):\n",
    "        for step, (x, y) in enumerate(cross.train_loader):   # gives batch data, normalize x when iterate train_loader\n",
    "            print(step)\n",
    "            b_x = Variable(x).cuda()   # batch x\n",
    "            b_y = Variable(y).cuda()   # batch y\n",
    "#             print(\"b_x.size:\",b_x.size())\n",
    "            output = cnn(b_x)[0]             # cnn output\n",
    "            loss = loss_func(output, b_y)   # cross entropy loss\n",
    "            optimizer.zero_grad()           # clear gradients for this training step\n",
    "            loss.backward()                 # backpropagation, compute gradients\n",
    "            optimizer.step()                # apply gradients\n",
    "            test_output, last_layer = cnn(cross.test_x)\n",
    "            pred_y = torch.max(test_output, 1)[1].cuda().data.squeeze()\n",
    "            accuracy = sum(pred_y == cross.test_y).item() / float(cross.test_y.size(0))\n",
    "            error  = (1-accuracy)*100\n",
    "            \n",
    "            if error<min_error:\n",
    "                min_error=error\n",
    "                min_lose=100\n",
    "                count=0\n",
    "            else:\n",
    "                count+=1\n",
    "                print(\".\",end=\"\")\n",
    "\n",
    "            if error == min_error:\n",
    "                if loss.data.item()<min_lose:\n",
    "                    min_lose=loss.data.item()\n",
    "#                     torch.save(cnn,'cnn_save/cnn'+str(number)+'.pkl')\n",
    "                    number+=1\n",
    "    #                 print('save!!')\n",
    "    #                 print('\\ncount: ',count,'Epoch: ', epoch, '| train loss: %.15f' % loss.data[0], '| min validation error rate: %.20f' % min_error)\n",
    "            if count>=350:\n",
    "                break\n",
    "        if count>=350:\n",
    "            break\n",
    "    print(\"min_error:\",min_error)\n",
    "    # print 10 predictions from test data\n",
    "    # test_output, _ = cnn(test_x)\n",
    "    # pred_y = torch.max(test_output, 1)[1].cuda().data.squeeze()\n",
    "\n",
    "    # print(pred_y, 'prediction number')\n",
    "    # print(test_y, 'real number')\n",
    "    accuracy_matrix.append(min_error)\n",
    "    print('\\n number: ',number-1 ,min_error)\n",
    "    \n",
    "import IPython.display as ipd\n",
    "import numpy\n",
    "sr = 22050 # sample rate\n",
    "T = 0.3    # seconds\n",
    "t = numpy.linspace(0, T, int(T*sr), endpoint=False) # time variable\n",
    "x = 0.5*numpy.sin(2*numpy.pi*490*t)                # pure sine wave at 440 284Hz\n",
    "ipd.Audio(x, rate=sr,autoplay=True) # load a NumPy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.MaxPool2d?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "accuracy_matrix =[]\n",
    "import dataset\n",
    "# import dataset_ecg_four\n",
    "import dataset_four\n",
    "import time\n",
    "\n",
    "whole=[True]\n",
    "method=[\"gasf\"]\n",
    "\n",
    "F = open('method_2_ROD.txt', 'w')  \n",
    "# F.write(\"method,whole,experiments,r1,r2,r3,r4,r5,average \\n\")\n",
    "for l in method:\n",
    "    for m in whole:\n",
    "        for k in range(20):\n",
    "            accuracy_matrix=[]\n",
    "            start_time = time.time()\n",
    "            for i in range(5):\n",
    "                min_error=100\n",
    "                min_lose=100\n",
    "                count=0\n",
    "                number=1\n",
    "                cross=dataset_four.dataset_dataloader(m,True,l,i+1,batchsize=1,numworkers=2)\n",
    "                import torch.nn as nn\n",
    "\n",
    "                EPOCH = 50              # train the training data n times, to save time, we just train 1 epoch\n",
    "                LR = 0.0023\n",
    "\n",
    "                class CNN(nn.Module):\n",
    "                    def __init__(self):\n",
    "                        super(CNN, self).__init__()\n",
    "                        self.conv1 =nn.Sequential( #卷基層   \n",
    "                            nn.Conv2d(\n",
    "                                in_channels  = 1  , # 圖片是有幾層的 若 RGB三層 灰階 1層\n",
    "                                out_channels = 3, # 同時16個filter 進行掃描 會提取16個特徵 代表下一層高度為16\n",
    "                                kernel_size  = 5  , # 一次畫出來的框 畫幾格 ex 5*5\n",
    "                                stride       = 1  , # 每一個框框跳幾格\n",
    "                                padding      = 2, # 在 5x5逐步掃描後會有少的格子 因此2代表在外面為兩圈的0\n",
    "                                                    # if stride =1 padding = (kernel_size-1)/2\n",
    "                            ),  # 過濾器 卷基層 蒐集圖片訊息 三維的空間  \n",
    "                            nn.ReLU(),\n",
    "                            nn.MaxPool2d(kernel_size=2), #找出更重要的特徵  像是在2x2中 找到 最大的值\n",
    "                        )\n",
    "                        self.conv2 = nn.Sequential(\n",
    "                            nn.Conv2d(3,5,5,1,2), # 輸入的是上一層的16 把輸出層在變大因此是32 其他不變\n",
    "                            nn.ReLU(),\n",
    "                            nn.MaxPool2d(2)\n",
    "                        )\n",
    "            #             self.conv3 = nn.Sequential(\n",
    "            #                 nn.Conv2d(10,20,3,1,1), # 輸入的是上一層的16 把輸出層在變大因此是32 其他不變\n",
    "            #                 nn.ReLU(),\n",
    "            # #                 nn.MaxPool2d(2)\n",
    "            #             )\n",
    "\n",
    "                        self.out = nn.Linear(5*192*128,2)\n",
    "                    def forward(self,x):\n",
    "\n",
    "                        x=self.conv1(x)\n",
    "                        x=self.conv2(x) #(batch.32.7.7)\n",
    "            #             x=self.conv3(x)\n",
    "                        x=x.view(x.size(0),-1) #將三維轉二維 (batch , 32*7*7)\n",
    "                        output=self.out(x)\n",
    "                        return output,x\n",
    "\n",
    "                cnn=CNN()\n",
    "                cnn.cuda()\n",
    "            #     print(cnn)\n",
    "\n",
    "                optimizer = torch.optim.Adam(cnn.parameters(), lr=LR)   # optimize all cnn parameters\n",
    "                loss_func = nn.CrossEntropyLoss()                       # the target label is not one-hotted\n",
    "\n",
    "                # following function (plot_with_labels) is for visualization, can be ignored if not interested\n",
    "\n",
    "                # training and testing\n",
    "\n",
    "\n",
    "                for epoch in range(EPOCH):\n",
    "                    for step, (x, y) in enumerate(cross.train_loader):   # gives batch data, normalize x when iterate train_loader\n",
    "                        b_x = Variable(x).cuda()   # batch x\n",
    "                        b_y = Variable(y).cuda()   # batch y\n",
    "\n",
    "                        output = cnn(b_x)[0]             # cnn output\n",
    "                        loss = loss_func(output, b_y)   # cross entropy loss\n",
    "                        optimizer.zero_grad()           # clear gradients for this training step\n",
    "                        loss.backward()                 # backpropagation, compute gradients\n",
    "                        optimizer.step()                # apply gradients\n",
    "                        test_output, last_layer = cnn(cross.test_x)\n",
    "                        pred_y = torch.max(test_output, 1)[1].cuda().data.squeeze()\n",
    "                        accuracy = sum(pred_y == cross.test_y) / float(cross.test_y.size(0))\n",
    "                        error  = (1-accuracy)*100\n",
    "                        if error<min_error:\n",
    "                            min_error=error\n",
    "                            min_lose=100\n",
    "                            count=0\n",
    "                        else:\n",
    "                            count+=1\n",
    "                            print(\".\",end=\"\")\n",
    "\n",
    "                        if error == min_error:\n",
    "                            if loss.data.item()<min_lose:\n",
    "                                min_lose=loss.data.item()\n",
    "                                torch.save(cnn,'cnn_save/cnn'+str(number)+'.pkl')\n",
    "                                number+=1\n",
    "                #                 print('save!!')\n",
    "                #                 print('\\ncount: ',count,'Epoch: ', epoch, '| train loss: %.15f' % loss.data[0], '| min validation error rate: %.20f' % min_error)\n",
    "                        if count>=500:\n",
    "                            break\n",
    "                    if count>=500:\n",
    "                        break\n",
    "\n",
    "                # print 10 predictions from test data\n",
    "                # test_output, _ = cnn(test_x)\n",
    "                # pred_y = torch.max(test_output, 1)[1].cuda().data.squeeze()\n",
    "\n",
    "                # print(pred_y, 'prediction number')\n",
    "                # print(test_y, 'real number')\n",
    "                accuracy_matrix.append(min_error)\n",
    "                print('\\n number: ',number-1 ,min_error)\n",
    "            import IPython.display as ipd\n",
    "            import numpy\n",
    "            sr = 22050 # sample rate\n",
    "            T = 0.3    # seconds\n",
    "            t = numpy.linspace(0, T, int(T*sr), endpoint=False) # time variable\n",
    "            x = 0.5*numpy.sin(2*numpy.pi*490*t)                # pure sine wave at 440 284Hz\n",
    "\n",
    "            print(accuracy_matrix)\n",
    "            sum(accuracy_matrix) / float(len(accuracy_matrix))\n",
    "            F.write(str(l)+\",\"+str(m)+\",\"+str(k)+\",\")\n",
    "            for i in accuracy_matrix:  \n",
    "                F.write(str(i) + \",\")\n",
    "            F.write(str(sum(accuracy_matrix) / float(len(accuracy_matrix)))+\",\")\n",
    "            F.write(str(time.time() - start_time)+\"\\n\")\n",
    "            print(time.time() - start_time)\n",
    "F.close()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_matrix)\n",
    "sum(accuracy_matrix) / float(len(accuracy_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset_four\n",
    "dataloader=dataset_four.dataset_dataloader(True,False,\"mtf\",1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "view_data = dataloader.train_x[:N_TEST_IMG].view(-1, 768*512).type(torch.FloatTensor)\n",
    "\n",
    "imshow(np.reshape(view_data.numpy()[i], (768, 512)), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, a = plt.subplots(2, N_TEST_IMG, figsize=(100, 20))\n",
    "plt.ion() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import dataset_four\n",
    "import dataset_ecg_four\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# torch.manual_seed(1)    # reproducible\n",
    "\n",
    "# Hyper Parameters\n",
    "EPOCH = 10\n",
    "LR = 0.005         # learning rate\n",
    "DOWNLOAD_MNIST = False\n",
    "N_TEST_IMG = 5\n",
    "\n",
    "\n",
    "# plot one example\n",
    "\n",
    "# Data Loader for easy mini-batch return in training, the image batch shape will be (50, 1, 28, 28)\n",
    "dataloader=dataset_ecg_four.dataset_dataloader(True,False,\"mtf\",1,batchsize=10)\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(256*512, 2048),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 128),   \n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 64), \n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 10), \n",
    "            nn.Tanh(),\n",
    "            nn.Linear(10, 3), \n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(3, 10),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(10, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 512),   \n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 1024), \n",
    "            nn.Tanh(),\n",
    "            nn.Linear(1024, 2048), \n",
    "            nn.Tanh(),\n",
    "            nn.Linear(2048, 256*512), \n",
    "            nn.Sigmoid(),       # compress to a range (0, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "\n",
    "\n",
    "autoencoder = AutoEncoder()\n",
    "\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=LR)\n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "# initialize figure\n",
    "f, a = plt.subplots(2, N_TEST_IMG, figsize=(100, 20))\n",
    "plt.ion()   # continuously plot\n",
    "\n",
    "# original data (first row) for viewing\n",
    "view_data = dataloader.train_x[:N_TEST_IMG].view(-1, 256*512).type(torch.FloatTensor)\n",
    "for i in range(N_TEST_IMG):\n",
    "    a[0][i].imshow(np.reshape(view_data.numpy()[i], (256, 512)), cmap='gray'); a[0][i].set_xticks(()); a[0][i].set_yticks(())\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    for step, (x, b_label) in enumerate(dataloader.train_loader):\n",
    "        b_x = Variable(x.view(-1, 256*512))   # batch x, shape (batch, 28*28)\n",
    "        b_y = Variable(x.view(-1, 256*512))   # batch y, shape (batch, 28*28)\n",
    "\n",
    "        encoded, decoded = autoencoder(b_x)\n",
    "\n",
    "        loss = loss_func(decoded, b_y)      # mean square error\n",
    "        optimizer.zero_grad()               # clear gradients for this training step\n",
    "        loss.backward()                     # backpropagation, compute gradients\n",
    "        optimizer.step()                    # apply gradients\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print('Epoch: ', epoch, '| train loss: %.4f' % loss.data.numpy())\n",
    "\n",
    "#             plotting decoded image (second row)\n",
    "            _, decoded_data = autoencoder(Variable(view_data))\n",
    "            for i in range(N_TEST_IMG):\n",
    "                a[1][i].clear()\n",
    "                a[1][i].imshow(np.reshape(decoded_data.data.numpy()[i], (256, 512)), cmap='gray')\n",
    "                a[1][i].set_xticks(()); a[1][i].set_yticks(())\n",
    "            plt.draw(); plt.pause(0.05)\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize in 3D plot\n",
    "%matplotlib notebook\n",
    "\n",
    "view_data = dataloader.train_x[:200].view(-1, 256*512).type(torch.FloatTensor)\n",
    "encoded_data, _ = autoencoder(Variable(view_data))\n",
    "fig = plt.figure(2); ax = Axes3D(fig)\n",
    "X, Y, Z = encoded_data.data[:, 0].numpy(), encoded_data.data[:, 1].numpy(), encoded_data.data[:, 2].numpy()\n",
    "values = dataloader.train_y[:200].numpy()\n",
    "for x, y, z, s in zip(X, Y, Z, values):\n",
    "    c = cm.rainbow(int(255*s/9)); ax.text(x, y, z, s, backgroundcolor=c)\n",
    "ax.set_xlim(X.min(), X.max()); ax.set_ylim(Y.min(), Y.max()); ax.set_zlim(Z.min(), Z.max())\n",
    "\n",
    "plt.savefig(\"3D.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import Function\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "import random\n",
    "from skimage.transform import resize\n",
    "import dataset_ecg_four\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_sided_padding(x):\n",
    "    rand1 = random.randrange(0,15,3)\n",
    "    rand2 = random.randrange(0,15,3)\n",
    "\n",
    "    zero = np.zeros(shape=[28,28,1])\n",
    "    zero[rand1:rand1+12,rand2:rand2+12,:]=np.asarray(x).reshape(12,12,1)\n",
    "    return zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader=dataset_ecg_four.dataset_dataloader(True,False,\"mtf\",1,batchsize=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
