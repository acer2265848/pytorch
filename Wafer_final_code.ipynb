{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#f47641\">cread detile csv</font>  \n",
    "###  <font color=\"#DAC9A6\"># deal with normalization </font>  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio controls=\"controls\" autoplay=\"autoplay\">\n",
       "                    <source src=\"data:audio/wav;base64,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\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "import numpy\n",
    "sr = 22050 # sample rate\n",
    "T = 0.2    # seconds\n",
    "t = numpy.linspace(0, T, int(T*sr), endpoint=False) # time variable\n",
    "x = 0.5*numpy.sin(2*numpy.pi*490*t)                # pure sine wave at 440 Hz\n",
    "ipd.Audio(x, rate=sr,autoplay=True) # load a NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "# import re\n",
    "# import pandas as pd\n",
    "# from sklearn import preprocessing \n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# count = 0\n",
    "# is_normal=[\"normal\",\"abnormal\"]\n",
    "# for a in is_normal:\n",
    "#     df = pd.DataFrame(columns=['batch','sensor','value','status'])\n",
    "#     df[\"value\"]=df[\"value\"].astype(\"float\")\n",
    "#     for filename in (glob.glob('wafer/'+a+'/*')):\n",
    "#         file = pd.read_csv(filename,sep=\"\\t\",header=None)\n",
    "#         batch=re.search(\"1.+?(?=\\.)\",filename)\n",
    "#         sensor=re.search(\"(?<=\\.)(.*)\",filename)\n",
    "#         status=a\n",
    "#         if (sensor.group(0)!=\"ann\" and sensor.group(0)!=\"box\"):\n",
    "#             value = file[1].values\n",
    "#             #normalization\n",
    "#             value = preprocessing.minmax_scale(value,feature_range=(0,100))\n",
    "#             df.loc[count]=[batch.group(0),sensor.group(0),value,status]\n",
    "#         count+=1\n",
    "\n",
    "#     df.to_json(\"wafer_\"+a+\".json\")\n",
    "\n",
    "# #write csv to let the value type in array\n",
    "\n",
    "# print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#f47641\">create train and test dataset csv</font>  \n",
    "###  <font color=\"#DAC9A6\"> # if want change train and test set change seed </font>  \n",
    "###  <font color=\"#DAC9A6\"> # separate data to train test validation        </font>  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import random \n",
    "# import numpy as np\n",
    "\n",
    "# def dataset(is_whole):\n",
    "#     if is_whole==True:\n",
    "#         np.random.seed(1)\n",
    "#         df_normal = pd.read_json(\"wafer_normal.json\")\n",
    "#         df_abnormal = pd.read_json(\"wafer_abnormal.json\")\n",
    "#         np.random.seed(1)\n",
    "#         normal_train =np.random.choice(df_normal.batch.unique(),747,replace=False)\n",
    "#         abnormal_train =np.random.choice(df_abnormal.batch.unique(),89,replace=False)\n",
    "\n",
    "#         df_normal_train=df_normal[df_normal.batch.isin(normal_train)]\n",
    "#         df_normal_left=df_normal[~df_normal.batch.isin(normal_train)]\n",
    "#         normal_validation =np.random.choice(df_normal_left.batch.unique(),160,replace=False)\n",
    "#         df_normal_validation=df_normal_left[df_normal_left.batch.isin(normal_validation)]\n",
    "#         df_normal_test=df_normal_left[~df_normal_left.batch.isin(normal_validation)]\n",
    "\n",
    "#         df_abnormal_train=df_abnormal[df_abnormal.batch.isin(abnormal_train)]\n",
    "#         df_abnormal_left=df_abnormal[~df_abnormal.batch.isin(abnormal_train)]\n",
    "#         abnormal_validation =np.random.choice(df_abnormal_left.batch.unique(),19,replace=False)\n",
    "#         df_abnormal_validation=df_abnormal_left[df_abnormal_left.batch.isin(abnormal_validation)]\n",
    "#         df_abnormal_test=df_abnormal_left[~df_abnormal_left.batch.isin(abnormal_validation)]\n",
    "\n",
    "\n",
    "#         df_normal_train.to_json(\"normal_train_all.json\")\n",
    "#         df_abnormal_train.to_json(\"abnormal_train_all.json\")\n",
    "#         df_normal_validation.to_json(\"normal_validation_all.json\")\n",
    "#         df_abnormal_validation.to_json(\"abnormal_validation_all.json\")\n",
    "#         df_normal_test.to_json(\"normal_test_all.json\")\n",
    "#         df_abnormal_test.to_json(\"abnormal_test_all.json\")\n",
    "        \n",
    "#     else: \n",
    "#         np.random.seed(1)\n",
    "#         df_normal = pd.read_json(\"wafer_normal.json\")\n",
    "#         normal_sample =np.random.choice(df_normal.batch.unique(),200,replace=False)\n",
    "#         df_normal_sample=df_normal[df_normal.batch.isin(normal_sample)]\n",
    "\n",
    "#         df_abnormal = pd.read_json(\"wafer_abnormal.json\")\n",
    "\n",
    "#         np.random.seed(1)\n",
    "#         normal_train =np.random.choice(df_normal_sample.batch.unique(),140,replace=False)\n",
    "#         abnormal_train =np.random.choice(df_abnormal.batch.unique(),89,replace=False)\n",
    "\n",
    "#         df_normal_train=df_normal_sample[df_normal_sample.batch.isin(normal_train)]\n",
    "#         df_normal_left=df_normal_sample[~df_normal_sample.batch.isin(normal_train)]\n",
    "#         normal_validation =np.random.choice(df_normal_left.batch.unique(),30,replace=False)\n",
    "#         df_normal_validation=df_normal_left[df_normal_left.batch.isin(normal_validation)]\n",
    "#         df_normal_test=df_normal_left[~df_normal_left.batch.isin(normal_validation)]\n",
    "\n",
    "#         df_abnormal_train=df_abnormal[df_abnormal.batch.isin(abnormal_train)]\n",
    "#         df_abnormal_left=df_abnormal[~df_abnormal.batch.isin(abnormal_train)]\n",
    "#         abnormal_validation =np.random.choice(df_abnormal_left.batch.unique(),19,replace=False)\n",
    "#         df_abnormal_validation=df_abnormal_left[df_abnormal_left.batch.isin(abnormal_validation)]\n",
    "#         df_abnormal_test=df_abnormal_left[~df_abnormal_left.batch.isin(abnormal_validation)]\n",
    "\n",
    "\n",
    "#         df_normal_train.to_json(\"normal_train.json\")\n",
    "#         df_abnormal_train.to_json(\"abnormal_train.json\")\n",
    "#         df_normal_validation.to_json(\"normal_validation.json\")\n",
    "#         df_abnormal_validation.to_json(\"abnormal_validation.json\")\n",
    "#         df_normal_test.to_json(\"normal_test.json\")\n",
    "#         df_abnormal_test.to_json(\"abnormal_test.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#f47641\">create crossvalidation dataset csv</font>  \n",
    "###  <font color=\"#DAC9A6\"> # if want change train and test set change seed </font>  \n",
    "###  <font color=\"#DAC9A6\"> # separate data to 5-fold crossvalidation        </font>  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def dataset_cross(is_whole):\n",
    "#     import pandas as pd\n",
    "#     import random \n",
    "#     import numpy as np\n",
    "#     if is_whole==True:\n",
    "#         np.random.seed(1)\n",
    "#         df_normal = pd.read_json(\"wafer_normal.json\")\n",
    "#         df_abnormal = pd.read_json(\"wafer_abnormal.json\")\n",
    "#         np.random.seed(1)\n",
    "\n",
    "#         normal_coross_1 =np.random.choice(df_normal.batch.unique(),213,replace=False)\n",
    "#         df_normal_cross_1=df_normal[df_normal.batch.isin(normal_coross_1)]\n",
    "#         df_normal_cross_1[\"cross\"]=1\n",
    "#         df_normal_left=df_normal[~df_normal.batch.isin(normal_coross_1)]\n",
    "\n",
    "#         normal_coross_2 =np.random.choice(df_normal_left.batch.unique(),213,replace=False)\n",
    "#         df_normal_cross_2=df_normal_left[df_normal_left.batch.isin(normal_coross_2)]\n",
    "#         df_normal_cross_2[\"cross\"]=2\n",
    "#         df_normal_left=df_normal_left[~df_normal_left.batch.isin(normal_coross_2)]\n",
    "\n",
    "#         normal_coross_3 =np.random.choice(df_normal_left.batch.unique(),214,replace=False)\n",
    "#         df_normal_cross_3=df_normal_left[df_normal_left.batch.isin(normal_coross_3)]\n",
    "#         df_normal_cross_3[\"cross\"]=3\n",
    "#         df_normal_left=df_normal_left[~df_normal_left.batch.isin(normal_coross_3)]\n",
    "\n",
    "#         normal_coross_4 =np.random.choice(df_normal_left.batch.unique(),213,replace=False)\n",
    "#         df_normal_cross_4=df_normal_left[df_normal_left.batch.isin(normal_coross_4)]\n",
    "#         df_normal_cross_4[\"cross\"]=4\n",
    "#         df_normal_cross_5=df_normal_left[~df_normal_left.batch.isin(normal_coross_4)]\n",
    "#         df_normal_cross_5[\"cross\"]=5\n",
    "\n",
    "#         abnormal_coross_1 =np.random.choice(df_abnormal.batch.unique(),25,replace=False)\n",
    "#         df_abnormal_cross_1=df_abnormal[df_abnormal.batch.isin(abnormal_coross_1)]\n",
    "#         df_abnormal_cross_1[\"cross\"]=1\n",
    "#         df_abnormal_left=df_abnormal[~df_abnormal.batch.isin(abnormal_coross_1)]\n",
    "\n",
    "#         abnormal_coross_2 =np.random.choice(df_abnormal_left.batch.unique(),26,replace=False)\n",
    "#         df_abnormal_cross_2=df_abnormal_left[df_abnormal_left.batch.isin(abnormal_coross_2)]\n",
    "#         df_abnormal_cross_2[\"cross\"]=2\n",
    "#         df_abnormal_left=df_abnormal_left[~df_abnormal_left.batch.isin(abnormal_coross_2)]\n",
    "\n",
    "#         abnormal_coross_3 =np.random.choice(df_abnormal_left.batch.unique(),25,replace=False)\n",
    "#         df_abnormal_cross_3=df_abnormal_left[df_abnormal_left.batch.isin(abnormal_coross_3)]\n",
    "#         df_abnormal_cross_3[\"cross\"]=3\n",
    "#         df_abnormal_left=df_abnormal_left[~df_abnormal_left.batch.isin(abnormal_coross_3)]\n",
    "\n",
    "#         abnormal_coross_4 =np.random.choice(df_abnormal_left.batch.unique(),25,replace=False)\n",
    "#         df_abnormal_cross_4=df_abnormal_left[df_abnormal_left.batch.isin(abnormal_coross_4)]\n",
    "#         df_abnormal_cross_4[\"cross\"]=4\n",
    "#         df_abnormal_cross_5=df_abnormal_left[~df_abnormal_left.batch.isin(abnormal_coross_4)]\n",
    "#         df_abnormal_cross_5[\"cross\"]=5\n",
    "\n",
    "\n",
    "#         df_normal_cross_1.to_json(\"normal_all_cross_1.json\")\n",
    "#         df_normal_cross_2.to_json(\"normal_all_cross_2.json\")\n",
    "#         df_normal_cross_3.to_json(\"normal_all_cross_3.json\")\n",
    "#         df_normal_cross_4.to_json(\"normal_all_cross_4.json\")\n",
    "#         df_normal_cross_5.to_json(\"normal_all_cross_5.json\")\n",
    "\n",
    "#         df_abnormal_cross_1.to_json(\"abnormal_all_cross_1.json\")\n",
    "#         df_abnormal_cross_2.to_json(\"abnormal_all_cross_2.json\")\n",
    "#         df_abnormal_cross_3.to_json(\"abnormal_all_cross_3.json\")\n",
    "#         df_abnormal_cross_4.to_json(\"abnormal_all_cross_4.json\")\n",
    "#         df_abnormal_cross_5.to_json(\"abnormal_all_cross_5.json\")\n",
    "\n",
    "#         print(len(df_normal_cross_1),len(df_normal_cross_2),len(df_normal_cross_3),len(df_normal_cross_4),len(df_normal_cross_5))\n",
    "#         print(len(df_abnormal_cross_1),len(df_abnormal_cross_2),len(df_abnormal_cross_3),len(df_abnormal_cross_4),len(df_abnormal_cross_5))\n",
    "#     else:\n",
    "#         np.random.seed(1)\n",
    "#         df_normal = pd.read_json(\"wafer_normal.json\")\n",
    "#         normal_sample =np.random.choice(df_normal.batch.unique(),200,replace=False)\n",
    "#         df_normal_sample=df_normal[df_normal.batch.isin(normal_sample)]\n",
    "#         df_abnormal = pd.read_json(\"wafer_abnormal.json\")\n",
    "#         np.random.seed(1)\n",
    "\n",
    "#         normal_coross_1 =np.random.choice(df_normal_sample.batch.unique(),40,replace=False)\n",
    "#         df_normal_cross_1=df_normal_sample[df_normal_sample.batch.isin(normal_coross_1)]\n",
    "#         df_normal_cross_1[\"cross\"]=1\n",
    "#         df_normal_left=df_normal_sample[~df_normal_sample.batch.isin(normal_coross_1)]\n",
    "\n",
    "#         normal_coross_2 =np.random.choice(df_normal_left.batch.unique(),40,replace=False)\n",
    "#         df_normal_cross_2=df_normal_left[df_normal_left.batch.isin(normal_coross_2)]\n",
    "#         df_normal_cross_2[\"cross\"]=2\n",
    "#         df_normal_left=df_normal_left[~df_normal_left.batch.isin(normal_coross_2)]\n",
    "\n",
    "#         normal_coross_3 =np.random.choice(df_normal_left.batch.unique(),40,replace=False)\n",
    "#         df_normal_cross_3=df_normal_left[df_normal_left.batch.isin(normal_coross_3)]\n",
    "#         df_normal_cross_3[\"cross\"]=3\n",
    "#         df_normal_left=df_normal_left[~df_normal_left.batch.isin(normal_coross_3)]\n",
    "\n",
    "#         normal_coross_4 =np.random.choice(df_normal_left.batch.unique(),40,replace=False)\n",
    "#         df_normal_cross_4=df_normal_left[df_normal_left.batch.isin(normal_coross_4)]\n",
    "#         df_normal_cross_4[\"cross\"]=4\n",
    "#         df_normal_cross_5=df_normal_left[~df_normal_left.batch.isin(normal_coross_4)]\n",
    "#         df_normal_cross_5[\"cross\"]=5\n",
    "\n",
    "#         abnormal_coross_1 =np.random.choice(df_abnormal.batch.unique(),25,replace=False)\n",
    "#         df_abnormal_cross_1=df_abnormal[df_abnormal.batch.isin(abnormal_coross_1)]\n",
    "#         df_abnormal_cross_1[\"cross\"]=1\n",
    "#         df_abnormal_left=df_abnormal[~df_abnormal.batch.isin(abnormal_coross_1)]\n",
    "\n",
    "#         abnormal_coross_2 =np.random.choice(df_abnormal_left.batch.unique(),26,replace=False)\n",
    "#         df_abnormal_cross_2=df_abnormal_left[df_abnormal_left.batch.isin(abnormal_coross_2)]\n",
    "#         df_abnormal_cross_2[\"cross\"]=2\n",
    "#         df_abnormal_left=df_abnormal_left[~df_abnormal_left.batch.isin(abnormal_coross_2)]\n",
    "\n",
    "#         abnormal_coross_3 =np.random.choice(df_abnormal_left.batch.unique(),25,replace=False)\n",
    "#         df_abnormal_cross_3=df_abnormal_left[df_abnormal_left.batch.isin(abnormal_coross_3)]\n",
    "#         df_abnormal_cross_3[\"cross\"]=3\n",
    "#         df_abnormal_left=df_abnormal_left[~df_abnormal_left.batch.isin(abnormal_coross_3)]\n",
    "\n",
    "#         abnormal_coross_4 =np.random.choice(df_abnormal_left.batch.unique(),25,replace=False)\n",
    "#         df_abnormal_cross_4=df_abnormal_left[df_abnormal_left.batch.isin(abnormal_coross_4)]\n",
    "#         df_abnormal_cross_4[\"cross\"]=4\n",
    "#         df_abnormal_cross_5=df_abnormal_left[~df_abnormal_left.batch.isin(abnormal_coross_4)]\n",
    "#         df_abnormal_cross_5[\"cross\"]=5\n",
    "\n",
    "\n",
    "#         df_normal_cross_1.to_json(\"normal_cross_1.json\")\n",
    "#         df_normal_cross_2.to_json(\"normal_cross_2.json\")\n",
    "#         df_normal_cross_3.to_json(\"normal_cross_3.json\")\n",
    "#         df_normal_cross_4.to_json(\"normal_cross_4.json\")\n",
    "#         df_normal_cross_5.to_json(\"normal_cross_5.json\")\n",
    "\n",
    "#         df_abnormal_cross_1.to_json(\"abnormal_cross_1.json\")\n",
    "#         df_abnormal_cross_2.to_json(\"abnormal_cross_2.json\")\n",
    "#         df_abnormal_cross_3.to_json(\"abnormal_cross_3.json\")\n",
    "#         df_abnormal_cross_4.to_json(\"abnormal_cross_4.json\")\n",
    "#         df_abnormal_cross_5.to_json(\"abnormal_cross_5.json\")\n",
    "\n",
    "#         print(len(df_normal_cross_1),len(df_normal_cross_2),len(df_normal_cross_3),len(df_normal_cross_4),len(df_normal_cross_5))\n",
    "#         print(len(df_abnormal_cross_1),len(df_abnormal_cross_2),len(df_abnormal_cross_3),len(df_abnormal_cross_4),len(df_abnormal_cross_5))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_cross(True)\n",
    "# dataset_cross(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#f47641\">Represent to image</font>  \n",
    "###  <font color=\"#DAC9A6\"> # Write in function just need call method </font>  \n",
    "###  <font color=\"#DAC9A6\"> # is_whole : decide call all datasate or sample datasat </font>  \n",
    "###  <font color=\"#DAC9A6\"> # is_cross : decide call crossvalidation dataset or not </font>  \n",
    "\n",
    "###  <font color=\"#DAC9A6\"> # method : \"mtf\",\"gasf\",\"gadf\",\"recurrence_plots\" </font>  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167713_12\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAABOFJREFUeJzt3TFqVdEahuGVGyMGjUERKwutHIW1heOwtLZUS3u7jENwAE7CSgsbQSSJoEKUc4egF/aP15fnqTdfNufwspqwzt5ut1tA03/+9gsAcwQOYQKHMIFDmMAhTOAQJnAIEziECRzCLk2MvnjxYuTf4549ezYxuz5+/Lj55unp6eaba6319u3bkd0ph4eHI7sfPnzYfPPo6GjzzbXWevLkycjuwcHB3u+ecYJDmMAhTOAQJnAIEziECRzCBA5hAocwgUOYwCFM4BAmcAgTOIQJHMIEDmEChzCBQ5jAIUzgECZwCBM4hI3cqvr8+fOJ2TFv3rzZfPPLly+bb6611rt370Z2p0z9/vze3m8vFP2/8fjx45Hdk5OT3z7jBIcwgUOYwCFM4BAmcAgTOIQJHMIEDmEChzCBQ5jAIUzgECZwCBM4hAkcwgQOYQKHMIFDmMAhTOAQJnAIG7lV9c6dOxOz6+HDhyO7Ez59+jSye3Z2NrI7ZX9/f2T39PR0880rV65svrnWWo8ePRrZ/RNOcAgTOIQJHMIEDmEChzCBQ5jAIUzgECZwCBM4hAkcwgQOYQKHMIFDmMAhTOAQJnAIEziECRzCBA5hI5cuHh8fT8yumzdvjuzevn17ZHfC169f//Yr/E8uX748sjtx6eLVq1c331xrrVu3bo3s/gknOIQJHMIEDmEChzCBQ5jAIUzgECZwCBM4hAkcwgQOYQKHMIFDmMAhTOAQJnAIEziECRzCBA5hAocwgUPYyK2qDx48mJhd9+/fH9mduAX22rVrm2/+i759+zaye+/evc03z8/PN99ca62nT5+O7O52u98+4wSHMIFDmMAhTOAQJnAIEziECRzCBA5hAocwgUOYwCFM4BAmcAgTOIQJHMIEDmEChzCBQ5jAIUzgECZwCBM4hAkcwgQOYQKHMIFDmMAhTOAQJnAIEziECRzCBA5hAocwgUOYwCFM4BAmcAgTOIQJHMIEDmEChzCBQ5jAIezSxOj3798nZtfe3t7I7s+fPzffvLi42HxzrbUODw9Hdv81E5/D1Hf2NznBIUzgECZwCBM4hAkcwgQOYQKHMIFDmMAhTOAQJnAIEziECRzCBA5hAocwgUOYwCFM4BAmcAgTOIQJHMJGblW9e/fuxOza7XYjuzdu3Nh88/j4ePPNtdZ6//79yO6UqRt2J5ydnY3sHhwcjOz+CSc4hAkcwgQOYQKHMIFDmMAhTOAQJnAIEziECRzCBA5hAocwgUOYwCFM4BAmcAgTOIQJHMIEDmECh7CRSxfPz88nZsf8+PFj881Ll0Y+2nX9+vWR3SlTFw4eHR2N7E749evXX/vbTnAIEziECRzCBA5hAocwgUOYwCFM4BAmcAgTOIQJHMIEDmEChzCBQ5jAIUzgECZwCBM4hAkcwgQOYQKHsJGrP1+9ejUxu05OTkZ2X79+vfnm58+fN99ca62XL1+O7P5rLi4uNt/c39/ffPNvc4JDmMAhTOAQJnAIEziECRzCBA5hAocwgUOYwCFM4BAmcAgTOIQJHMIEDmEChzCBQ5jAIUzgECZwCBM4hO3tdru//Q7AECc4hAkcwgQOYQKHMIFDmMAhTOAQJnAIEziECRzCBA5hAocwgUOYwCFM4BAmcAgTOIQJHMIEDmEChzCBQ5jAIUzgEPZfuuhye0883FoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyts.visualization import plot_gasf,plot_gadf,plot_mtf,plot_recurrence_plots\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "method=\"gadf\"\n",
    "path=\"_all\"\n",
    "a = \"train\"\n",
    "b=\"normal\"\n",
    "x= pd.read_json(b+\"_\"+a+path+\".json\")\n",
    "i=0\n",
    "x_time=np.array(x.iloc[:,3].values[i])\n",
    "x_class= str(x.iloc[:,0].values[i])+'_'+str(x.iloc[:,1].values[i])\n",
    "print(x_class)\n",
    "locals()[\"plot_\"+method.lower()](x_time,\n",
    "                             cmap=\"gray\",\n",
    "                             image_size=10,\n",
    "                             output_file=\"wafer_img\"+path+\"/\"+method+\"/\"+a+\"/\"+b+\"/\"+x_class+\".png\")\n",
    "\n",
    "x_time=np.array(x.iloc[:,3].values[i])\n",
    "x_class= str(x.iloc[:,0].values[i])+'_'+str(x.iloc[:,1].values[i])\n",
    "img = Image.open(\"wafer_img\"+path+\"/\"+method+\"/\"+a+\"/\"+b+\"/\"+x_class+\".png\")\n",
    "# crop(起始點橫坐標，起始點縱座標，寬，高)\n",
    "img=img.crop((20, 3, 237, 220))\n",
    "img.thumbnail((128,128))  #resize\n",
    "img.save(\"wafer_img\"+path+\"/\"+method+\"/\"+a+\"/\"+b+\"/\"+x_class+\".png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformation(is_whole,is_cross,method):\n",
    "    from pyts.visualization import plot_gasf,plot_gadf,plot_mtf,plot_recurrence_plots\n",
    "    import matplotlib.pyplot as plt\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from PIL import Image\n",
    "\n",
    "    \n",
    "    if is_cross==False:\n",
    "        is_train=[\"train\",\"test\",\"validation\"]\n",
    "        is_normal=[\"normal\",\"abnormal\"]\n",
    "        if is_whole==True:\n",
    "            path=\"_all\"\n",
    "        else:\n",
    "            path=\"\"\n",
    "        for a in is_train:\n",
    "            for b in is_normal:\n",
    "                x= pd.read_json(b+\"_\"+a+path+\".json\")\n",
    "                for i in range(len(x)):\n",
    "                    x_time=np.array(x.iloc[:,3].values[i])\n",
    "                    x_class= str(x.iloc[:,0].values[i])+'_'+str(x.iloc[:,1].values[i])\n",
    "                    if method.lower() != \"recurrence_plots\":\n",
    "\n",
    "                        locals()[\"plot_\"+method.lower()](x_time,\n",
    "                                                 cmap=\"gray\",\n",
    "                                                 image_size=20,\n",
    "                                                 output_file=\"wafer_img\"+path+\"/\"+method+\"/\"+a+\"/\"+b+\"/\"+x_class+\".png\")\n",
    "                    else:\n",
    "                        locals()[\"plot_\"+method.lower()](x_time,\n",
    "                                                 output_file=\"wafer_img\"+path+\"/\"+method+\"/\"+a+\"/\"+b+\"/\"+x_class+\".png\")\n",
    "\n",
    "                for i in range(len(x)):\n",
    "                    x_time=np.array(x.iloc[:,3].values[i])\n",
    "                    x_class= str(x.iloc[:,0].values[i])+'_'+str(x.iloc[:,1].values[i])\n",
    "                    img = Image.open(\"wafer_img\"+path+\"/\"+method+\"/\"+a+\"/\"+b+\"/\"+x_class+\".png\")\n",
    "                    img=img.crop((20, 3, 237, 220))\n",
    "                    img.thumbnail((128,128))  #resize\n",
    "                    img.save(\"wafer_img\"+path+\"/\"+method+\"/\"+a+\"/\"+b+\"/\"+x_class+\".png\")\n",
    "    else:\n",
    "        if is_whole==True:\n",
    "            path=\"_all\"\n",
    "        else:\n",
    "            path=\"\"\n",
    "        cross=[\"1\",\"2\",\"3\",\"4\",\"5\"]\n",
    "        is_normal=[\"normal\",\"abnormal\"]\n",
    "        for a in cross:\n",
    "            for b in is_normal:\n",
    "                x= pd.read_json(b+path+\"_cross_\"+a+\".json\")\n",
    "                for i in range(len(x)):\n",
    "                    x_time=np.array(x.iloc[:,4].values[i])\n",
    "                    x_class= str(x.iloc[:,0].values[i])+'_'+str(x.iloc[:,2].values[i])\n",
    "                    if method.lower() != \"recurrence_plots\":\n",
    "\n",
    "                        locals()[\"plot_\"+method.lower()](x_time,\n",
    "                                                 cmap=\"gray\",\n",
    "                                                 image_size=20,\n",
    "                                                 output_file=\"wafer_img_cross\"+path+\"/\"+method+\"/\"+a+\"/\"+b+\"/\"+x_class+\".png\")\n",
    "                    else:\n",
    "                        locals()[\"plot_\"+method.lower()](x_time,\n",
    "                                                 output_file=\"wafer_img_cross\"+path+\"/\"+method+\"/\"+a+\"/\"+b+\"/\"+x_class+\".png\")\n",
    "\n",
    "                for i in range(len(x)):\n",
    "                    x_time=np.array(x.iloc[:,4].values[i])\n",
    "                    x_class= str(x.iloc[:,0].values[i])+'_'+str(x.iloc[:,2].values[i])\n",
    "                    img = Image.open(\"wafer_img_cross\"+path+\"/\"+method+\"/\"+a+\"/\"+b+\"/\"+x_class+\".png\")\n",
    "                    img=img.crop((20, 3, 237, 220))\n",
    "                    img.thumbnail((128,128))  #resize\n",
    "                    img.save(\"wafer_img_cross\"+path+\"/\"+method+\"/\"+a+\"/\"+b+\"/\"+x_class+\".png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'ts' must be a 1-dimensional np.ndarray.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-fa879a33cb8b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtransformation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"gasf\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtransformation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"gadf\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtransformation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"mtf\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtransformation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"recurrence_plots\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"done whole=true cross=true\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-8f522de3deca>\u001b[0m in \u001b[0;36mtransformation\u001b[1;34m(is_whole, is_cross, method)\u001b[0m\n\u001b[0;32m     56\u001b[0m                                                  \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"gray\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m                                                  \u001b[0mimage_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m                                                  output_file=\"wafer_img_cross\"+path+\"/\"+method+\"/\"+a+\"/\"+b+\"/\"+x_class+\".png\")\n\u001b[0m\u001b[0;32m     59\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m                         locals()[\"plot_\"+method.lower()](x_time,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyts\\visualization\\visualization.py\u001b[0m in \u001b[0;36mplot_gasf\u001b[1;34m(ts, image_size, overlapping, scale, cmap, output_file, interpolation)\u001b[0m\n\u001b[0;32m    519\u001b[0m     \u001b[1;31m# Check input data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"'ts' must be a 1-dimensional np.ndarray.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m     \u001b[1;31m# Size of ts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: 'ts' must be a 1-dimensional np.ndarray."
     ]
    }
   ],
   "source": [
    "transformation(True,True,\"gasf\")\n",
    "transformation(True,True,\"gadf\")\n",
    "transformation(True,True,\"mtf\")\n",
    "transformation(True,True,\"recurrence_plots\")\n",
    "print(\"done whole=true cross=true\")\n",
    "\n",
    "transformation(False,True,\"gasf\")\n",
    "transformation(False,True,\"gadf\")\n",
    "transformation(False,True,\"mtf\")\n",
    "transformation(False,True,\"recurrence_plots\")\n",
    "print(\"done whole=False cross=true\")\n",
    "\n",
    "transformation(True,False,\"gasf\")\n",
    "transformation(True,False,\"gadf\")\n",
    "transformation(True,False,\"mtf\")\n",
    "transformation(True,False,\"recurrence_plots\")\n",
    "print(\"done whole=true cross=False\")\n",
    "\n",
    "transformation(False,False,\"gasf\")\n",
    "transformation(False,False,\"gadf\")\n",
    "transformation(False,False,\"mtf\")\n",
    "transformation(False,False,\"recurrence_plots\")\n",
    "print(\"done whole=False cross=False\")\n",
    "\n",
    "import IPython.display as ipd\n",
    "import numpy\n",
    "sr = 22050 # sample rate\n",
    "T = 0.3    # seconds\n",
    "t = numpy.linspace(0, T, int(T*sr), endpoint=False) # time variable\n",
    "x = 0.5*numpy.sin(2*numpy.pi*490*t)                # pure sine wave at 440 284Hz\n",
    "ipd.Audio(x, rate=sr,autoplay=True) # load a NumPy array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#f47641\">create dataset & dataloader</font>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## six channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function setting done\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self,train_x,train_y):\n",
    "        self.train_x=train_x\n",
    "        self.train_y=train_y\n",
    "        self.data_len = len(train_x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        train_x=self.train_x[index]\n",
    "        train_y=self.train_y[index]\n",
    "        return (train_x, train_y)\n",
    "    def __len__(self):\n",
    "        return self.data_len\n",
    "    \n",
    "class dataset_dataloader:\n",
    "    def __init__ (self,is_whole,is_cross,method,cross_number_for_test=1,batchsize=24,numworkers=2):\n",
    "        if is_cross==False:\n",
    "            import pandas as pd\n",
    "            import numpy as np\n",
    "            from PIL import Image\n",
    "            from pyts.visualization import plot_gasf\n",
    "            import matplotlib.pyplot as plt\n",
    "            import torch\n",
    "            from torch.autograd import Variable\n",
    "\n",
    "            if is_whole==True:\n",
    "                whole=\"_all\"\n",
    "            else:\n",
    "                whole=\"\"\n",
    "\n",
    "            normal   = pd.read_json(\"normal_train\"+whole+\".json\")\n",
    "            abnormal = pd.read_json(\"abnormal_train\"+whole+\".json\")\n",
    "            x=normal.append(abnormal)\n",
    "            x=x.sample(frac=1)\n",
    "\n",
    "            def pil_loader(path):\n",
    "                with open(path, 'rb') as f:\n",
    "                    img = Image.open(f)\n",
    "#                   轉成灰階\n",
    "                    return img.convert('L')\n",
    "            lst = list()\n",
    "            count=0\n",
    "            lst2=list()\n",
    "            y=list()\n",
    "\n",
    "            for i in x.batch.unique():\n",
    "                df=x[x.batch==i]\n",
    "                lst = list()\n",
    "                if np.all(df.status==\"abnormal\"):\n",
    "                    path=\"wafer_img\"+whole+\"/\"+method+\"/train/abnormal/\"\n",
    "                else:\n",
    "                    path=\"wafer_img\"+whole+\"/\"+method+\"/train/normal/\"\n",
    "\n",
    "                for j in x.sensor.unique():\n",
    "                    img=pil_loader(path+str(i)+\"_\"+str(j)+\".png\")\n",
    "                    lst.append(np.array(img)/255)\n",
    "                    \n",
    "                arr = np.array(lst)\n",
    "                if df.iloc[0].status==\"normal\":\n",
    "                    y.append(0)\n",
    "                else:\n",
    "                    y.append(1)\n",
    "\n",
    "                lst2.append(arr)    \n",
    "            #     count+=1\n",
    "            #     if count==1:\n",
    "            #         break\n",
    "            train_x = torch.from_numpy(np.array(lst2)).float()\n",
    "            train_y = torch.LongTensor(np.array(y))\n",
    "\n",
    "            train_data = Dataset(train_x,train_y)\n",
    "            train_loader = DataLoader(dataset=train_data,\n",
    "                                      batch_size=24,\n",
    "                                      shuffle=True,\n",
    "                                      num_workers=2)\n",
    "            self.train_x=train_x\n",
    "            self.train_y=train_y\n",
    "            self.train_loader=train_loader\n",
    "\n",
    "            #test data set\n",
    "            normal   = pd.read_json(\"normal_validation\"+whole+\".json\")\n",
    "            abnormal = pd.read_json(\"abnormal_validation\"+whole+\".json\")\n",
    "            x=normal.append(abnormal)\n",
    "            x=x.sample(frac=1)\n",
    "\n",
    "            # print(x)\n",
    "            lst = list()\n",
    "            count=0\n",
    "            lst2=list()\n",
    "            y=list()\n",
    "            path=\"\"\n",
    "            for i in x.batch.unique():\n",
    "                df=x[x.batch==i]\n",
    "                lst = list()\n",
    "                if np.all(df.status==\"abnormal\"):\n",
    "                    path=\"wafer_img\"+whole+\"/\"+method+\"/validation/abnormal/\"\n",
    "                else:\n",
    "                    path=\"wafer_img\"+whole+\"/\"+method+\"/validation/normal/\"\n",
    "\n",
    "                for j in x.sensor.unique():\n",
    "                    img=pil_loader(path+str(i)+\"_\"+str(j)+\".png\")\n",
    "                    lst.append(np.array(img)/255)\n",
    "                    \n",
    "                arr = np.array(lst)\n",
    "                if df.iloc[0].status==\"normal\":\n",
    "                    y.append(0)\n",
    "                else:\n",
    "                    y.append(1)\n",
    "\n",
    "                lst2.append(arr)    \n",
    "            #     if count==1:\n",
    "            #         break\n",
    "            test_x = Variable(torch.from_numpy(np.array(lst2)).float()).cuda()\n",
    "            test_y = torch.LongTensor(np.array(y)).cuda()\n",
    "            self.test_x=test_x\n",
    "            self.test_y=test_y        \n",
    "\n",
    "            print(\"done\")\n",
    "            \n",
    "        else:\n",
    "            import pandas as pd\n",
    "            import numpy as np\n",
    "            from PIL import Image\n",
    "            from pyts.visualization import plot_gasf\n",
    "            import matplotlib.pyplot as plt\n",
    "            import torch\n",
    "            from torch.autograd import Variable\n",
    "\n",
    "            if is_whole==True:\n",
    "                whole=\"_all\"\n",
    "            else:\n",
    "                whole=\"\"\n",
    "            cross=[\"1\",\"2\",\"3\",\"4\",\"5\"]\n",
    "            cross.remove(str(cross_number_for_test))\n",
    "            x = [  \n",
    "                pd.read_json(\"normal\"+whole+\"_cross_\"+cross[0]+\".json\"),\n",
    "                pd.read_json(\"normal\"+whole+\"_cross_\"+cross[1]+\".json\"),\n",
    "                pd.read_json(\"normal\"+whole+\"_cross_\"+cross[2]+\".json\"),\n",
    "                pd.read_json(\"normal\"+whole+\"_cross_\"+cross[3]+\".json\"),\n",
    "                pd.read_json(\"abnormal\"+whole+\"_cross_\"+cross[0]+\".json\"),\n",
    "                pd.read_json(\"abnormal\"+whole+\"_cross_\"+cross[1]+\".json\"),\n",
    "                pd.read_json(\"abnormal\"+whole+\"_cross_\"+cross[2]+\".json\"),\n",
    "                pd.read_json(\"abnormal\"+whole+\"_cross_\"+cross[3]+\".json\")\n",
    "            ]\n",
    "            x = pd.concat(x)\n",
    "            x =  x.sample(frac=1)\n",
    "\n",
    "\n",
    "            def pil_loader(path):\n",
    "                with open(path, 'rb') as f:\n",
    "                    img = Image.open(f)\n",
    "                    return img.convert('L')\n",
    "            lst = list()\n",
    "            count=0\n",
    "            lst2=list()\n",
    "            y=list()\n",
    "\n",
    "            for i in x.batch.unique():\n",
    "                df=x[x.batch==i]\n",
    "                lst = list()\n",
    "                if np.all(df.status==\"abnormal\"):\n",
    "                    path=\"wafer_img_cross\"+whole+\"/\"+method+\"/\"+str(df.iloc[0].cross)+\"/abnormal/\"\n",
    "                else:\n",
    "                    path=\"wafer_img_cross\"+whole+\"/\"+method+\"/\"+str(df.iloc[0].cross)+\"/normal/\"\n",
    "                for j in x.sensor.unique():\n",
    "                    img=pil_loader(path+str(i)+\"_\"+str(j)+\".png\")\n",
    "                    lst.append(np.array(img)/255)\n",
    "\n",
    "                arr = np.array(lst)\n",
    "                if df.iloc[0].status==\"normal\":\n",
    "                    y.append(0)\n",
    "                else:\n",
    "                    y.append(1)\n",
    "\n",
    "                lst2.append(arr)    \n",
    "            #     count+=1\n",
    "            #     if count==1:\n",
    "            #         break\n",
    "            train_x = torch.from_numpy(np.array(lst2)).float()\n",
    "            train_y = torch.LongTensor(np.array(y))\n",
    "\n",
    "            train_data = Dataset(train_x,train_y)\n",
    "            train_loader = DataLoader(dataset=train_data,\n",
    "                                      batch_size=24,\n",
    "                                      shuffle=True,\n",
    "                                      num_workers=2)\n",
    "            self.train_x=train_x\n",
    "            self.train_y=train_y\n",
    "            self.train_loader=train_loader\n",
    "\n",
    "            #test data set\n",
    "            normal   = pd.read_json(\"normal\"+whole+\"_cross_\"+str(cross_number_for_test)+\".json\")\n",
    "            abnormal = pd.read_json(\"abnormal\"+whole+\"_cross_\"+str(cross_number_for_test)+\".json\")\n",
    "            x=normal.append(abnormal)\n",
    "            x=x.sample(frac=1)\n",
    "\n",
    "            # print(x)\n",
    "            lst = list()\n",
    "            count=0\n",
    "            lst2=list()\n",
    "            y=list()\n",
    "            path=\"\"\n",
    "            for i in x.batch.unique():\n",
    "                df=x[x.batch==i]\n",
    "                lst = list()\n",
    "                if np.all(df.status==\"abnormal\"):\n",
    "                    path=\"wafer_img_cross\"+whole+\"/\"+method+\"/\"+str(df.iloc[0].cross)+\"/abnormal/\"\n",
    "                else:\n",
    "                    path=\"wafer_img_cross\"+whole+\"/\"+method+\"/\"+str(df.iloc[0].cross)+\"/normal/\"\n",
    "\n",
    "                for j in x.sensor.unique():\n",
    "                    img=pil_loader(path+str(i)+\"_\"+str(j)+\".png\")\n",
    "                    lst.append(np.array(img)/255)\n",
    "                    \n",
    "                arr = np.array(lst)\n",
    "                if df.iloc[0].status==\"normal\":\n",
    "                    y.append(0)\n",
    "                else:\n",
    "                    y.append(1)\n",
    "\n",
    "                lst2.append(arr)    \n",
    "            #     if count==1:\n",
    "            #         break\n",
    "            test_x = Variable(torch.from_numpy(np.array(lst2)).float()).cuda()\n",
    "            test_y = torch.LongTensor(np.array(y)).cuda()\n",
    "            self.test_x=test_x\n",
    "            self.test_y=test_y        \n",
    "\n",
    "            print(\"done\")\n",
    "            \n",
    "        \n",
    "print(\"Function setting done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.4627, 0.4627, 0.4627],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.4627, 0.4627, 0.4627],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.4627, 0.4627, 0.4627]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.2784, 0.2784, 0.2784],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.2784, 0.2784, 0.2784],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.2784, 0.2784, 0.2784]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0471, 0.0471, 0.0471],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0471, 0.0471, 0.0471],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0471, 0.0471, 0.0471],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.9098, 0.9098, 0.9098],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.9098, 0.9098, 0.9098],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.9098, 0.9098, 0.9098]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.7020, 0.7020, 0.7020],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.7020, 0.7020, 0.7020],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.7020, 0.7020, 0.7020]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.9765, 0.9765, 0.9765],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.9765, 0.9765, 0.9765],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.9765, 0.9765, 0.9765]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.4824, 0.4824, 0.4824],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.4824, 0.4824, 0.4824],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.4824, 0.4824, 0.4824]]],\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0196, 0.0196],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0196, 0.0196],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0196, 0.0196],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.4549, 0.4549, 0.4549],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.4549, 0.4549, 0.4549],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.4549, 0.4549, 0.4549]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.3922, 0.3922, 0.3922],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.3922, 0.3922, 0.3922],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.3922, 0.3922, 0.3922]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0314, 0.0314, 0.0314],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0314, 0.0314, 0.0314],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0314, 0.0314, 0.0314],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.4824, 0.4824, 0.4824],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.4824, 0.4824, 0.4824],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.4824, 0.4824, 0.4824]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.9216, 0.9216, 0.9216],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.9216, 0.9216, 0.9216],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.9216, 0.9216, 0.9216]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.2784, 0.2784, 0.2784],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.2784, 0.2784, 0.2784],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.2784, 0.2784, 0.2784]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.4941, 0.4941, 0.4941],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.4941, 0.4941, 0.4941],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.4941, 0.4941, 0.4941]]],\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.0000, 0.0000,  ..., 0.0902, 0.0902, 0.0902],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0902, 0.0902, 0.0902],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0902, 0.0902, 0.0902],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.9647, 0.9647, 0.9647],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.9647, 0.9647, 0.9647],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.9647, 0.9647, 0.9647]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0196, 0.0196],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0196, 0.0196],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0196, 0.0196],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.3569, 0.3569, 0.3569],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.3569, 0.3569, 0.3569],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.3569, 0.3569, 0.3569]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0196, 0.0196],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0196, 0.0196],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0196, 0.0196],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.5059, 0.5059, 0.5059],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.5059, 0.5059, 0.5059],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.5059, 0.5059, 0.5059]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0039, 0.0039],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0039, 0.0039],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0039, 0.0039],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.2784, 0.2784, 0.2784],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.2784, 0.2784, 0.2784],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.2784, 0.2784, 0.2784]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.9922, 0.9922, 0.9922],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.9922, 0.9922, 0.9922],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.9922, 0.9922, 0.9922]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.6392, 0.6392, 0.6392],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.6392, 0.6392, 0.6392],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.6392, 0.6392, 0.6392]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.2353, 0.2353, 0.2353],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.2353, 0.2353, 0.2353],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.2353, 0.2353, 0.2353]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.3412, 0.3412, 0.3412],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.3412, 0.3412, 0.3412],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.3412, 0.3412, 0.3412]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0078, 0.0078, 0.0078],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0078, 0.0078, 0.0078],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0078, 0.0078, 0.0078],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.3333, 0.3333, 0.3333],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.3333, 0.3333, 0.3333],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.3333, 0.3333, 0.3333]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.6980, 0.6980, 0.6980],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.6980, 0.6980, 0.6980],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.6980, 0.6980, 0.6980]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.3804, 0.3804, 0.3804],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.3804, 0.3804, 0.3804],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.3804, 0.3804, 0.3804]]],\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.0000, 0.0000,  ..., 0.0078, 0.0078, 0.0078],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0078, 0.0078, 0.0078],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0078, 0.0078, 0.0078],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.2980, 0.2980, 0.2980],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.2980, 0.2980, 0.2980],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.2980, 0.2980, 0.2980]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.2627, 0.2627, 0.2627],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.2627, 0.2627, 0.2627],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.2627, 0.2627, 0.2627]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.2314, 0.2314, 0.2314],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.2314, 0.2314, 0.2314],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.2314, 0.2314, 0.2314],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.4824, 0.4824, 0.4824],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.4824, 0.4824, 0.4824],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.4824, 0.4824, 0.4824]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.9961, 0.9961, 0.9961],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.9961, 0.9961, 0.9961],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.9961, 0.9961, 0.9961]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.4078, 0.4078, 0.4078],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.4078, 0.4078, 0.4078],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.4078, 0.4078, 0.4078]]],\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.3176, 0.3176, 0.3176],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.3176, 0.3176, 0.3176],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.3176, 0.3176, 0.3176]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.3059, 0.3059, 0.3059],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.3059, 0.3059, 0.3059],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.3059, 0.3059, 0.3059]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0314, 0.0314, 0.0314],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0314, 0.0314, 0.0314],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0314, 0.0314, 0.0314],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.4824, 0.4824, 0.4824],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.4824, 0.4824, 0.4824],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.4824, 0.4824, 0.4824]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.9137, 0.9137, 0.9137],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.9137, 0.9137, 0.9137],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.9137, 0.9137, 0.9137]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.4667, 0.4667, 0.4667],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.4667, 0.4667, 0.4667],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.4667, 0.4667, 0.4667]]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=dataset_dataloader(True,True,\"mtf\")\n",
    "a.train_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one chaneel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self,train_x,train_y):\n",
    "        self.train_x=train_x\n",
    "        self.train_y=train_y\n",
    "        self.data_len = len(train_x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        train_x=self.train_x[index]\n",
    "        train_y=self.train_y[index]\n",
    "        return (train_x, train_y)\n",
    "    def __len__(self):\n",
    "        return self.data_len\n",
    "    \n",
    "class dataset_dataloader:\n",
    "    def __init__ (self,is_whole,is_cross,method,cross_number_for_test=1,batchsize=24,numworkers=2):\n",
    "        if is_cross==False:\n",
    "            import pandas as pd\n",
    "            import numpy as np\n",
    "            from PIL import Image\n",
    "            from pyts.visualization import plot_gasf\n",
    "            import matplotlib.pyplot as plt\n",
    "            import torch\n",
    "            from torch.autograd import Variable\n",
    "\n",
    "            if is_whole==True:\n",
    "                whole=\"_all\"\n",
    "            else:\n",
    "                whole=\"\"\n",
    "\n",
    "            normal   = pd.read_json(\"normal_train\"+whole+\".json\")\n",
    "            abnormal = pd.read_json(\"abnormal_train\"+whole+\".json\")\n",
    "            x=normal.append(abnormal)\n",
    "            x=x.sample(frac=1)\n",
    "\n",
    "            def pil_loader(path):\n",
    "                with open(path, 'rb') as f:\n",
    "                    img = Image.open(f)\n",
    "                    return img.convert('L')\n",
    "            lst = list()\n",
    "            count=0\n",
    "            lst2=list()\n",
    "            y=list()\n",
    "\n",
    "            for i in x.batch.unique():\n",
    "                df=x[x.batch==i]\n",
    "                lst = list()\n",
    "                if np.all(df.status==\"abnormal\"):\n",
    "                    path=\"wafer_img\"+whole+\"/\"+method+\"/train/abnormal/\"\n",
    "                else:\n",
    "                    path=\"wafer_img\"+whole+\"/\"+method+\"/train/normal/\"\n",
    "\n",
    "                list_im = [\n",
    "                    path+str(i)+'_6.png', \n",
    "                    path+str(i)+'_7.png', \n",
    "                    path+str(i)+'_8.png',                     \n",
    "                    path+str(i)+'_11.png', \n",
    "                    path+str(i)+'_12.png', \n",
    "                    path+str(i)+'_15.png', \n",
    "                ]\n",
    "                imgs = [ pil_loader(j) for j in list_im ]\n",
    "                min_shape = sorted( [(np.sum(j.size), j.size ) for j in imgs])[0][1]\n",
    "                imgs_comb = np.vstack( (np.asarray( j.resize(min_shape) ) for j in imgs ) )\n",
    "                imgs_comb = Image.fromarray( imgs_comb)\n",
    "                lst.append(np.array(imgs_comb)/255)\n",
    "\n",
    "                arr = np.array(lst)\n",
    "                if df.iloc[0].status==\"normal\":\n",
    "                    y.append(0)\n",
    "                else:\n",
    "                    y.append(1)\n",
    "\n",
    "                lst2.append(arr)    \n",
    "            #     count+=1\n",
    "            #     if count==1:\n",
    "            #         break\n",
    "            train_x = torch.from_numpy(np.array(lst2)).float()\n",
    "            train_y = torch.LongTensor(np.array(y))\n",
    "\n",
    "            train_data = Dataset(train_x,train_y)\n",
    "            train_loader = DataLoader(dataset=train_data,\n",
    "                                      batch_size=24,\n",
    "                                      shuffle=True,\n",
    "                                      num_workers=2)\n",
    "            self.train_x=train_x\n",
    "            self.train_y=train_y\n",
    "            self.train_loader=train_loader\n",
    "\n",
    "            #test data set\n",
    "            normal   = pd.read_json(\"normal_validation\"+whole+\".json\")\n",
    "            abnormal = pd.read_json(\"abnormal_validation\"+whole+\".json\")\n",
    "            x=normal.append(abnormal)\n",
    "            x=x.sample(frac=1)\n",
    "\n",
    "            # print(x)\n",
    "            lst = list()\n",
    "            count=0\n",
    "            lst2=list()\n",
    "            y=list()\n",
    "            path=\"\"\n",
    "            for i in x.batch.unique():\n",
    "                df=x[x.batch==i]\n",
    "                lst = list()\n",
    "                if np.all(df.status==\"abnormal\"):\n",
    "                    path=\"wafer_img\"+whole+\"/\"+method+\"/validation/abnormal/\"\n",
    "                else:\n",
    "                    path=\"wafer_img\"+whole+\"/\"+method+\"/validation/normal/\"\n",
    "\n",
    "                list_im = [\n",
    "                    path+str(i)+'_6.png', \n",
    "                    path+str(i)+'_7.png', \n",
    "                    path+str(i)+'_8.png',                     \n",
    "                    path+str(i)+'_11.png', \n",
    "                    path+str(i)+'_12.png', \n",
    "                    path+str(i)+'_15.png', \n",
    "                ]\n",
    "                imgs = [ pil_loader(i) for i in list_im ]\n",
    "                min_shape = sorted( [(np.sum(i.size), i.size ) for i in imgs])[0][1]\n",
    "                imgs_comb = np.vstack( (np.asarray( i.resize(min_shape) ) for i in imgs ) )\n",
    "                imgs_comb = Image.fromarray( imgs_comb)\n",
    "                lst.append(np.array(imgs_comb)/255)\n",
    "                arr = np.array(lst)\n",
    "                if df.iloc[0].status==\"normal\":\n",
    "                    y.append(0)\n",
    "                else:\n",
    "                    y.append(1)\n",
    "\n",
    "                lst2.append(arr)    \n",
    "            #     if count==1:\n",
    "            #         break\n",
    "            test_x = Variable(torch.from_numpy(np.array(lst2)).float()).cuda()\n",
    "            test_y = torch.LongTensor(np.array(y)).cuda()\n",
    "            self.test_x=test_x\n",
    "            self.test_y=test_y        \n",
    "\n",
    "            print(\"done\")\n",
    "            \n",
    "        else:\n",
    "            import pandas as pd\n",
    "            import numpy as np\n",
    "            from PIL import Image\n",
    "            from pyts.visualization import plot_gasf\n",
    "            import matplotlib.pyplot as plt\n",
    "            import torch\n",
    "            from torch.autograd import Variable\n",
    "\n",
    "            if is_whole==True:\n",
    "                whole=\"_all\"\n",
    "            else:\n",
    "                whole=\"\"\n",
    "            cross=[\"1\",\"2\",\"3\",\"4\",\"5\"]\n",
    "            cross.remove(str(cross_number_for_test))\n",
    "            x = [  \n",
    "                pd.read_json(\"normal\"+whole+\"_cross_\"+cross[0]+\".json\"),\n",
    "                pd.read_json(\"normal\"+whole+\"_cross_\"+cross[1]+\".json\"),\n",
    "                pd.read_json(\"normal\"+whole+\"_cross_\"+cross[2]+\".json\"),\n",
    "                pd.read_json(\"normal\"+whole+\"_cross_\"+cross[3]+\".json\"),\n",
    "                pd.read_json(\"abnormal\"+whole+\"_cross_\"+cross[0]+\".json\"),\n",
    "                pd.read_json(\"abnormal\"+whole+\"_cross_\"+cross[1]+\".json\"),\n",
    "                pd.read_json(\"abnormal\"+whole+\"_cross_\"+cross[2]+\".json\"),\n",
    "                pd.read_json(\"abnormal\"+whole+\"_cross_\"+cross[3]+\".json\")\n",
    "            ]\n",
    "            x = pd.concat(x)\n",
    "            x =  x.sample(frac=1)\n",
    "\n",
    "\n",
    "            def pil_loader(path):\n",
    "                with open(path, 'rb') as f:\n",
    "                    img = Image.open(f)\n",
    "                    return img.convert('L')\n",
    "            lst = list()\n",
    "            count=0\n",
    "            lst2=list()\n",
    "            y=list()\n",
    "\n",
    "            for i in x.batch.unique():\n",
    "                df=x[x.batch==i]\n",
    "                lst = list()\n",
    "                if np.all(df.status==\"abnormal\"):\n",
    "                    path=\"wafer_img_cross\"+whole+\"/\"+method+\"/\"+str(df.iloc[0].cross)+\"/abnormal/\"\n",
    "                else:\n",
    "                    path=\"wafer_img_cross\"+whole+\"/\"+method+\"/\"+str(df.iloc[0].cross)+\"/normal/\"\n",
    "\n",
    "                list_im = [\n",
    "                    path+str(i)+'_6.png', \n",
    "                    path+str(i)+'_7.png', \n",
    "                    path+str(i)+'_8.png',                     \n",
    "                    path+str(i)+'_11.png', \n",
    "                    path+str(i)+'_12.png', \n",
    "                    path+str(i)+'_15.png', \n",
    "                ]\n",
    "                imgs = [ pil_loader(i) for i in list_im ]\n",
    "                min_shape = sorted( [(np.sum(i.size), i.size ) for i in imgs])[0][1]\n",
    "                imgs_comb = np.vstack( (np.asarray( i.resize(min_shape) ) for i in imgs ) )\n",
    "                imgs_comb = Image.fromarray( imgs_comb)\n",
    "                lst.append(np.array(imgs_comb)/255)\n",
    "\n",
    "                arr = np.array(lst)\n",
    "                if df.iloc[0].status==\"normal\":\n",
    "                    y.append(0)\n",
    "                else:\n",
    "                    y.append(1)\n",
    "\n",
    "                lst2.append(arr)    \n",
    "            #     count+=1\n",
    "            #     if count==1:\n",
    "            #         break\n",
    "            train_x = torch.from_numpy(np.array(lst2)).float()\n",
    "            train_y = torch.LongTensor(np.array(y))\n",
    "\n",
    "            train_data = Dataset(train_x,train_y)\n",
    "            train_loader = DataLoader(dataset=train_data,\n",
    "                                      batch_size=24,\n",
    "                                      shuffle=True,\n",
    "                                      num_workers=2)\n",
    "            self.train_x=train_x\n",
    "            self.train_y=train_y\n",
    "            self.train_loader=train_loader\n",
    "\n",
    "            #test data set\n",
    "            normal   = pd.read_json(\"normal\"+whole+\"_cross_\"+str(cross_number_for_test)+\".json\")\n",
    "            abnormal = pd.read_json(\"abnormal\"+whole+\"_cross_\"+str(cross_number_for_test)+\".json\")\n",
    "            x=normal.append(abnormal)\n",
    "            x=x.sample(frac=1)\n",
    "\n",
    "            # print(x)\n",
    "            lst = list()\n",
    "            count=0\n",
    "            lst2=list()\n",
    "            y=list()\n",
    "            path=\"\"\n",
    "            for i in x.batch.unique():\n",
    "                df=x[x.batch==i]\n",
    "                lst = list()\n",
    "                if np.all(df.status==\"abnormal\"):\n",
    "                    path=\"wafer_img_cross\"+whole+\"/\"+method+\"/\"+str(df.iloc[0].cross)+\"/abnormal/\"\n",
    "                else:\n",
    "                    path=\"wafer_img_cross\"+whole+\"/\"+method+\"/\"+str(df.iloc[0].cross)+\"/normal/\"\n",
    "\n",
    "                list_im = [\n",
    "                    path+str(i)+'_6.png', \n",
    "                    path+str(i)+'_7.png', \n",
    "                    path+str(i)+'_8.png',                     \n",
    "                    path+str(i)+'_11.png', \n",
    "                    path+str(i)+'_12.png', \n",
    "                    path+str(i)+'_15.png', \n",
    "                ]\n",
    "                imgs = [ pil_loader(i) for i in list_im ]\n",
    "                min_shape = sorted( [(np.sum(i.size), i.size ) for i in imgs])[0][1]\n",
    "                imgs_comb = np.vstack( (np.asarray( i.resize(min_shape) ) for i in imgs ) )\n",
    "                imgs_comb = Image.fromarray( imgs_comb)\n",
    "                lst.append(np.array(imgs_comb)/255)\n",
    "                arr = np.array(lst)\n",
    "                if df.iloc[0].status==\"normal\":\n",
    "                    y.append(0)\n",
    "                else:\n",
    "                    y.append(1)\n",
    "\n",
    "                lst2.append(arr)    \n",
    "            #     if count==1:\n",
    "            #         break\n",
    "            test_x = Variable(torch.from_numpy(np.array(lst2)).float()).cuda()\n",
    "            test_y = torch.LongTensor(np.array(y)).cuda()\n",
    "            self.test_x=test_x\n",
    "            self.test_y=test_y        \n",
    "\n",
    "            print(\"done\")\n",
    "            \n",
    "        \n",
    "print(\"Function setting done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __init__ (self,is_whole,is_cross,method,cross_number_for_test=1,batchsize=24,numworkers=2):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#f47641\">Cnn start</font>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "accuracy_matrix =[]\n",
    "import dataset\n",
    "import dataset_six\n",
    "\n",
    "    \n",
    "for i in range(5):\n",
    "    min_error=100\n",
    "    min_lose=100\n",
    "    count=0\n",
    "    number=1\n",
    "#     cross=dataset.dataset_dataloader(False,True,\"mtf\",i+1)\n",
    "    cross=dataset_six.dataset_dataloader(True,True,\"mtf\",i+1)\n",
    "    import torch.nn as nn\n",
    "\n",
    "    EPOCH = 50              # train the training data n times, to save time, we just train 1 epoch\n",
    "    LR = 0.0023\n",
    "    \n",
    "    class CNN(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(CNN, self).__init__()\n",
    "            self.conv1 =nn.Sequential( #卷基層   \n",
    "                nn.Conv2d(\n",
    "                    in_channels  = 6  , # 圖片是有幾層的 若 RGB三層 灰階 1層\n",
    "                    out_channels = 10, # 同時有幾個filter 進行掃描 會提取擠個特徵 代表下一層高度為擠\n",
    "                    kernel_size  = 5  , # 一次畫出來的框 畫幾格 ex 5*5\n",
    "                    stride       = 1  , # 每一個框框跳幾格\n",
    "                    padding      = 2, # 在 5x5逐步掃描後會有少的格子 因此2代表在外面為兩圈的0\n",
    "                                        # if stride =1 padding = (kernel_size-1)/2\n",
    "                ),  # 過濾器 卷基層 蒐集圖片訊息 三維的空間  \n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2), #找出更重要的特徵  像是在2x2中 找到 最大的值\n",
    "            )\n",
    "            self.conv2 = nn.Sequential(\n",
    "                nn.Conv2d(10,20,5,1,2), # 輸入的是上一層的16 把輸出層在變大因此是32 其他不變\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2)\n",
    "            )\n",
    "#             self.conv3 = nn.Sequential(\n",
    "#                 nn.Conv2d(5,10,5,1,2), # 輸入的是上一層的16 把輸出層在變大因此是32 其他不變\n",
    "#                 nn.ReLU(),\n",
    "#                 nn.MaxPool2d(2)\n",
    "#             )\n",
    "\n",
    "            self.out = nn.Linear(20*32*32,2)\n",
    "        def forward(self,x):\n",
    "\n",
    "            x=self.conv1(x)\n",
    "            x=self.conv2(x) #(batch.32.7.7)\n",
    "#             x=self.conv3(x)\n",
    "            x=x.view(x.size(0),-1) #將三維轉二維 (batch , 32*7*7)\n",
    "            output=self.out(x)\n",
    "            return output,x\n",
    "\n",
    "    cnn=CNN()\n",
    "    cnn.cuda()\n",
    "#     print(cnn)\n",
    "\n",
    "    optimizer = torch.optim.Adam(cnn.parameters(), lr=LR)   # optimize all cnn parameters\n",
    "    loss_func = nn.CrossEntropyLoss()                       # the target label is not one-hotted\n",
    "\n",
    "    # following function (plot_with_labels) is for visualization, can be ignored if not interested\n",
    "\n",
    "    # training and testing\n",
    "\n",
    "\n",
    "    for epoch in range(EPOCH):\n",
    "        for step, (x, y) in enumerate(cross.train_loader):   # gives batch data, normalize x when iterate train_loader\n",
    "            b_x = Variable(x).cuda()   # batch x\n",
    "            b_y = Variable(y).cuda()   # batch y\n",
    "\n",
    "            output = cnn(b_x)[0]             # cnn output\n",
    "            loss = loss_func(output, b_y)   # cross entropy loss\n",
    "            optimizer.zero_grad()           # clear gradients for this training step\n",
    "            loss.backward()                 # backpropagation, compute gradients\n",
    "            optimizer.step()                # apply gradients\n",
    "            test_output, last_layer = cnn(cross.test_x)\n",
    "            pred_y = torch.max(test_output, 1)[1].cuda().data.squeeze()\n",
    "            accuracy = sum(pred_y == cross.test_y) / float(cross.test_y.size(0))\n",
    "            error  = (1-accuracy)*100\n",
    "            if error<min_error:\n",
    "                min_error=error\n",
    "                min_lose=100\n",
    "                count=0\n",
    "            else:\n",
    "                count+=1\n",
    "                print(\".\",end=\"\")\n",
    "\n",
    "            if error == min_error:\n",
    "                if loss.data[0]<min_lose:\n",
    "                    min_lose=loss.data[0]\n",
    "                    torch.save(cnn,'cnn_save/cnn'+str(number)+'.pkl')\n",
    "                    number+=1\n",
    "    #                 print('save!!')\n",
    "    #                 print('\\ncount: ',count,'Epoch: ', epoch, '| train loss: %.15f' % loss.data[0], '| min validation error rate: %.20f' % min_error)\n",
    "            if count>=350:\n",
    "                break\n",
    "        if count>=350:\n",
    "            break\n",
    "    \n",
    "    # print 10 predictions from test data\n",
    "    # test_output, _ = cnn(test_x)\n",
    "    # pred_y = torch.max(test_output, 1)[1].cuda().data.squeeze()\n",
    "\n",
    "    # print(pred_y, 'prediction number')\n",
    "    # print(test_y, 'real number')\n",
    "    accuracy_matrix.append(min_error)\n",
    "    print('\\n number: ',number-1 ,min_error)\n",
    "    \n",
    "import IPython.display as ipd\n",
    "import numpy\n",
    "sr = 22050 # sample rate\n",
    "T = 0.3    # seconds\n",
    "t = numpy.linspace(0, T, int(T*sr), endpoint=False) # time variable\n",
    "x = 0.5*numpy.sin(2*numpy.pi*490*t)                # pure sine wave at 440 284Hz\n",
    "ipd.Audio(x, rate=sr,autoplay=True) # load a NumPy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "accuracy_matrix =[]\n",
    "import dataset\n",
    "import dataset_ecg_four\n",
    "import dataset_four\n",
    "import time\n",
    "\n",
    "whole=[True]\n",
    "method=[\"gasf\"]\n",
    "\n",
    "F = open('method_2_four.txt', 'w')  \n",
    "# F.write(\"method,whole,experiments,r1,r2,r3,r4,r5,average \\n\")\n",
    "for l in method:\n",
    "    for m in whole:\n",
    "        for k in range(20):\n",
    "            accuracy_matrix=[]\n",
    "            start_time = time.time()\n",
    "            for i in range(5):\n",
    "                min_error=100\n",
    "                min_lose=100\n",
    "                count=0\n",
    "                number=1\n",
    "                cross=dataset_four.dataset_dataloader(m,True,l,i+1,batchsize=1,numworkers=2)\n",
    "                import torch.nn as nn\n",
    "\n",
    "                EPOCH = 50              # train the training data n times, to save time, we just train 1 epoch\n",
    "                LR = 0.0023\n",
    "\n",
    "                class CNN(nn.Module):\n",
    "                    def __init__(self):\n",
    "                        super(CNN, self).__init__()\n",
    "                        self.conv1 =nn.Sequential( #卷基層   \n",
    "                            nn.Conv2d(\n",
    "                                in_channels  = 1  , # 圖片是有幾層的 若 RGB三層 灰階 1層\n",
    "                                out_channels = 3, # 同時16個filter 進行掃描 會提取16個特徵 代表下一層高度為16\n",
    "                                kernel_size  = 5  , # 一次畫出來的框 畫幾格 ex 5*5\n",
    "                                stride       = 1  , # 每一個框框跳幾格\n",
    "                                padding      = 2, # 在 5x5逐步掃描後會有少的格子 因此2代表在外面為兩圈的0\n",
    "                                                    # if stride =1 padding = (kernel_size-1)/2\n",
    "                            ),  # 過濾器 卷基層 蒐集圖片訊息 三維的空間  \n",
    "                            nn.ReLU(),\n",
    "                            nn.MaxPool2d(kernel_size=2), #找出更重要的特徵  像是在2x2中 找到 最大的值\n",
    "                        )\n",
    "                        self.conv2 = nn.Sequential(\n",
    "                            nn.Conv2d(3,5,5,1,2), # 輸入的是上一層的16 把輸出層在變大因此是32 其他不變\n",
    "                            nn.ReLU(),\n",
    "                            nn.MaxPool2d(2)\n",
    "                        )\n",
    "            #             self.conv3 = nn.Sequential(\n",
    "            #                 nn.Conv2d(10,20,3,1,1), # 輸入的是上一層的16 把輸出層在變大因此是32 其他不變\n",
    "            #                 nn.ReLU(),\n",
    "            # #                 nn.MaxPool2d(2)\n",
    "            #             )\n",
    "\n",
    "                        self.out = nn.Linear(5*192*128,2)\n",
    "                    def forward(self,x):\n",
    "\n",
    "                        x=self.conv1(x)\n",
    "                        x=self.conv2(x) #(batch.32.7.7)\n",
    "            #             x=self.conv3(x)\n",
    "                        x=x.view(x.size(0),-1) #將三維轉二維 (batch , 32*7*7)\n",
    "                        output=self.out(x)\n",
    "                        return output,x\n",
    "\n",
    "                cnn=CNN()\n",
    "                cnn.cuda()\n",
    "            #     print(cnn)\n",
    "\n",
    "                optimizer = torch.optim.Adam(cnn.parameters(), lr=LR)   # optimize all cnn parameters\n",
    "                loss_func = nn.CrossEntropyLoss()                       # the target label is not one-hotted\n",
    "\n",
    "                # following function (plot_with_labels) is for visualization, can be ignored if not interested\n",
    "\n",
    "                # training and testing\n",
    "\n",
    "\n",
    "                for epoch in range(EPOCH):\n",
    "                    for step, (x, y) in enumerate(cross.train_loader):   # gives batch data, normalize x when iterate train_loader\n",
    "                        b_x = Variable(x).cuda()   # batch x\n",
    "                        b_y = Variable(y).cuda()   # batch y\n",
    "\n",
    "                        output = cnn(b_x)[0]             # cnn output\n",
    "                        loss = loss_func(output, b_y)   # cross entropy loss\n",
    "                        optimizer.zero_grad()           # clear gradients for this training step\n",
    "                        loss.backward()                 # backpropagation, compute gradients\n",
    "                        optimizer.step()                # apply gradients\n",
    "                        test_output, last_layer = cnn(cross.test_x)\n",
    "                        pred_y = torch.max(test_output, 1)[1].cuda().data.squeeze()\n",
    "                        accuracy = sum(pred_y == cross.test_y) / float(cross.test_y.size(0))\n",
    "                        error  = (1-accuracy)*100\n",
    "                        if error<min_error:\n",
    "                            min_error=error\n",
    "                            min_lose=100\n",
    "                            count=0\n",
    "                        else:\n",
    "                            count+=1\n",
    "                            print(\".\",end=\"\")\n",
    "\n",
    "                        if error == min_error:\n",
    "                            if loss.data[0]<min_lose:\n",
    "                                min_lose=loss.data[0]\n",
    "                                torch.save(cnn,'cnn_save/cnn'+str(number)+'.pkl')\n",
    "                                number+=1\n",
    "                #                 print('save!!')\n",
    "                #                 print('\\ncount: ',count,'Epoch: ', epoch, '| train loss: %.15f' % loss.data[0], '| min validation error rate: %.20f' % min_error)\n",
    "                        if count>=500:\n",
    "                            break\n",
    "                    if count>=500:\n",
    "                        break\n",
    "\n",
    "                # print 10 predictions from test data\n",
    "                # test_output, _ = cnn(test_x)\n",
    "                # pred_y = torch.max(test_output, 1)[1].cuda().data.squeeze()\n",
    "\n",
    "                # print(pred_y, 'prediction number')\n",
    "                # print(test_y, 'real number')\n",
    "                accuracy_matrix.append(min_error)\n",
    "                print('\\n number: ',number-1 ,min_error)\n",
    "            import IPython.display as ipd\n",
    "            import numpy\n",
    "            sr = 22050 # sample rate\n",
    "            T = 0.3    # seconds\n",
    "            t = numpy.linspace(0, T, int(T*sr), endpoint=False) # time variable\n",
    "            x = 0.5*numpy.sin(2*numpy.pi*490*t)                # pure sine wave at 440 284Hz\n",
    "\n",
    "            print(accuracy_matrix)\n",
    "            sum(accuracy_matrix) / float(len(accuracy_matrix))\n",
    "            F.write(str(l)+\",\"+str(m)+\",\"+str(k)+\",\")\n",
    "            for i in accuracy_matrix:  \n",
    "                F.write(str(i) + \",\")\n",
    "            F.write(str(sum(accuracy_matrix) / float(len(accuracy_matrix)))+\",\")\n",
    "            F.write(str(time.time() - start_time)+\"\\n\")\n",
    "            print(time.time() - start_time)\n",
    "F.close()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_matrix)\n",
    "sum(accuracy_matrix) / float(len(accuracy_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset_four\n",
    "dataloader=dataset_four.dataset_dataloader(True,False,\"mtf\",1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "view_data = dataloader.train_x[:N_TEST_IMG].view(-1, 768*512).type(torch.FloatTensor)\n",
    "\n",
    "imshow(np.reshape(view_data.numpy()[i], (768, 512)), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, a = plt.subplots(2, N_TEST_IMG, figsize=(100, 20))\n",
    "plt.ion() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import dataset_four\n",
    "import dataset_ecg_four\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# torch.manual_seed(1)    # reproducible\n",
    "\n",
    "# Hyper Parameters\n",
    "EPOCH = 10\n",
    "LR = 0.005         # learning rate\n",
    "DOWNLOAD_MNIST = False\n",
    "N_TEST_IMG = 5\n",
    "\n",
    "\n",
    "# plot one example\n",
    "\n",
    "# Data Loader for easy mini-batch return in training, the image batch shape will be (50, 1, 28, 28)\n",
    "dataloader=dataset_ecg_four.dataset_dataloader(True,False,\"mtf\",1,batchsize=10)\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(256*512, 2048),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 128),   \n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 64), \n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 10), \n",
    "            nn.Tanh(),\n",
    "            nn.Linear(10, 3), \n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(3, 10),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(10, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 512),   \n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 1024), \n",
    "            nn.Tanh(),\n",
    "            nn.Linear(1024, 2048), \n",
    "            nn.Tanh(),\n",
    "            nn.Linear(2048, 256*512), \n",
    "            nn.Sigmoid(),       # compress to a range (0, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "\n",
    "\n",
    "autoencoder = AutoEncoder()\n",
    "\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=LR)\n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "# initialize figure\n",
    "f, a = plt.subplots(2, N_TEST_IMG, figsize=(100, 20))\n",
    "plt.ion()   # continuously plot\n",
    "\n",
    "# original data (first row) for viewing\n",
    "view_data = dataloader.train_x[:N_TEST_IMG].view(-1, 256*512).type(torch.FloatTensor)\n",
    "for i in range(N_TEST_IMG):\n",
    "    a[0][i].imshow(np.reshape(view_data.numpy()[i], (256, 512)), cmap='gray'); a[0][i].set_xticks(()); a[0][i].set_yticks(())\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    for step, (x, b_label) in enumerate(dataloader.train_loader):\n",
    "        b_x = Variable(x.view(-1, 256*512))   # batch x, shape (batch, 28*28)\n",
    "        b_y = Variable(x.view(-1, 256*512))   # batch y, shape (batch, 28*28)\n",
    "\n",
    "        encoded, decoded = autoencoder(b_x)\n",
    "\n",
    "        loss = loss_func(decoded, b_y)      # mean square error\n",
    "        optimizer.zero_grad()               # clear gradients for this training step\n",
    "        loss.backward()                     # backpropagation, compute gradients\n",
    "        optimizer.step()                    # apply gradients\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print('Epoch: ', epoch, '| train loss: %.4f' % loss.data.numpy())\n",
    "\n",
    "#             plotting decoded image (second row)\n",
    "            _, decoded_data = autoencoder(Variable(view_data))\n",
    "            for i in range(N_TEST_IMG):\n",
    "                a[1][i].clear()\n",
    "                a[1][i].imshow(np.reshape(decoded_data.data.numpy()[i], (256, 512)), cmap='gray')\n",
    "                a[1][i].set_xticks(()); a[1][i].set_yticks(())\n",
    "            plt.draw(); plt.pause(0.05)\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize in 3D plot\n",
    "%matplotlib notebook\n",
    "\n",
    "view_data = dataloader.train_x[:200].view(-1, 256*512).type(torch.FloatTensor)\n",
    "encoded_data, _ = autoencoder(Variable(view_data))\n",
    "fig = plt.figure(2); ax = Axes3D(fig)\n",
    "X, Y, Z = encoded_data.data[:, 0].numpy(), encoded_data.data[:, 1].numpy(), encoded_data.data[:, 2].numpy()\n",
    "values = dataloader.train_y[:200].numpy()\n",
    "for x, y, z, s in zip(X, Y, Z, values):\n",
    "    c = cm.rainbow(int(255*s/9)); ax.text(x, y, z, s, backgroundcolor=c)\n",
    "ax.set_xlim(X.min(), X.max()); ax.set_ylim(Y.min(), Y.max()); ax.set_zlim(Z.min(), Z.max())\n",
    "\n",
    "plt.savefig(\"3D.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import Function\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "import random\n",
    "from skimage.transform import resize\n",
    "import dataset_ecg_four\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_sided_padding(x):\n",
    "    rand1 = random.randrange(0,15,3)\n",
    "    rand2 = random.randrange(0,15,3)\n",
    "\n",
    "    zero = np.zeros(shape=[28,28,1])\n",
    "    zero[rand1:rand1+12,rand2:rand2+12,:]=np.asarray(x).reshape(12,12,1)\n",
    "    return zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader=dataset_ecg_four.dataset_dataloader(True,False,\"mtf\",1,batchsize=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
