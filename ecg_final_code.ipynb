{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#f47641\">cread detile csv</font>  \n",
    "###  <font color=\"#DAC9A6\"># deal with normalization </font>  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "import numpy\n",
    "sr = 22050 # sample rate\n",
    "T = 0.2    # seconds\n",
    "t = numpy.linspace(0, T, int(T*sr), endpoint=False) # time variable\n",
    "x = 0.5*numpy.sin(2*numpy.pi*490*t)                # pure sine wave at 440 Hz\n",
    "ipd.Audio(x, rate=sr,autoplay=True) # load a NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "count = 0\n",
    "is_normal=[\"normal\",\"abnormal\"]\n",
    "\n",
    "for a in is_normal:\n",
    "    df = pd.DataFrame(columns=['batch','sensor','value','status'])\n",
    "    df[\"value\"]=df[\"value\"].astype(\"float\")\n",
    "    for filename in (glob.glob('ecg/'+a+'/*')):\n",
    "        file = pd.read_csv(filename,sep=\"   \",header=None)\n",
    "        batch=re.search(\"8.+?(?=\\.)\",filename)\n",
    "        sensor=re.search(\"(?<=\\.)(.*)\",filename)\n",
    "        status=a\n",
    "        if (sensor.group(0)!=\"ann\" and sensor.group(0)!=\"box\"):\n",
    "            value = file[1].values\n",
    "            #normalization\n",
    "            value = preprocessing.minmax_scale(value,feature_range=(0,1))\n",
    "            df.loc[count]=[batch.group(0),sensor.group(0),value,status]\n",
    "            \n",
    "        count+=1\n",
    "\n",
    "    df.to_json(\"ecg_\"+a+\".json\")\n",
    "\n",
    "# #write csv to let the value type in array\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#f47641\">create train and test dataset csv</font>  \n",
    "###  <font color=\"#DAC9A6\"> # if want change train and test set change seed </font>  \n",
    "###  <font color=\"#DAC9A6\"> # separate data to train test validation        </font>  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random \n",
    "import numpy as np\n",
    "\n",
    "def dataset(is_whole):\n",
    "    if is_whole==True:\n",
    "        np.random.seed(1)\n",
    "        df_normal = pd.read_json(\"ecg_normal.json\")\n",
    "        df_abnormal = pd.read_json(\"ecg_abnormal.json\")\n",
    "        np.random.seed(1)\n",
    "        normal_train =np.random.choice(df_normal.batch.unique(),93,replace=False)\n",
    "        abnormal_train =np.random.choice(df_abnormal.batch.unique(),47,replace=False)\n",
    "\n",
    "        df_normal_train=df_normal[df_normal.batch.isin(normal_train)]\n",
    "        df_normal_left=df_normal[~df_normal.batch.isin(normal_train)]\n",
    "        normal_validation =np.random.choice(df_normal_left.batch.unique(),20,replace=False)\n",
    "        df_normal_validation=df_normal_left[df_normal_left.batch.isin(normal_validation)]\n",
    "        df_normal_test=df_normal_left[~df_normal_left.batch.isin(normal_validation)]\n",
    "\n",
    "        df_abnormal_train=df_abnormal[df_abnormal.batch.isin(abnormal_train)]\n",
    "        df_abnormal_left=df_abnormal[~df_abnormal.batch.isin(abnormal_train)]\n",
    "        abnormal_validation =np.random.choice(df_abnormal_left.batch.unique(),10,replace=False)\n",
    "        df_abnormal_validation=df_abnormal_left[df_abnormal_left.batch.isin(abnormal_validation)]\n",
    "        df_abnormal_test=df_abnormal_left[~df_abnormal_left.batch.isin(abnormal_validation)]\n",
    "\n",
    "\n",
    "        df_normal_train.to_json(\"ecg_normal_train_all.json\")\n",
    "        df_abnormal_train.to_json(\"ecg_abnormal_train_all.json\")\n",
    "        df_normal_validation.to_json(\"ecg_normal_validation_all.json\")\n",
    "        df_abnormal_validation.to_json(\"ecg_abnormal_validation_all.json\")\n",
    "        df_normal_test.to_json(\"ecg_normal_test_all.json\")\n",
    "        df_abnormal_test.to_json(\"ecg_abnormal_test_all.json\")\n",
    "        \n",
    "    else: \n",
    "        np.random.seed(1)\n",
    "        df_normal = pd.read_json(\"ecg_normal.json\")\n",
    "        normal_sample =np.random.choice(df_normal.batch.unique(),70,replace=False)\n",
    "        df_normal_sample=df_normal[df_normal.batch.isin(normal_sample)]\n",
    "\n",
    "        df_abnormal = pd.read_json(\"ecg_abnormal.json\")\n",
    "\n",
    "        np.random.seed(1)\n",
    "        normal_train =np.random.choice(df_normal_sample.batch.unique(),50,replace=False)\n",
    "        abnormal_train =np.random.choice(df_abnormal.batch.unique(),47,replace=False)\n",
    "\n",
    "        df_normal_train=df_normal_sample[df_normal_sample.batch.isin(normal_train)]\n",
    "        df_normal_left=df_normal_sample[~df_normal_sample.batch.isin(normal_train)]\n",
    "        normal_validation =np.random.choice(df_normal_left.batch.unique(),10,replace=False)\n",
    "        df_normal_validation=df_normal_left[df_normal_left.batch.isin(normal_validation)]\n",
    "        df_normal_test=df_normal_left[~df_normal_left.batch.isin(normal_validation)]\n",
    "\n",
    "        df_abnormal_train=df_abnormal[df_abnormal.batch.isin(abnormal_train)]\n",
    "        df_abnormal_left=df_abnormal[~df_abnormal.batch.isin(abnormal_train)]\n",
    "        abnormal_validation =np.random.choice(df_abnormal_left.batch.unique(),10,replace=False)\n",
    "        df_abnormal_validation=df_abnormal_left[df_abnormal_left.batch.isin(abnormal_validation)]\n",
    "        df_abnormal_test=df_abnormal_left[~df_abnormal_left.batch.isin(abnormal_validation)]\n",
    "\n",
    "\n",
    "        df_normal_train.to_json(\"ecg_normal_train.json\")\n",
    "        df_abnormal_train.to_json(\"ecg_abnormal_train.json\")\n",
    "        df_normal_validation.to_json(\"ecg_normal_validation.json\")\n",
    "        df_abnormal_validation.to_json(\"ecg_abnormal_validation.json\")\n",
    "        df_normal_test.to_json(\"ecg_normal_test.json\")\n",
    "        df_abnormal_test.to_json(\"ecg_abnormal_test.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset(True)\n",
    "dataset(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#f47641\">create crossvalidation dataset csv</font>  \n",
    "###  <font color=\"#DAC9A6\"> # if want change train and test set change seed </font>  \n",
    "###  <font color=\"#DAC9A6\"> # separate data to 5-fold crossvalidation        </font>  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_cross(is_whole):\n",
    "    import pandas as pd\n",
    "    import random \n",
    "    import numpy as np\n",
    "    if is_whole==True:\n",
    "        np.random.seed(1)\n",
    "        df_normal = pd.read_json(\"ecg_normal.json\")\n",
    "        df_abnormal = pd.read_json(\"ecg_abnormal.json\")\n",
    "        np.random.seed(1)\n",
    "\n",
    "        normal_coross_1 =np.random.choice(df_normal.batch.unique(),27,replace=False)\n",
    "        df_normal_cross_1=df_normal[df_normal.batch.isin(normal_coross_1)]\n",
    "        df_normal_cross_1[\"cross\"]=1\n",
    "        df_normal_left=df_normal[~df_normal.batch.isin(normal_coross_1)]\n",
    "\n",
    "        normal_coross_2 =np.random.choice(df_normal_left.batch.unique(),26,replace=False)\n",
    "        df_normal_cross_2=df_normal_left[df_normal_left.batch.isin(normal_coross_2)]\n",
    "        df_normal_cross_2[\"cross\"]=2\n",
    "        df_normal_left=df_normal_left[~df_normal_left.batch.isin(normal_coross_2)]\n",
    "\n",
    "        normal_coross_3 =np.random.choice(df_normal_left.batch.unique(),27,replace=False)\n",
    "        df_normal_cross_3=df_normal_left[df_normal_left.batch.isin(normal_coross_3)]\n",
    "        df_normal_cross_3[\"cross\"]=3\n",
    "        df_normal_left=df_normal_left[~df_normal_left.batch.isin(normal_coross_3)]\n",
    "\n",
    "        normal_coross_4 =np.random.choice(df_normal_left.batch.unique(),26,replace=False)\n",
    "        df_normal_cross_4=df_normal_left[df_normal_left.batch.isin(normal_coross_4)]\n",
    "        df_normal_cross_4[\"cross\"]=4\n",
    "        df_normal_cross_5=df_normal_left[~df_normal_left.batch.isin(normal_coross_4)]\n",
    "        df_normal_cross_5[\"cross\"]=5\n",
    "\n",
    "        abnormal_coross_1 =np.random.choice(df_abnormal.batch.unique(),13,replace=False)\n",
    "        df_abnormal_cross_1=df_abnormal[df_abnormal.batch.isin(abnormal_coross_1)]\n",
    "        df_abnormal_cross_1[\"cross\"]=1\n",
    "        df_abnormal_left=df_abnormal[~df_abnormal.batch.isin(abnormal_coross_1)]\n",
    "\n",
    "        abnormal_coross_2 =np.random.choice(df_abnormal_left.batch.unique(),14,replace=False)\n",
    "        df_abnormal_cross_2=df_abnormal_left[df_abnormal_left.batch.isin(abnormal_coross_2)]\n",
    "        df_abnormal_cross_2[\"cross\"]=2\n",
    "        df_abnormal_left=df_abnormal_left[~df_abnormal_left.batch.isin(abnormal_coross_2)]\n",
    "\n",
    "        abnormal_coross_3 =np.random.choice(df_abnormal_left.batch.unique(),13,replace=False)\n",
    "        df_abnormal_cross_3=df_abnormal_left[df_abnormal_left.batch.isin(abnormal_coross_3)]\n",
    "        df_abnormal_cross_3[\"cross\"]=3\n",
    "        df_abnormal_left=df_abnormal_left[~df_abnormal_left.batch.isin(abnormal_coross_3)]\n",
    "\n",
    "        abnormal_coross_4 =np.random.choice(df_abnormal_left.batch.unique(),14,replace=False)\n",
    "        df_abnormal_cross_4=df_abnormal_left[df_abnormal_left.batch.isin(abnormal_coross_4)]\n",
    "        df_abnormal_cross_4[\"cross\"]=4\n",
    "        df_abnormal_cross_5=df_abnormal_left[~df_abnormal_left.batch.isin(abnormal_coross_4)]\n",
    "        df_abnormal_cross_5[\"cross\"]=5\n",
    "\n",
    "\n",
    "        df_normal_cross_1.to_json(\"ecg_normal_all_cross_1.json\")\n",
    "        df_normal_cross_2.to_json(\"ecg_normal_all_cross_2.json\")\n",
    "        df_normal_cross_3.to_json(\"ecg_normal_all_cross_3.json\")\n",
    "        df_normal_cross_4.to_json(\"ecg_normal_all_cross_4.json\")\n",
    "        df_normal_cross_5.to_json(\"ecg_normal_all_cross_5.json\")\n",
    "\n",
    "        df_abnormal_cross_1.to_json(\"ecg_abnormal_all_cross_1.json\")\n",
    "        df_abnormal_cross_2.to_json(\"ecg_abnormal_all_cross_2.json\")\n",
    "        df_abnormal_cross_3.to_json(\"ecg_abnormal_all_cross_3.json\")\n",
    "        df_abnormal_cross_4.to_json(\"ecg_abnormal_all_cross_4.json\")\n",
    "        df_abnormal_cross_5.to_json(\"ecg_abnormal_all_cross_5.json\")\n",
    "\n",
    "        print(len(df_normal_cross_1),len(df_normal_cross_2),len(df_normal_cross_3),len(df_normal_cross_4),len(df_normal_cross_5))\n",
    "        print(len(df_abnormal_cross_1),len(df_abnormal_cross_2),len(df_abnormal_cross_3),len(df_abnormal_cross_4),len(df_abnormal_cross_5))\n",
    "    else:\n",
    "        np.random.seed(1)\n",
    "        df_normal = pd.read_json(\"ecg_normal.json\")\n",
    "        normal_sample =np.random.choice(df_normal.batch.unique(),70,replace=False)\n",
    "        df_normal_sample=df_normal[df_normal.batch.isin(normal_sample)]\n",
    "        df_abnormal = pd.read_json(\"ecg_abnormal.json\")\n",
    "        np.random.seed(1)\n",
    "\n",
    "        normal_coross_1 =np.random.choice(df_normal_sample.batch.unique(),14,replace=False)\n",
    "        df_normal_cross_1=df_normal_sample[df_normal_sample.batch.isin(normal_coross_1)]\n",
    "        df_normal_cross_1[\"cross\"]=1\n",
    "        df_normal_left=df_normal_sample[~df_normal_sample.batch.isin(normal_coross_1)]\n",
    "\n",
    "        normal_coross_2 =np.random.choice(df_normal_left.batch.unique(),14,replace=False)\n",
    "        df_normal_cross_2=df_normal_left[df_normal_left.batch.isin(normal_coross_2)]\n",
    "        df_normal_cross_2[\"cross\"]=2\n",
    "        df_normal_left=df_normal_left[~df_normal_left.batch.isin(normal_coross_2)]\n",
    "\n",
    "        normal_coross_3 =np.random.choice(df_normal_left.batch.unique(),14,replace=False)\n",
    "        df_normal_cross_3=df_normal_left[df_normal_left.batch.isin(normal_coross_3)]\n",
    "        df_normal_cross_3[\"cross\"]=3\n",
    "        df_normal_left=df_normal_left[~df_normal_left.batch.isin(normal_coross_3)]\n",
    "\n",
    "        normal_coross_4 =np.random.choice(df_normal_left.batch.unique(),14,replace=False)\n",
    "        df_normal_cross_4=df_normal_left[df_normal_left.batch.isin(normal_coross_4)]\n",
    "        df_normal_cross_4[\"cross\"]=4\n",
    "        df_normal_cross_5=df_normal_left[~df_normal_left.batch.isin(normal_coross_4)]\n",
    "        df_normal_cross_5[\"cross\"]=5\n",
    "\n",
    "        abnormal_coross_1 =np.random.choice(df_abnormal.batch.unique(),13,replace=False)\n",
    "        df_abnormal_cross_1=df_abnormal[df_abnormal.batch.isin(abnormal_coross_1)]\n",
    "        df_abnormal_cross_1[\"cross\"]=1\n",
    "        df_abnormal_left=df_abnormal[~df_abnormal.batch.isin(abnormal_coross_1)]\n",
    "\n",
    "        abnormal_coross_2 =np.random.choice(df_abnormal_left.batch.unique(),14,replace=False)\n",
    "        df_abnormal_cross_2=df_abnormal_left[df_abnormal_left.batch.isin(abnormal_coross_2)]\n",
    "        df_abnormal_cross_2[\"cross\"]=2\n",
    "        df_abnormal_left=df_abnormal_left[~df_abnormal_left.batch.isin(abnormal_coross_2)]\n",
    "\n",
    "        abnormal_coross_3 =np.random.choice(df_abnormal_left.batch.unique(),13,replace=False)\n",
    "        df_abnormal_cross_3=df_abnormal_left[df_abnormal_left.batch.isin(abnormal_coross_3)]\n",
    "        df_abnormal_cross_3[\"cross\"]=3\n",
    "        df_abnormal_left=df_abnormal_left[~df_abnormal_left.batch.isin(abnormal_coross_3)]\n",
    "\n",
    "        abnormal_coross_4 =np.random.choice(df_abnormal_left.batch.unique(),14,replace=False)\n",
    "        df_abnormal_cross_4=df_abnormal_left[df_abnormal_left.batch.isin(abnormal_coross_4)]\n",
    "        df_abnormal_cross_4[\"cross\"]=4\n",
    "        df_abnormal_cross_5=df_abnormal_left[~df_abnormal_left.batch.isin(abnormal_coross_4)]\n",
    "        df_abnormal_cross_5[\"cross\"]=5\n",
    "\n",
    "\n",
    "        df_normal_cross_1.to_json(\"ecg_normal_cross_1.json\")\n",
    "        df_normal_cross_2.to_json(\"ecg_normal_cross_2.json\")\n",
    "        df_normal_cross_3.to_json(\"ecg_normal_cross_3.json\")\n",
    "        df_normal_cross_4.to_json(\"ecg_normal_cross_4.json\")\n",
    "        df_normal_cross_5.to_json(\"ecg_normal_cross_5.json\")\n",
    "\n",
    "        df_abnormal_cross_1.to_json(\"ecg_abnormal_cross_1.json\")\n",
    "        df_abnormal_cross_2.to_json(\"ecg_abnormal_cross_2.json\")\n",
    "        df_abnormal_cross_3.to_json(\"ecg_abnormal_cross_3.json\")\n",
    "        df_abnormal_cross_4.to_json(\"ecg_abnormal_cross_4.json\")\n",
    "        df_abnormal_cross_5.to_json(\"ecg_abnormal_cross_5.json\")\n",
    "\n",
    "        print(len(df_normal_cross_1),len(df_normal_cross_2),len(df_normal_cross_3),len(df_normal_cross_4),len(df_normal_cross_5))\n",
    "        print(len(df_abnormal_cross_1),len(df_abnormal_cross_2),len(df_abnormal_cross_3),len(df_abnormal_cross_4),len(df_abnormal_cross_5))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cross(True)\n",
    "dataset_cross(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#f47641\">Represent to image</font>  \n",
    "###  <font color=\"#DAC9A6\"> # Write in function just need call method </font>  \n",
    "###  <font color=\"#DAC9A6\"> # is_whole : decide call all datasate or sample datasat </font>  \n",
    "###  <font color=\"#DAC9A6\"> # is_cross : decide call crossvalidation dataset or not </font>  \n",
    "\n",
    "###  <font color=\"#DAC9A6\"> # method : \"mtf\",\"gasf\",\"gadf\",\"recurrence_plots\" </font>  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformation(is_whole,is_cross,method):\n",
    "    from pyts.visualization import plot_gasf,plot_gadf,plot_mtf,plot_recurrence_plots\n",
    "    import matplotlib.pyplot as plt\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from PIL import Image\n",
    "\n",
    "    \n",
    "    if is_cross==False:\n",
    "        is_train=[\"train\",\"test\",\"validation\"]\n",
    "        is_normal=[\"normal\",\"abnormal\"]\n",
    "        if is_whole==True:\n",
    "            path=\"_all\"\n",
    "        else:\n",
    "            path=\"\" \n",
    "        for a in is_train:\n",
    "            for b in is_normal:\n",
    "                x= pd.read_json(\"ecg_\"+b+\"_\"+a+path+\".json\")\n",
    "                for i in range(len(x)):\n",
    "                    x_time=np.array(x.iloc[:,3].values[i])\n",
    "                    x_class= str(x.iloc[:,0].values[i])+'_'+str(x.iloc[:,1].values[i])\n",
    "                    if method.lower() != \"recurrence_plots\":\n",
    "\n",
    "                        locals()[\"plot_\"+method.lower()](x_time,\n",
    "                                                 cmap=\"gray\",\n",
    "                                                 image_size=39, \n",
    "                                                 output_file=\"ecg_img\"+path+\"/\"+method+\"/\"+a+\"/\"+b+\"/\"+x_class+\".png\")\n",
    "                    else:\n",
    "                        locals()[\"plot_\"+method.lower()](x_time,\n",
    "                                                 output_file=\"ecg_img\"+path+\"/\"+method+\"/\"+a+\"/\"+b+\"/\"+x_class+\".png\")\n",
    "\n",
    "                for i in range(len(x)):\n",
    "                    x_time=np.array(x.iloc[:,3].values[i])\n",
    "                    x_class= str(x.iloc[:,0].values[i])+'_'+str(x.iloc[:,1].values[i])\n",
    "                    img = Image.open(\"ecg_img\"+path+\"/\"+method+\"/\"+a+\"/\"+b+\"/\"+x_class+\".png\")\n",
    "                    img=img.crop((20, 3, 237, 220))\n",
    "                    img.thumbnail((128,128))  #resize\n",
    "                    img.save(\"ecg_img\"+path+\"/\"+method+\"/\"+a+\"/\"+b+\"/\"+x_class+\".png\")\n",
    "    else:\n",
    "        if is_whole==True:\n",
    "            path=\"_all\"\n",
    "        else:\n",
    "            path=\"\"\n",
    "        cross=[\"1\",\"2\",\"3\",\"4\",\"5\"]\n",
    "        is_normal=[\"normal\",\"abnormal\"]\n",
    "        for a in cross:\n",
    "            for b in is_normal:\n",
    "                x= pd.read_json(\"ecg_\"+b+path+\"_cross_\"+a+\".json\")\n",
    "                for i in range(len(x)):\n",
    "                    x_time=np.array(x.iloc[:,4].values[i])\n",
    "                    x_class= str(x.iloc[:,0].values[i])+'_'+str(x.iloc[:,2].values[i])\n",
    "                    if method.lower() != \"recurrence_plots\":\n",
    "\n",
    "                        locals()[\"plot_\"+method.lower()](x_time,\n",
    "                                                 cmap=\"gray\",\n",
    "                                                 image_size=39, \n",
    "                                                 output_file=\"ecg_img_cross\"+path+\"/\"+method+\"/\"+a+\"/\"+b+\"/\"+x_class+\".png\")\n",
    "                    else:\n",
    "                        locals()[\"plot_\"+method.lower()](x_time,\n",
    "                                                 output_file=\"ecg_img_cross\"+path+\"/\"+method+\"/\"+a+\"/\"+b+\"/\"+x_class+\".png\")\n",
    "\n",
    "                for i in range(len(x)):\n",
    "                    x_time=np.array(x.iloc[:,4].values[i])\n",
    "                    x_class= str(x.iloc[:,0].values[i])+'_'+str(x.iloc[:,2].values[i])\n",
    "                    img = Image.open(\"ecg_img_cross\"+path+\"/\"+method+\"/\"+a+\"/\"+b+\"/\"+x_class+\".png\")\n",
    "                    img=img.crop((20, 3, 237, 220))\n",
    "                    img.thumbnail((128,128))  #resize\n",
    "                    img.save(\"ecg_img_cross\"+path+\"/\"+method+\"/\"+a+\"/\"+b+\"/\"+x_class+\".png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transformation(True,True,\"gasf\")\n",
    "transformation(True,True,\"gadf\")\n",
    "transformation(True,True,\"mtf\")\n",
    "transformation(True,True,\"recurrence_plots\")\n",
    "print(\"done whole=true cross=true\")\n",
    "\n",
    "transformation(True,False,\"gasf\")\n",
    "transformation(True,False,\"gadf\")\n",
    "transformation(True,False,\"mtf\")\n",
    "transformation(True,False,\"recurrence_plots\")\n",
    "print(\"done whole=true cross=False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "import numpy\n",
    "sr = 22050 # sample rate\n",
    "T = 0.2    # seconds\n",
    "t = numpy.linspace(0, T, int(T*sr), endpoint=False) # time variable\n",
    "x = 0.5*numpy.sin(2*numpy.pi*490*t)                # pure sine wave at 440 Hz\n",
    "ipd.Audio(x, rate=sr,autoplay=True) # load a NumPy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#f47641\">create dataset & dataloader</font>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## six channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self,train_x,train_y):\n",
    "        self.train_x=train_x\n",
    "        self.train_y=train_y\n",
    "        self.data_len = len(train_x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        train_x=self.train_x[index]\n",
    "        train_y=self.train_y[index]\n",
    "        return (train_x, train_y)\n",
    "    def __len__(self):\n",
    "        return self.data_len\n",
    "    \n",
    "class dataset_dataloader:\n",
    "    def __init__ (self,is_whole,is_cross,method,cross_number_for_test=1,batchsize=24,numworkers=2):\n",
    "        if is_cross==False:\n",
    "            import pandas as pd\n",
    "            import numpy as np\n",
    "            from PIL import Image\n",
    "            from pyts.visualization import plot_gasf\n",
    "            import matplotlib.pyplot as plt\n",
    "            import torch\n",
    "            from torch.autograd import Variable\n",
    "\n",
    "            if is_whole==True:\n",
    "                whole=\"_all\"\n",
    "            else:\n",
    "                whole=\"\"\n",
    "\n",
    "            normal   = pd.read_json(\"ecg_normal_train\"+whole+\".json\")\n",
    "            abnormal = pd.read_json(\"ecg_abnormal_train\"+whole+\".json\")\n",
    "            x=normal.append(abnormal)\n",
    "            x=x.sample(frac=1)\n",
    "\n",
    "            def pil_loader(path):\n",
    "                with open(path, 'rb') as f:\n",
    "                    img = Image.open(f)\n",
    "                    return img.convert('L')\n",
    "            lst = list()\n",
    "            count=0\n",
    "            lst2=list()\n",
    "            y=list()\n",
    "\n",
    "            for i in x.batch.unique():\n",
    "                df=x[x.batch==i]\n",
    "                lst = list()\n",
    "                if np.all(df.status==\"abnormal\"):\n",
    "                    path=\"ecg_img\"+whole+\"/\"+method+\"/train/abnormal/\"\n",
    "                else:\n",
    "                    path=\"ecg_img\"+whole+\"/\"+method+\"/train/normal/\"\n",
    "\n",
    "                for j in x.sensor.unique():\n",
    "                    img=pil_loader(path+str(i)+\"_\"+str(j)+\".png\")\n",
    "                    lst.append(np.array(img)/255)\n",
    "                    \n",
    "                arr = np.array(lst)\n",
    "                if df.iloc[0].status==\"normal\":\n",
    "                    y.append(0)\n",
    "                else:\n",
    "                    y.append(1)\n",
    "\n",
    "                lst2.append(arr)    \n",
    "            #     count+=1\n",
    "            #     if count==1:\n",
    "            #         break\n",
    "            train_x = torch.from_numpy(np.array(lst2)).float()\n",
    "            train_y = torch.LongTensor(np.array(y))\n",
    "\n",
    "            train_data = Dataset(train_x,train_y)\n",
    "            train_loader = DataLoader(dataset=train_data,\n",
    "                                      batch_size=24,\n",
    "                                      shuffle=True,\n",
    "                                      num_workers=2)\n",
    "            self.train_x=train_x\n",
    "            self.train_y=train_y\n",
    "            self.train_loader=train_loader\n",
    "\n",
    "            #test data set\n",
    "            normal   = pd.read_json(\"ecg_normal_validation\"+whole+\".json\")\n",
    "            abnormal = pd.read_json(\"ecg_abnormal_validation\"+whole+\".json\")\n",
    "            x=normal.append(abnormal)\n",
    "            x=x.sample(frac=1)\n",
    "\n",
    "            # print(x)\n",
    "            lst = list()\n",
    "            count=0\n",
    "            lst2=list()\n",
    "            y=list()\n",
    "            path=\"\"\n",
    "            for i in x.batch.unique():\n",
    "                df=x[x.batch==i]\n",
    "                lst = list()\n",
    "                if np.all(df.status==\"abnormal\"):\n",
    "                    path=\"ecg_img\"+whole+\"/\"+method+\"/validation/abnormal/\"\n",
    "                else:\n",
    "                    path=\"ecg_mg\"+whole+\"/\"+method+\"/validation/normal/\"\n",
    "\n",
    "                for j in x.sensor.unique():\n",
    "                    img=pil_loader(path+str(i)+\"_\"+str(j)+\".png\")\n",
    "                    lst.append(np.array(img)/255)\n",
    "                    \n",
    "                arr = np.array(lst)\n",
    "                if df.iloc[0].status==\"normal\":\n",
    "                    y.append(0)\n",
    "                else:\n",
    "                    y.append(1)\n",
    "\n",
    "                lst2.append(arr)    \n",
    "            #     if count==1:\n",
    "            #         break\n",
    "            test_x = Variable(torch.from_numpy(np.array(lst2)).float()).cuda()\n",
    "            test_y = torch.LongTensor(np.array(y)).cuda()\n",
    "            self.test_x=test_x\n",
    "            self.test_y=test_y        \n",
    "\n",
    "            print(\"done\")\n",
    "            \n",
    "        else:\n",
    "            import pandas as pd\n",
    "            import numpy as np\n",
    "            from PIL import Image\n",
    "            from pyts.visualization import plot_gasf\n",
    "            import matplotlib.pyplot as plt\n",
    "            import torch\n",
    "            from torch.autograd import Variable\n",
    "\n",
    "            if is_whole==True:\n",
    "                whole=\"_all\"\n",
    "            else:\n",
    "                whole=\"\"\n",
    "            cross=[\"1\",\"2\",\"3\",\"4\",\"5\"]\n",
    "            cross.remove(str(cross_number_for_test))\n",
    "            x = [  \n",
    "                pd.read_json(\"ecg_normal\"+whole+\"_cross_\"+cross[0]+\".json\"),\n",
    "                pd.read_json(\"ecg_normal\"+whole+\"_cross_\"+cross[1]+\".json\"),\n",
    "                pd.read_json(\"ecg_normal\"+whole+\"_cross_\"+cross[2]+\".json\"),\n",
    "                pd.read_json(\"ecg_normal\"+whole+\"_cross_\"+cross[3]+\".json\"),\n",
    "                pd.read_json(\"ecg_abnormal\"+whole+\"_cross_\"+cross[0]+\".json\"),\n",
    "                pd.read_json(\"ecg_abnormal\"+whole+\"_cross_\"+cross[1]+\".json\"),\n",
    "                pd.read_json(\"ecg_abnormal\"+whole+\"_cross_\"+cross[2]+\".json\"),\n",
    "                pd.read_json(\"ecg_abnormal\"+whole+\"_cross_\"+cross[3]+\".json\")\n",
    "            ]\n",
    "            x = pd.concat(x)\n",
    "            x =  x.sample(frac=1)\n",
    "\n",
    "\n",
    "            def pil_loader(path):\n",
    "                with open(path, 'rb') as f:\n",
    "                    img = Image.open(f)\n",
    "                    return img.convert('L')\n",
    "            lst = list()\n",
    "            count=0\n",
    "            lst2=list()\n",
    "            y=list()\n",
    "\n",
    "            for i in x.batch.unique():\n",
    "                df=x[x.batch==i]\n",
    "                lst = list()\n",
    "                if np.all(df.status==\"abnormal\"):\n",
    "                    path=\"ecg_img_cross\"+whole+\"/\"+method+\"/\"+str(df.iloc[0].cross)+\"/abnormal/\"\n",
    "                else:\n",
    "                    path=\"ecg_img_cross\"+whole+\"/\"+method+\"/\"+str(df.iloc[0].cross)+\"/normal/\"\n",
    "                for j in x.sensor.unique():\n",
    "                    img=pil_loader(path+str(i)+\"_\"+str(j)+\".png\")\n",
    "                    lst.append(np.array(img)/255)\n",
    "\n",
    "                arr = np.array(lst)\n",
    "                if df.iloc[0].status==\"normal\":\n",
    "                    y.append(0)\n",
    "                else:\n",
    "                    y.append(1)\n",
    "\n",
    "                lst2.append(arr)    \n",
    "            #     count+=1\n",
    "            #     if count==1:\n",
    "            #         break\n",
    "            train_x = torch.from_numpy(np.array(lst2)).float()\n",
    "            train_y = torch.LongTensor(np.array(y))\n",
    "\n",
    "            train_data = Dataset(train_x,train_y)\n",
    "            train_loader = DataLoader(dataset=train_data,\n",
    "                                      batch_size=24,\n",
    "                                      shuffle=True,\n",
    "                                      num_workers=2)\n",
    "            self.train_x=train_x\n",
    "            self.train_y=train_y\n",
    "            self.train_loader=train_loader\n",
    "\n",
    "            #test data set\n",
    "            normal   = pd.read_json(\"ecg_normal\"+whole+\"_cross_\"+str(cross_number_for_test)+\".json\")\n",
    "            abnormal = pd.read_json(\"ecg_abnormal\"+whole+\"_cross_\"+str(cross_number_for_test)+\".json\")\n",
    "            x=normal.append(abnormal)\n",
    "            x=x.sample(frac=1)\n",
    "\n",
    "            # print(x)\n",
    "            lst = list()\n",
    "            count=0\n",
    "            lst2=list()\n",
    "            y=list()\n",
    "            path=\"\"\n",
    "            for i in x.batch.unique():\n",
    "                df=x[x.batch==i]\n",
    "                lst = list()\n",
    "                if np.all(df.status==\"abnormal\"):\n",
    "                    path=\"ecg_img_cross\"+whole+\"/\"+method+\"/\"+str(df.iloc[0].cross)+\"/abnormal/\"\n",
    "                else:\n",
    "                    path=\"ecg_img_cross\"+whole+\"/\"+method+\"/\"+str(df.iloc[0].cross)+\"/normal/\"\n",
    "\n",
    "                for j in x.sensor.unique():\n",
    "                    img=pil_loader(path+str(i)+\"_\"+str(j)+\".png\")\n",
    "                    lst.append(np.array(img)/255)\n",
    "                    \n",
    "                arr = np.array(lst)\n",
    "                if df.iloc[0].status==\"normal\":\n",
    "                    y.append(0)\n",
    "                else:\n",
    "                    y.append(1)\n",
    "\n",
    "                lst2.append(arr)    \n",
    "            #     if count==1:\n",
    "            #         break\n",
    "            test_x = Variable(torch.from_numpy(np.array(lst2)).float()).cuda()\n",
    "            test_y = torch.LongTensor(np.array(y)).cuda()\n",
    "            self.test_x=test_x\n",
    "            self.test_y=test_y        \n",
    "\n",
    "            print(\"done\")\n",
    "            \n",
    "        \n",
    "print(\"Function setting done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=dataset_dataloader(True,True,\"mtf\")\n",
    "a.train_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one chaneel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self,train_x,train_y):\n",
    "        self.train_x=train_x\n",
    "        self.train_y=train_y\n",
    "        self.data_len = len(train_x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        train_x=self.train_x[index]\n",
    "        train_y=self.train_y[index]\n",
    "        return (train_x, train_y)\n",
    "    def __len__(self):\n",
    "        return self.data_len\n",
    "    \n",
    "class dataset_dataloader:\n",
    "    def __init__ (self,is_whole,is_cross,method,cross_number_for_test=1,batchsize=24,numworkers=2):\n",
    "        if is_cross==False:\n",
    "            import pandas as pd\n",
    "            import numpy as np\n",
    "            from PIL import Image\n",
    "            from pyts.visualization import plot_gasf\n",
    "            import matplotlib.pyplot as plt\n",
    "            import torch\n",
    "            from torch.autograd import Variable\n",
    "\n",
    "            if is_whole==True:\n",
    "                whole=\"_all\"\n",
    "            else:\n",
    "                whole=\"\"\n",
    "\n",
    "            normal   = pd.read_json(\"ecg_normal_train\"+whole+\".json\")\n",
    "            abnormal = pd.read_json(\"ecg_abnormal_train\"+whole+\".json\")\n",
    "            x=normal.append(abnormal)\n",
    "            x=x.sample(frac=1)\n",
    "\n",
    "            def pil_loader(path):\n",
    "                with open(path, 'rb') as f:\n",
    "                    img = Image.open(f)\n",
    "                    return img.convert('L')\n",
    "            lst = list()\n",
    "            count=0\n",
    "            lst2=list()\n",
    "            y=list()\n",
    "\n",
    "            for i in x.batch.unique():\n",
    "                df=x[x.batch==i]\n",
    "                lst = list()\n",
    "                if np.all(df.status==\"abnormal\"):\n",
    "                    path=\"ecg_img\"+whole+\"/\"+method+\"/train/abnormal/\"\n",
    "                else:\n",
    "                    path=\"ecg_img\"+whole+\"/\"+method+\"/train/normal/\"\n",
    "\n",
    "                list_im = [\n",
    "                    path+str(i)+'_0.png', \n",
    "                    path+str(i)+'_1.png', \n",
    "                ]\n",
    "                imgs = [ pil_loader(i) for i in list_im ]\n",
    "                min_shape = sorted( [(np.sum(i.size), i.size ) for i in imgs])[0][1]\n",
    "                imgs_comb = np.vstack( (np.asarray( i.resize(min_shape) ) for i in imgs ) )\n",
    "                imgs_comb = Image.fromarray( imgs_comb)\n",
    "                lst.append(np.array(imgs_comb)/255)\n",
    "\n",
    "                arr = np.array(lst)\n",
    "                if df.iloc[0].status==\"normal\":\n",
    "                    y.append(0)\n",
    "                else:\n",
    "                    y.append(1)\n",
    "\n",
    "                lst2.append(arr)    \n",
    "            #     count+=1\n",
    "            #     if count==1:\n",
    "            #         break\n",
    "            train_x = torch.from_numpy(np.array(lst2)).float()\n",
    "            train_y = torch.LongTensor(np.array(y))\n",
    "\n",
    "            train_data = Dataset(train_x,train_y)\n",
    "            train_loader = DataLoader(dataset=train_data,\n",
    "                                      batch_size=24,\n",
    "                                      shuffle=True,\n",
    "                                      num_workers=2)\n",
    "            self.train_x=train_x\n",
    "            self.train_y=train_y\n",
    "            self.train_loader=train_loader\n",
    "\n",
    "            #test data set\n",
    "            normal   = pd.read_json(\"ecg_normal_validation\"+whole+\".json\")\n",
    "            abnormal = pd.read_json(\"ecg_abnormal_validation\"+whole+\".json\")\n",
    "            x=normal.append(abnormal)\n",
    "            x=x.sample(frac=1)\n",
    "\n",
    "            # print(x)\n",
    "            lst = list()\n",
    "            count=0\n",
    "            lst2=list()\n",
    "            y=list()\n",
    "            path=\"\"\n",
    "            for i in x.batch.unique():\n",
    "                df=x[x.batch==i]\n",
    "                lst = list()\n",
    "                if np.all(df.status==\"abnormal\"):\n",
    "                    path=\"ecg_img\"+whole+\"/\"+method+\"/validation/abnormal/\"\n",
    "                else:\n",
    "                    path=\"ecg_img\"+whole+\"/\"+method+\"/validation/normal/\"\n",
    "\n",
    "                list_im = [\n",
    "                    path+str(i)+'_0.png', \n",
    "                    path+str(i)+'_1.png', \n",
    "\n",
    "                ]\n",
    "                imgs = [ pil_loader(i) for i in list_im ]\n",
    "                min_shape = sorted( [(np.sum(i.size), i.size ) for i in imgs])[0][1]\n",
    "                imgs_comb = np.vstack( (np.asarray( i.resize(min_shape) ) for i in imgs ) )\n",
    "                imgs_comb = Image.fromarray( imgs_comb)\n",
    "                lst.append(np.array(imgs_comb)/255)\n",
    "                arr = np.array(lst)\n",
    "                if df.iloc[0].status==\"normal\":\n",
    "                    y.append(0)\n",
    "                else:\n",
    "                    y.append(1)\n",
    "\n",
    "                lst2.append(arr)    \n",
    "            #     if count==1:\n",
    "            #         break\n",
    "            test_x = Variable(torch.from_numpy(np.array(lst2)).float()).cuda()\n",
    "            test_y = torch.LongTensor(np.array(y)).cuda()\n",
    "            self.test_x=test_x\n",
    "            self.test_y=test_y        \n",
    "\n",
    "            print(\"done\")\n",
    "            \n",
    "        else:\n",
    "            import pandas as pd\n",
    "            import numpy as np\n",
    "            from PIL import Image\n",
    "            from pyts.visualization import plot_gasf\n",
    "            import matplotlib.pyplot as plt\n",
    "            import torch\n",
    "            from torch.autograd import Variable\n",
    "\n",
    "            if is_whole==True:\n",
    "                whole=\"_all\"\n",
    "            else:\n",
    "                whole=\"\"\n",
    "            cross=[\"1\",\"2\",\"3\",\"4\",\"5\"]\n",
    "            cross.remove(str(cross_number_for_test))\n",
    "            x = [  \n",
    "                pd.read_json(\"ecg_normal\"+whole+\"_cross_\"+cross[0]+\".json\"),\n",
    "                pd.read_json(\"ecg_normal\"+whole+\"_cross_\"+cross[1]+\".json\"),\n",
    "                pd.read_json(\"ecg_normal\"+whole+\"_cross_\"+cross[2]+\".json\"),\n",
    "                pd.read_json(\"ecg_normal\"+whole+\"_cross_\"+cross[3]+\".json\"),\n",
    "                pd.read_json(\"ecg_abnormal\"+whole+\"_cross_\"+cross[0]+\".json\"),\n",
    "                pd.read_json(\"ecg_abnormal\"+whole+\"_cross_\"+cross[1]+\".json\"),\n",
    "                pd.read_json(\"ecg_abnormal\"+whole+\"_cross_\"+cross[2]+\".json\"),\n",
    "                pd.read_json(\"ecg_abnormal\"+whole+\"_cross_\"+cross[3]+\".json\")\n",
    "            ]\n",
    "            x = pd.concat(x)\n",
    "            x =  x.sample(frac=1)\n",
    "\n",
    "\n",
    "            def pil_loader(path):\n",
    "                with open(path, 'rb') as f:\n",
    "                    img = Image.open(f)\n",
    "                    return img.convert('L')\n",
    "            lst = list()\n",
    "            count=0\n",
    "            lst2=list()\n",
    "            y=list()\n",
    "\n",
    "            for i in x.batch.unique():\n",
    "                df=x[x.batch==i]\n",
    "                lst = list()\n",
    "                if np.all(df.status==\"abnormal\"):\n",
    "                    path=\"ecg_img_cross\"+whole+\"/\"+method+\"/\"+str(df.iloc[0].cross)+\"/abnormal/\"\n",
    "                else:\n",
    "                    path=\"ecg_img_cross\"+whole+\"/\"+method+\"/\"+str(df.iloc[0].cross)+\"/normal/\"\n",
    "\n",
    "                list_im = [\n",
    "                    path+str(i)+'_0.png', \n",
    "                    path+str(i)+'_1.png', \n",
    "                ]\n",
    "                imgs = [ pil_loader(i) for i in list_im ]\n",
    "                min_shape = sorted( [(np.sum(i.size), i.size ) for i in imgs])[0][1]\n",
    "                imgs_comb = np.vstack( (np.asarray( i.resize(min_shape) ) for i in imgs ) )\n",
    "                imgs_comb = Image.fromarray( imgs_comb)\n",
    "                lst.append(np.array(imgs_comb)/255)\n",
    "\n",
    "                arr = np.array(lst)\n",
    "                if df.iloc[0].status==\"normal\":\n",
    "                    y.append(0)\n",
    "                else:\n",
    "                    y.append(1)\n",
    "\n",
    "                lst2.append(arr)    \n",
    "            #     count+=1\n",
    "            #     if count==1:\n",
    "            #         break\n",
    "            train_x = torch.from_numpy(np.array(lst2)).float()\n",
    "            train_y = torch.LongTensor(np.array(y))\n",
    "\n",
    "            train_data = Dataset(train_x,train_y)\n",
    "            train_loader = DataLoader(dataset=train_data,\n",
    "                                      batch_size=24,\n",
    "                                      shuffle=True,\n",
    "                                      num_workers=2)\n",
    "            self.train_x=train_x\n",
    "            self.train_y=train_y\n",
    "            self.train_loader=train_loader\n",
    "\n",
    "            #test data set\n",
    "            normal   = pd.read_json(\"ecg_normal\"+whole+\"_cross_\"+str(cross_number_for_test)+\".json\")\n",
    "            abnormal = pd.read_json(\"ecg_abnormal\"+whole+\"_cross_\"+str(cross_number_for_test)+\".json\")\n",
    "            x=normal.append(abnormal)\n",
    "            x=x.sample(frac=1)\n",
    "\n",
    "            # print(x)\n",
    "            lst = list()\n",
    "            count=0\n",
    "            lst2=list()\n",
    "            y=list()\n",
    "            path=\"\"\n",
    "            for i in x.batch.unique():\n",
    "                df=x[x.batch==i]\n",
    "                lst = list()\n",
    "                if np.all(df.status==\"abnormal\"):\n",
    "                    path=\"ecg_img_cross\"+whole+\"/\"+method+\"/\"+str(df.iloc[0].cross)+\"/abnormal/\"\n",
    "                else:\n",
    "                    path=\"ecg_img_cross\"+whole+\"/\"+method+\"/\"+str(df.iloc[0].cross)+\"/normal/\"\n",
    "\n",
    "                list_im = [\n",
    "                    path+str(i)+'_0.png', \n",
    "                    path+str(i)+'_1.png', \n",
    "                ]\n",
    "                imgs = [ pil_loader(i) for i in list_im ]\n",
    "                min_shape = sorted( [(np.sum(i.size), i.size ) for i in imgs])[0][1]\n",
    "                imgs_comb = np.vstack( (np.asarray( i.resize(min_shape) ) for i in imgs ) )\n",
    "                imgs_comb = Image.fromarray( imgs_comb)\n",
    "                lst.append(np.array(imgs_comb)/255)\n",
    "                arr = np.array(lst)\n",
    "                if df.iloc[0].status==\"normal\":\n",
    "                    y.append(0)\n",
    "                else:\n",
    "                    y.append(1)\n",
    "\n",
    "                lst2.append(arr)    \n",
    "            #     if count==1:\n",
    "            #         break\n",
    "            test_x = Variable(torch.from_numpy(np.array(lst2)).float()).cuda()\n",
    "            test_y = torch.LongTensor(np.array(y)).cuda()\n",
    "            self.test_x=test_x\n",
    "            self.test_y=test_y        \n",
    "\n",
    "            print(\"done\")\n",
    "            \n",
    "        \n",
    "print(\"Function setting done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#f47641\">Cnn start</font>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "accuracy_matrix =[]\n",
    "import dataset\n",
    "import dataset_six\n",
    "\n",
    "    \n",
    "for i in range(5):\n",
    "    min_error=100\n",
    "    min_lose=100\n",
    "    count=0\n",
    "    number=1\n",
    "#     cross=dataset.dataset_dataloader(False,True,\"mtf\",i+1)\n",
    "    cross=dataset_dataloader(True,True,\"mtf\",i+1)\n",
    "    import torch.nn as nn\n",
    "\n",
    "    EPOCH = 50              # train the training data n times, to save time, we just train 1 epoch\n",
    "    LR = 0.0023\n",
    "    \n",
    "    class CNN(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(CNN, self).__init__()\n",
    "            self.conv1 =nn.Sequential( #卷基層   \n",
    "                nn.Conv2d(\n",
    "                    in_channels  = 2  , # 圖片是有幾層的 若 RGB三層 灰階 1層\n",
    "                    out_channels = 10, # 同時16個filter 進行掃描 會提取16個特徵 代表下一層高度為16\n",
    "                    kernel_size  = 5  , # 一次畫出來的框 畫幾格 ex 5*5\n",
    "                    stride       = 1  , # 每一個框框跳幾格\n",
    "                    padding      = 2, # 在 5x5逐步掃描後會有少的格子 因此2代表在外面為兩圈的0\n",
    "                                        # if stride =1 padding = (kernel_size-1)/2\n",
    "                ),  # 過濾器 卷基層 蒐集圖片訊息 三維的空間  \n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2), #找出更重要的特徵  像是在2x2中 找到 最大的值\n",
    "            )\n",
    "            self.conv2 = nn.Sequential(\n",
    "                nn.Conv2d(10,20,5,1,2), # 輸入的是上一層的16 把輸出層在變大因此是32 其他不變\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2)\n",
    "            )\n",
    "#             self.conv3 = nn.Sequential(\n",
    "#                 nn.Conv2d(5,10,5,1,2), # 輸入的是上一層的16 把輸出層在變大因此是32 其他不變\n",
    "#                 nn.ReLU(),\n",
    "#                 nn.MaxPool2d(2)\n",
    "#             )\n",
    "\n",
    "            self.out = nn.Linear(20*32*32,2)\n",
    "        def forward(self,x):\n",
    "\n",
    "            x=self.conv1(x)\n",
    "            x=self.conv2(x) #(batch.32.7.7)\n",
    "#             x=self.conv3(x)\n",
    "            x=x.view(x.size(0),-1) #將三維轉二維 (batch , 32*7*7)\n",
    "            output=self.out(x)\n",
    "            return output,x\n",
    "\n",
    "    cnn=CNN()\n",
    "    cnn.cuda()\n",
    "#     print(cnn)\n",
    "\n",
    "    optimizer = torch.optim.Adam(cnn.parameters(), lr=LR)   # optimize all cnn parameters\n",
    "    loss_func = nn.CrossEntropyLoss()                       # the target label is not one-hotted\n",
    "\n",
    "    # following function (plot_with_labels) is for visualization, can be ignored if not interested\n",
    "\n",
    "    # training and testing\n",
    "\n",
    "\n",
    "    for epoch in range(EPOCH):\n",
    "        for step, (x, y) in enumerate(cross.train_loader):   # gives batch data, normalize x when iterate train_loader\n",
    "            b_x = Variable(x).cuda()   # batch x\n",
    "            b_y = Variable(y).cuda()   # batch y\n",
    "\n",
    "            output = cnn(b_x)[0]             # cnn output\n",
    "            loss = loss_func(output, b_y)   # cross entropy loss\n",
    "            optimizer.zero_grad()           # clear gradients for this training step\n",
    "            loss.backward()                 # backpropagation, compute gradients\n",
    "            optimizer.step()                # apply gradients\n",
    "            test_output, last_layer = cnn(cross.test_x)\n",
    "            pred_y = torch.max(test_output, 1)[1].cuda().data.squeeze()\n",
    "            accuracy = sum(pred_y == cross.test_y) / float(cross.test_y.size(0))\n",
    "            error  = (1-accuracy)*100\n",
    "            if error<min_error:\n",
    "                min_error=error\n",
    "                min_lose=100\n",
    "                count=0\n",
    "            else:\n",
    "                count+=1\n",
    "                print(\".\",end=\"\")\n",
    "\n",
    "            if error == min_error:\n",
    "                if loss.data[0]<min_lose:\n",
    "                    min_lose=loss.data[0]\n",
    "                    torch.save(cnn,'cnn_save/cnn'+str(number)+'.pkl')\n",
    "                    number+=1\n",
    "    #                 print('save!!')\n",
    "    #                 print('\\ncount: ',count,'Epoch: ', epoch, '| train loss: %.15f' % loss.data[0], '| min validation error rate: %.20f' % min_error)\n",
    "            if count>=350:\n",
    "                break\n",
    "        if count>=350:\n",
    "            break\n",
    "    \n",
    "    # print 10 predictions from test data\n",
    "    # test_output, _ = cnn(test_x)\n",
    "    # pred_y = torch.max(test_output, 1)[1].cuda().data.squeeze()\n",
    "\n",
    "    # print(pred_y, 'prediction number')\n",
    "    # print(test_y, 'real number')\n",
    "    accuracy_matrix.append(min_error)\n",
    "    print('\\n number: ',number-1 ,min_error)\n",
    "    \n",
    "import IPython.display as ipd\n",
    "import numpy\n",
    "sr = 22050 # sample rate\n",
    "T = 0.3    # seconds\n",
    "t = numpy.linspace(0, T, int(T*sr), endpoint=False) # time variable\n",
    "x = 0.5*numpy.sin(2*numpy.pi*490*t)                # pure sine wave at 440 284Hz\n",
    "ipd.Audio(x, rate=sr,autoplay=True) # load a NumPy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_matrix)\n",
    "sum(accuracy_matrix) / float(len(accuracy_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "accuracy_matrix =[]\n",
    "import dataset\n",
    "import dataset_six\n",
    "\n",
    "    \n",
    "for i in range(5):\n",
    "    min_error=100\n",
    "    min_lose=100\n",
    "    count=0\n",
    "    number=1\n",
    "#     cross=dataset.dataset_dataloader(False,True,\"mtf\",i+1)\n",
    "    cross=dataset_dataloader(True,True,\"mtf\",i+1)\n",
    "    import torch.nn as nn\n",
    "\n",
    "    EPOCH = 50              # train the training data n times, to save time, we just train 1 epoch\n",
    "    LR = 0.0023\n",
    "    \n",
    "    class CNN(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(CNN, self).__init__()\n",
    "            self.conv1 =nn.Sequential( #卷基層   \n",
    "                nn.Conv2d(\n",
    "                    in_channels  = 1  , # 圖片是有幾層的 若 RGB三層 灰階 1層\n",
    "                    out_channels = 3, # 同時16個filter 進行掃描 會提取16個特徵 代表下一層高度為16\n",
    "                    kernel_size  = 5  , # 一次畫出來的框 畫幾格 ex 5*5\n",
    "                    stride       = 1  , # 每一個框框跳幾格\n",
    "                    padding      = 2, # 在 5x5逐步掃描後會有少的格子 因此2代表在外面為兩圈的0\n",
    "                                        # if stride =1 padding = (kernel_size-1)/2\n",
    "                ),  # 過濾器 卷基層 蒐集圖片訊息 三維的空間  \n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2), #找出更重要的特徵  像是在2x2中 找到 最大的值\n",
    "            )\n",
    "            self.conv2 = nn.Sequential(\n",
    "                nn.Conv2d(3,5,5,1,2), # 輸入的是上一層的16 把輸出層在變大因此是32 其他不變\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2)\n",
    "            )\n",
    "#             self.conv3 = nn.Sequential(\n",
    "#                 nn.Conv2d(5,10,5,1,2), # 輸入的是上一層的16 把輸出層在變大因此是32 其他不變\n",
    "#                 nn.ReLU(),\n",
    "#                 nn.MaxPool2d(2)\n",
    "#             )\n",
    "\n",
    "            self.out = nn.Linear(5*64*32,2)\n",
    "        def forward(self,x):\n",
    "\n",
    "            x=self.conv1(x)\n",
    "            x=self.conv2(x) #(batch.32.7.7)\n",
    "#             x=self.conv3(x)\n",
    "            x=x.view(x.size(0),-1) #將三維轉二維 (batch , 32*7*7)\n",
    "            output=self.out(x)\n",
    "            return output,x\n",
    "\n",
    "    cnn=CNN()\n",
    "    cnn.cuda()\n",
    "#     print(cnn)\n",
    "\n",
    "    optimizer = torch.optim.Adam(cnn.parameters(), lr=LR)   # optimize all cnn parameters\n",
    "    loss_func = nn.CrossEntropyLoss()                       # the target label is not one-hotted\n",
    "\n",
    "    # following function (plot_with_labels) is for visualization, can be ignored if not interested\n",
    "\n",
    "    # training and testing\n",
    "\n",
    "\n",
    "    for epoch in range(EPOCH):\n",
    "        for step, (x, y) in enumerate(cross.train_loader):   # gives batch data, normalize x when iterate train_loader\n",
    "            b_x = Variable(x).cuda()   # batch x\n",
    "            b_y = Variable(y).cuda()   # batch y\n",
    "\n",
    "            output = cnn(b_x)[0]             # cnn output\n",
    "            loss = loss_func(output, b_y)   # cross entropy loss\n",
    "            optimizer.zero_grad()           # clear gradients for this training step\n",
    "            loss.backward()                 # backpropagation, compute gradients\n",
    "            optimizer.step()                # apply gradients\n",
    "            test_output, last_layer = cnn(cross.test_x)\n",
    "            pred_y = torch.max(test_output, 1)[1].cuda().data.squeeze()\n",
    "            accuracy = sum(pred_y == cross.test_y) / float(cross.test_y.size(0))\n",
    "            error  = (1-accuracy)*100\n",
    "            if error<min_error:\n",
    "                min_error=error\n",
    "                min_lose=100\n",
    "                count=0\n",
    "            else:\n",
    "                count+=1\n",
    "                print(\".\",end=\"\")\n",
    "\n",
    "            if error == min_error:\n",
    "                if loss.data[0]<min_lose:\n",
    "                    min_lose=loss.data[0]\n",
    "                    torch.save(cnn,'cnn_save/cnn'+str(number)+'.pkl')\n",
    "                    number+=1\n",
    "    #                 print('save!!')\n",
    "    #                 print('\\ncount: ',count,'Epoch: ', epoch, '| train loss: %.15f' % loss.data[0], '| min validation error rate: %.20f' % min_error)\n",
    "            if count>=350:\n",
    "                break\n",
    "        if count>=350:\n",
    "            break\n",
    "    \n",
    "    # print 10 predictions from test data\n",
    "    # test_output, _ = cnn(test_x)\n",
    "    # pred_y = torch.max(test_output, 1)[1].cuda().data.squeeze()\n",
    "\n",
    "    # print(pred_y, 'prediction number')\n",
    "    # print(test_y, 'real number')\n",
    "    accuracy_matrix.append(min_error)\n",
    "    print('\\n number: ',number-1 ,min_error)\n",
    "    \n",
    "import IPython.display as ipd\n",
    "import numpy\n",
    "sr = 22050 # sample rate\n",
    "T = 0.3    # seconds\n",
    "t = numpy.linspace(0, T, int(T*sr), endpoint=False) # time variable\n",
    "x = 0.5*numpy.sin(2*numpy.pi*490*t)                # pure sine wave at 440 284Hz\n",
    "ipd.Audio(x, rate=sr,autoplay=True) # load a NumPy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_matrix)\n",
    "sum(accuracy_matrix) / float(len(accuracy_matrix))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
